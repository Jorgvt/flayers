# AUTOGENERATED! DO NOT EDIT! File to edit: ../../Notebooks/Experimental/01_basic_functional_layer.ipynb.

# %% auto 0
__all__ = ['FunctionalLayer', 'GaussianLayer', 'GaborLayer', 'PreInitGaborLayer', 'CenterSurroundLayer', 'EnergyConserver']

# %% ../../Notebooks/Experimental/01_basic_functional_layer.ipynb 2
import numpy as np
import matplotlib.pyplot as plt
from einops import rearrange

import tensorflow as tf
from tensorflow.keras import layers

# %% ../../Notebooks/Experimental/01_basic_functional_layer.ipynb 5
class FunctionalLayer(layers.Conv2D):
    """Generic functional layer designed to be used as a base for building new layers through inheritance."""

    def __init__(self,
                 **kwargs, # Keyword arguments to be passed to the base `Conv2D` layer.
                 ) -> None:
        super(FunctionalLayer, self).__init__(**kwargs)
        self.training = True
        self.has_to_generate = True

    def build(self,
              input_shape,
              ):
        if isinstance(input_shape, tuple): input_shape = tf.TensorShape(input_shape)
        input_channels = self._get_input_channel(input_shape)
        if self.use_bias: self.bias = self.add_weight(name="bias",
                                                      shape=(self.filters,),
                                                      initializer=self.bias_initializer,
                                                      regularizer=self.bias_regularizer,
                                                      constraint=self.bias_constraint,
                                                      trainable=True,
                                                      dtype=self.dtype,
                                                     )
        else:
            self.bias = None
        self.kernels_depth = input_channels // self.groups
        if self.kernels_depth == 0: self.kernels_depth = 1
        self.precalc_filters = tf.Variable(tf.zeros(shape=(*self.kernel_size, self.kernels_depth, self.filters)),
                                           trainable=False, name="precalc_filters",
                                        #    shape=tf.TensorShape(()),
                                           )    

    @property
    def kernel(self):
        if self.training: 
            self.has_to_generate = True
            return self.generate_kernel()
        else: 
            if self.has_to_generate:
                krnl = self.generate_kernel()
                self.precalc_filters.assign(krnl)
                self.has_to_generate = False
            return self.precalc_filters
    
    def generate_kernel(self):
        """Generates the functional kernel. Should be overrided. Must return them in shape (Kx, Ky, Cin//groups, Cout)"""
        raise NotImplementedError("Must implement a function to generate the functional kernel.")
    
    def call(self,
             inputs, # Inputs to the layer.
             training=None, # Tell the layer if the kernel has to be calculated again (training) or can be re-used (inference).
             ):
        """We overrode it to store the training variable."""
        self.training = training
        return super(FunctionalLayer, self).call(inputs)

    def show_filters(self,
                     show=True, # If we want to execute `plt.show()` or not.
                     **kwargs, # Key-word arguments to be pased to `plt.subplots`.
                     ):
        """Prints out the different filters in the kernel."""
        fig, axes = plt.subplots(self.kernels_depth, self.filters, squeeze=False, **kwargs)
        kernel = self.kernel
        for i in range(self.kernels_depth):
            for j in range(self.filters):
                axes[i,j].imshow(kernel[:,:,i,j])
                axes[i,j].axis("off")
        if show: plt.show()

# %% ../../Notebooks/Experimental/01_basic_functional_layer.ipynb 9
class GaussianLayer(FunctionalLayer):
    """Functional layer with gaussian kernels."""

    def __init__(self,
                 **kwargs,
                 ) -> None:
        super(GaussianLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        super(GaussianLayer, self).build(input_shape)
        self.sigma = tf.Variable(tf.random.uniform(shape=(self.kernels_depth*self.filters,), minval=0, maxval=self.kernel_size[0]),
                                 trainable=True,
                                 name="sigma")
        self.A = tf.Variable(tf.ones(shape=(self.kernels_depth*self.filters,)), trainable=True, name="A", dtype=tf.float32)
        self.xmean = tf.Variable(self.kernel_size[0]/2, trainable=False, name="xmean", dtype=tf.float32)
        self.ymean = tf.Variable(self.kernel_size[1]/2, trainable=False, name="ymean", dtype=tf.float32)    
        
    @tf.function
    def generate_kernel(self):
        """Generates a gaussian kernel based on the stored parameters."""
        gaussians = tf.TensorArray(dtype=tf.float32, size=self.sigma.shape[0])
        x, y = self.generate_dominion()
        for n in tf.range(start=0, limit=self.sigma.shape[0], delta=1, dtype=tf.int32):
            gaussian = self.gaussian(x, y, self.xmean, self.ymean, tf.gather(self.sigma, n), tf.gather(self.sigma, n), A=tf.gather(self.A, n))
            gaussians = gaussians.write(n, gaussian)
        gaussians = gaussians.stack()
        gaussians = rearrange(gaussians, "(Cin_groups Cout) Kx Ky -> Kx Ky Cin_groups Cout", Cin_groups=self.kernels_depth)
        return gaussians

    @staticmethod
    @tf.function
    def gaussian(x, # X coordinate.
                 y, # Y coordinate
                 xmean, # X mean.
                 ymean, # Y mean.
                 sigmax, # sigma^2 on the X direction.
                 sigmay, # sigma^2 on the Y direction.
                 A=1, # Amplitude factor of the gaussian.
                 normalize_prob=True, # Wether to normalize the integral to 1 or not.
                 ) -> float: # Returns the value for the point (x,y)
        A_norm = tf.convert_to_tensor((1/(2*np.pi*tf.sqrt(sigmax*sigmay)))) if normalize_prob else 1
        return A*A_norm*tf.exp(-(((xmean-x)**2)/(2*sigmax**2))-(((ymean-y)**2)/(2*sigmay**2)))

    @tf.function
    def generate_dominion(self,
                          ): # Returns two tensors, X and Y, to be passed into `gaussian`.
        """Generates the 2D dominion over which we want to calculate the gaussian."""
        range_x = tf.range(start=0, limit=self.kernel_size[0], delta=1, dtype=tf.float32)
        range_y = tf.range(start=0, limit=self.kernel_size[1], delta=1, dtype=tf.float32)
        return tf.meshgrid(range_x, range_y)        

# %% ../../Notebooks/Experimental/01_basic_functional_layer.ipynb 23
class GaborLayer(FunctionalLayer):
    """Functional layer with gabor kernels."""

    def __init__(self,
                 **kwargs,
                 ) -> None:
        super(GaborLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        super(GaborLayer, self).build(input_shape)
        self.fs = tf.Variable(self.kernel_size[0], trainable=False, name="fs", dtype=tf.float32)
        self.freq = tf.Variable(tf.random.uniform(shape=(self.kernels_depth*self.filters,), minval=0, maxval=self.fs/2),
                                trainable=True,
                                name="freq")
        self.logsigmax = tf.Variable(tf.random.uniform(shape=(self.kernels_depth*self.filters,), minval=0, maxval=tf.math.log(2/self.freq)),
                                  trainable=True,
                                  name="sigmax")
        self.logsigmay = tf.Variable(tf.random.uniform(shape=(self.kernels_depth*self.filters,), minval=0, maxval=tf.math.log(2/self.freq)),
                                  trainable=True,
                                  name="sigmay")

        self.theta = tf.Variable(tf.random.uniform(shape=(self.kernels_depth*self.filters,), minval=0, maxval=self.kernel_size[0]),
                                 trainable=True,
                                 name="theta")
        self.sigma_theta = tf.Variable(tf.random.uniform(shape=(self.kernels_depth*self.filters,), minval=0, maxval=self.kernel_size[0]),
                                       trainable=True,
                                       name="sigma_theta")
        self.rot_theta = tf.Variable(tf.random.uniform(shape=(self.kernels_depth*self.filters,), minval=0, maxval=self.kernel_size[0]),
                                     trainable=True,
                                     name="rot_theta")
        self.A = tf.Variable(tf.ones(shape=(self.kernels_depth*self.filters,)), trainable=True, name="A", dtype=tf.float32)
        self.xmean = tf.Variable(0.5, trainable=False, name="xmean", dtype=tf.float32)
        self.ymean = tf.Variable(0.5, trainable=False, name="ymean", dtype=tf.float32)
    
    @property
    def sigmax(self):
        return tf.math.exp(self.logsigmax)

    @property
    def sigmay(self):
        return tf.math.exp(self.logsigmay)

    @tf.function
    def generate_kernel(self):
        """Generates a gabor kernel based on the parameter sigma."""
        gabors = tf.TensorArray(dtype=tf.float32, size=self.logsigmax.shape[0])
        for n in tf.range(start=0, limit=self.logsigmax.shape[0], delta=1, dtype=tf.int32):
            gabor = self.gabor(self.kernel_size, 
                               self.xmean, self.ymean, 
                               tf.gather(self.sigmax, n), tf.gather(self.sigmay, n), 
                               tf.gather(self.freq, n),
                               tf.gather(self.theta, n), tf.gather(self.sigma_theta, n), tf.gather(self.rot_theta, n),
                               fs=self.fs,
                               A=tf.gather(self.A, n))
            gabors = gabors.write(n, gabor)
        gabors = gabors.stack()
        gabors = rearrange(gabors, "(Cin_groups Cout) Kx Ky -> Kx Ky Cin_groups Cout", Cin_groups=self.kernels_depth)
        return gabors

    @staticmethod
    @tf.function
    def gabor(kernel_size, xmean, ymean, sigmax, sigmay, freq, theta, sigma_theta, rot_theta, fs, A=1, normalize_prob=True):
        Nrows, Ncols = kernel_size
        
        int_x = Ncols/fs
        int_y = Nrows/fs

        fot_x = tf.linspace(0.0, int_x, Nrows+1)[:-1]
        fot_y = tf.linspace(0.0, int_y, Ncols+1)[:-1]
        x, y = tf.meshgrid(fot_x, fot_y, indexing='xy')

        x_r = tf.cos(rot_theta) * (x - xmean) - tf.sin(rot_theta) * (y - ymean)
        y_r = tf.sin(rot_theta) * (x - xmean) + tf.cos(rot_theta) * (y - ymean)
        return GaborLayer._gabor(x_r, y_r, xmean=xmean, ymean=ymean, sigmax=sigmax, sigmay=sigmay, freq=freq, theta=theta, sigma_theta=sigma_theta, A=A, normalize_prob=normalize_prob)

    @staticmethod
    @tf.function
    def _gabor(x, # X coordinate.
              y, # Y coordinate
              xmean, # X mean.
              ymean, # Y mean.
              sigmax, # sigma^2 on the X direction.
              sigmay, # sigma^2 on the Y direction.
              freq, # Frequency of the sinusoid.
              theta, # Orientation of the sinusoid.
              sigma_theta, # Orientation of the ??.
            #   rot_theta, # Rotation of the domain.
              A=1, # Amplitude factor.
              normalize_prob=True, # Wether to normalize the integral to 1 or not.
              ) -> float: # Returns the value for the point (x,y)
        sigma_vector = tf.convert_to_tensor([sigmax, sigmay])
        cov_matrix = tf.linalg.diag(sigma_vector)**2
        det_cov_matrix = tf.linalg.det(cov_matrix)
        A_norm = tf.convert_to_tensor((1/(2*np.pi*tf.sqrt(det_cov_matrix)))) if normalize_prob else 1

        rotation_matrix = tf.convert_to_tensor([[tf.cos(sigma_theta), -tf.sin(sigma_theta)],
                                                [tf.sin(sigma_theta), tf.cos(sigma_theta)]])
        rotated_covariance = rotation_matrix @ tf.linalg.inv(cov_matrix) @ tf.transpose(rotation_matrix)


        x_r_1 = rotated_covariance[0,0] * x + rotated_covariance[0,1] * y
        y_r_1 = rotated_covariance[1,0] * x + rotated_covariance[1,1] * y

        distance = x * x_r_1 + y * y_r_1

        gabor = A_norm * tf.exp(-distance/2) * tf.cos(2*np.pi*freq*(x*tf.cos(theta)+y*tf.sin(theta)))

        return A*gabor

    @tf.function
    def generate_dominion(self,
                          ): # Returns two tensors, X and Y, to be passed into `gabor`.
        """Generates the 2D dominion over which we want to calculate the gabor."""
        # range_x = tf.range(start=0, limit=self.kernel_size[0], delta=1, dtype=tf.float32)
        # range_y = tf.range(start=0, limit=self.kernel_size[1], delta=1, dtype=tf.float32)
        range_x = tf.linspace(start=0.0, stop=self.kernel_size[0]/self.fs, num=self.kernel_size[0]+1)[:-1]
        range_y = tf.linspace(start=0.0, stop=self.kernel_size[1]/self.fs, num=self.kernel_size[1]+1)[:-1]
        return tf.meshgrid(range_x, range_y)        

# %% ../../Notebooks/Experimental/01_basic_functional_layer.ipynb 33
class PreInitGaborLayer(GaborLayer):
    """Pre-initialized `GaborLayer`."""

    def __init__(self,
                 xmean,
                 ymean,
                 sigmax,
                 sigmay,
                 theta,
                 sigma_theta,
                 rot_theta,
                 freq,
                 fs,
                 **kwargs,
                 ):
        super(PreInitGaborLayer, self).__init__(**kwargs)
        self.xmean = xmean
        self.ymean = ymean
        self.logsigmax = tf.math.log(sigmax)
        self.logsigmay = tf.math.log(sigmay)
        self.theta = theta
        self.sigma_theta = sigma_theta
        self.rot_theta = rot_theta
        self.freq = freq
        self.fs = fs    

    def build(self,
              input_shape,
              ):
        super(PreInitGaborLayer, self).build(input_shape)
        self.xmean = tf.Variable(self.xmean, trainable=False, dtype=tf.float32, name="xmean")
        self.ymean = tf.Variable(self.ymean, trainable=False, dtype=tf.float32, name="ymean")
        self.logsigmax = tf.Variable(self.logsigmax, trainable=True, dtype=tf.float32, name="logsigmax")
        self.logsigmay = tf.Variable(self.logsigmay, trainable=True, dtype=tf.float32, name="logsigmay")
        self.theta = tf.Variable(self.theta, trainable=True, dtype=tf.float32, name="theta")
        self.sigma_theta = tf.Variable(self.sigma_theta, trainable=True, dtype=tf.float32, name="sigma_theta")
        self.rot_theta = tf.Variable(self.rot_theta, trainable=True, dtype=tf.float32, name="rot_theta")
        self.freq = tf.Variable(self.freq, trainable=True, dtype=tf.float32, name="freq")
        self.fs = tf.Variable(self.fs, trainable=True, dtype=tf.float32, name="fs")
        self.A = tf.Variable(tf.ones(shape=(self.kernels_depth*self.filters,)), trainable=True, name="A", dtype=tf.float32)

# %% ../../Notebooks/Experimental/01_basic_functional_layer.ipynb 37
class CenterSurroundLayer(GaussianLayer):
    """Center surround layer."""

    def __init__(self,
                 **kwargs,
                 ) -> None:
        super(CenterSurroundLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        super(CenterSurroundLayer, self).build(input_shape)
        self.sigma2 = tf.Variable(tf.random.uniform(shape=(self.kernels_depth*self.filters,), minval=0, maxval=self.kernel_size[0]),
                                  trainable=True,
                                  name="sigma2")

    @tf.function
    def generate_kernel(self):
        """Generates a gaussian kernel based on the parameter sigma."""
        gaussians = tf.TensorArray(dtype=tf.float32, size=self.sigma.shape[0])
        x, y = self.generate_dominion()
        for n in tf.range(start=0, limit=self.sigma.shape[0], delta=1, dtype=tf.int32):
            gaussian = self.gaussian(x, y, self.xmean, self.ymean, tf.gather(self.sigma, n), tf.gather(self.sigma, n), A=1)
            gaussian2 = self.gaussian(x, y, self.xmean, self.ymean, tf.gather(self.sigma2, n), tf.gather(self.sigma2, n), A=1)
            gaussians = gaussians.write(n, tf.gather(self.A, n)*(gaussian-gaussian2))
        gaussians = gaussians.stack()
        gaussians = rearrange(gaussians, "(Cin_groups Cout) Kx Ky -> Kx Ky Cin_groups Cout", Cin_groups=self.kernels_depth)
        return gaussians

# %% ../../Notebooks/Experimental/01_basic_functional_layer.ipynb 40
class EnergyConserver(layers.Layer):
    """Preserves the energy at the input and output of a layer."""
    
    def __init__(self,
                 layer, # Layer to wrap. Not instanciated.
                 **kwargs, # Key-word arguments to be passed at layer definition.
                 ) -> None:
        super(EnergyConserver, self).__init__()
        self.layer = layer(**kwargs)
    
    def build(self,
              input_shape,
              ) -> None:
        self.layer.build(input_shape)

    def call(self,
             inputs,
             **kwargs,
             ):
        inputs_energy = self.obtain_energy(inputs)
        outputs = self.layer(inputs, **kwargs)
        outputs_energy = self.obtain_energy(outputs)
        return tf.sqrt(inputs_energy/outputs_energy)*outputs

    @staticmethod
    def obtain_energy(inputs, keepdims=True):
        return tf.reduce_sum(inputs**2, axis=tf.range(start=1, limit=tf.rank(inputs), dtype=tf.int32), keepdims=keepdims)
