# AUTOGENERATED! DO NOT EDIT! File to edit: ../../Notebooks/Experimental/00_simple_gaussian.ipynb.

# %% auto 0
__all__ = ['SimpleGaussian', 'GDNConv']

# %% ../../Notebooks/Experimental/00_simple_gaussian.ipynb 3
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
from einops import rearrange, repeat, reduce

# %% ../../Notebooks/Experimental/00_simple_gaussian.ipynb 13
class SimpleGaussian(layers.Layer):
    """Simple gaussian layer."""
    
    def __init__(self,
                 filters,
                 kernel_size,
                 A=1, # Height of the peak of the Gaussian.
                 strides=1, # Strides of the convolution.
                 padding="SAME", # Can be "SAME" or "VALID".
                 normalize=True, # Wether to normalize the output filters or not.
                 **kwargs,
                 ):
        super(SimpleGaussian, self).__init__(**kwargs)
        self.filters = filters
        self.kernel_size = kernel_size
        self.A = tf.cast(A, dtype=tf.float32)
        self.strides = strides
        self.padding = padding
        self.normalize = normalize

    def build(self,
              input_shape,
              ):
        self.x0 = tf.Variable([self.kernel_size//2]*self.filters, trainable=True, name="x0", dtype=tf.float32)
        self.y0 = tf.Variable([self.kernel_size//2]*self.filters, trainable=True, name="y0", dtype=tf.float32)
        self.logsigma_x = tf.Variable(np.random.uniform(low=-2, high=2, size=self.filters), trainable=True, name="logsigma_x", dtype=tf.float32)
        self.logsigma_y = tf.Variable(np.random.uniform(low=-2, high=2, size=self.filters), trainable=True, name="logsigma_y", dtype=tf.float32)

    @property
    def sigma_x(self):
        return tf.math.exp(self.logsigma_x)

    @property
    def sigma_y(self):
        return tf.math.exp(self.logsigma_y)

    def call(self,
             X,
             training=False,
             ):
        
        ## 1. Build the filters (n_filters, kernel_size, kernel_size)
        filters = self.build_filters()

        ## 2. Reshape so they match what Keras expects (Kx, Ky, Cin, Cout)
        filters = repeat(filters, "n_filters kx ky -> kx ky Cin n_filters", Cin=X.shape[-1])

        return tf.nn.conv2d(X, filters, strides=self.strides, padding=self.padding)
    
    @tf.function
    def _build_filter(self,
                      x, # X domain.
                      y, # Y domain.
                      x0, # Mean of X.
                      y0, # Mean of Y.
                      sigma_x, # Width of X.
                      sigma_y, # Width of Y.
                      A, # Height of the peak.
                      ):
        """Builds the gaussian filters using the parameters of the layer."""

        ## 2. Build the gaussian
        if self.normalize: A = 1/(2*3.14*sigma_x*sigma_y)
        return A*tf.math.exp(-((x-x0)**2/(2*sigma_x**2)+(y-y0)**2/(2*sigma_y**2)))

    @tf.function
    def build_filters(self):
        ## 1. Build domain
        x, y = tf.meshgrid(tf.range(self.kernel_size), tf.range(self.kernel_size))
        x, y = tf.cast(x, dtype=tf.float32), tf.cast(y, dtype=tf.float32)
        
        ## 2. Initialize empty `TensorArray`
        filters = tf.TensorArray(dtype = tf.float32, size = self.filters)

        for n in tf.range(start=0, limit=self.filters, dtype=tf.int32):
            filters = filters.write(n, 
                                    self._build_filter(x, y, 
                                                       tf.gather(self.x0, n), tf.gather(self.y0, n), 
                                                       tf.gather(self.sigma_x, n), tf.gather(self.sigma_y, n),
                                                       self.A))

        filters = filters.stack()
        # Normalize the filters
        # if self.normalize: 
        #     max_per_filter = reduce(filters, "filters Ncols Nrows -> filters () ()", "max")
        #     min_per_filter = reduce(filters, "filters Ncols Nrows -> filters () ()", "min")
        #     filters = (filters-min_per_filter)/(max_per_filter-min_per_filter)
        return filters

    def show_filters(self,
                     show=True, # Wether to do plt.show() or not.
                     ):
        """Plots the filters created with the current parameters."""
        ncols = int(np.sqrt(self.filters))
        nrows = int(self.filters-ncols)
        if nrows == 0: nrows == 1
        filters = self.build_filters().numpy()
        fig, axes = plt.subplots(ncols, nrows)
        for filter, ax in zip(filters, axes.ravel()):
            ax.imshow(filter)
            ax.axis("off")
        if show: plt.show()
        

# %% ../../Notebooks/Experimental/00_simple_gaussian.ipynb 78
class GDNConv(layers.Layer):
    """GDN that takes as input a specific layer to use."""

    def __init__(self,
                 layer, # Layer to be used to extract the normalization.
                 **kwargs,
                 ):
        super(GDNConv, self).__init__(**kwargs)
        self.layer = layer

    def build(self,
              input_shape,
              ):
        self.layer.build(input_shape)
        self.alpha = tf.Variable(2, trainable=False, name="alpha", dtype=tf.float32)
        self.epsilon = tf.Variable(1/2, trainable=False, name="epsilon", dtype=tf.float32)
        # self.beta = tf.Variable(1, trainable=True, name="beta", dtype=tf.float32)

    def call(self,
             X,
             training=False,
             ):
        norm = tf.math.pow(X, self.alpha)
        norm = self.layer(norm, training=training)#+self.beta
        norm = tf.clip_by_value(norm, clip_value_min=1e-5, clip_value_max=tf.reduce_max(norm))
        norm = tf.math.pow(norm, self.epsilon)
        return X / norm
