[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "flayers",
    "section": "",
    "text": "git clone https://github.com/Jorgvt/flayers.git\ncd flayers\npip install -e .\npip install flayers"
  },
  {
    "objectID": "callbacks.html",
    "href": "callbacks.html",
    "title": "Callbacks",
    "section": "",
    "text": "Logging Gabor parameters into wandb\n\nWe found that sometimes, during training, an error would rise regarding the inverse of the matrix during the calculation of the Gabor filters. Our first thought was that the covariance matrix (calculated with the parameters sigma_i and sigma_j) wasn’t invertible, meaning that sigma_i and sigma_j were non-positive, but a constraint on the variables did not fix the problem. To inspect it in more detail, we are going to log all the layer’s weights during training to wandb to try and find the root of the problem.\n\nTo avoid introducing dependencies that won’t be used by many people, we can put the import wandb in the instantiation of the callback.\n\n\nsource\n\n\n\n GaborLayerLogger ()\n\nAbstract base class used to build new callbacks.\nCallbacks can be passed to keras methods such as fit, evaluate, and predict in order to hook into the various stages of the model training and inference lifecycle.\nTo create a custom callback, subclass keras.callbacks.Callback and override the method associated with the stage of interest. See https://www.tensorflow.org/guide/keras/custom_callback for more information.\nExample:\n\n\n\ntraining_finished = False class MyCallback(tf.keras.callbacks.Callback): … def on_train_end(self, logs=None): … global training_finished … training_finished = True model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))]) model.compile(loss=‘mean_squared_error’) model.fit(tf.constant([[1.0]]), tf.constant([[1.0]]), … callbacks=[MyCallback()]) assert training_finished == True\n\n\n\nIf you want to use Callback objects in a custom training loop:\n\nYou should pack all your callbacks into a single callbacks.CallbackList so they can all be called together.\nYou will need to manually call all the on_* methods at the apropriate locations in your loop. Like this:\ncallbacks =  tf.keras.callbacks.CallbackList([...])\ncallbacks.append(...)\n\ncallbacks.on_train_begin(...)\nfor epoch in range(EPOCHS):\n  callbacks.on_epoch_begin(epoch)\n  for i, data in dataset.enumerate():\n    callbacks.on_train_batch_begin(i)\n    batch_logs = model.train_step(data)\n    callbacks.on_train_batch_end(i, batch_logs)\n  epoch_logs = ...\n  callbacks.on_epoch_end(epoch, epoch_logs)\nfinal_logs=...\ncallbacks.on_train_end(final_logs)\n\nAttributes: params: Dict. Training parameters (eg. verbosity, batch size, number of epochs…). model: Instance of keras.models.Model. Reference of the model being trained.\nThe logs dictionary that callback methods take as argument will contain keys for quantities relevant to the current batch or epoch (see method-specific docstrings).\n\nsource\n\n\n\n\n GaborLayerSeqLogger ()\n\nAbstract base class used to build new callbacks.\nCallbacks can be passed to keras methods such as fit, evaluate, and predict in order to hook into the various stages of the model training and inference lifecycle.\nTo create a custom callback, subclass keras.callbacks.Callback and override the method associated with the stage of interest. See https://www.tensorflow.org/guide/keras/custom_callback for more information.\nExample:\n\n\n\ntraining_finished = False class MyCallback(tf.keras.callbacks.Callback): … def on_train_end(self, logs=None): … global training_finished … training_finished = True model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))]) model.compile(loss=‘mean_squared_error’) model.fit(tf.constant([[1.0]]), tf.constant([[1.0]]), … callbacks=[MyCallback()]) assert training_finished == True\n\n\n\nIf you want to use Callback objects in a custom training loop:\n\nYou should pack all your callbacks into a single callbacks.CallbackList so they can all be called together.\nYou will need to manually call all the on_* methods at the apropriate locations in your loop. Like this:\ncallbacks =  tf.keras.callbacks.CallbackList([...])\ncallbacks.append(...)\n\ncallbacks.on_train_begin(...)\nfor epoch in range(EPOCHS):\n  callbacks.on_epoch_begin(epoch)\n  for i, data in dataset.enumerate():\n    callbacks.on_train_batch_begin(i)\n    batch_logs = model.train_step(data)\n    callbacks.on_train_batch_end(i, batch_logs)\n  epoch_logs = ...\n  callbacks.on_epoch_end(epoch, epoch_logs)\nfinal_logs=...\ncallbacks.on_train_end(final_logs)\n\nAttributes: params: Dict. Training parameters (eg. verbosity, batch size, number of epochs…). model: Instance of keras.models.Model. Reference of the model being trained.\nThe logs dictionary that callback methods take as argument will contain keys for quantities relevant to the current batch or epoch (see method-specific docstrings).\nLet’s check if it logs the parameters appropriately:"
  },
  {
    "objectID": "callbacks.html#definition-of-simple-model",
    "href": "callbacks.html#definition-of-simple-model",
    "title": "Callbacks",
    "section": "Definition of simple model",
    "text": "Definition of simple model\n\nmodel = tf.keras.Sequential([\n    RandomGabor(n_gabors=4, size=20, input_shape=(28,28,1)),\n    layers.MaxPool2D(2),\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nrandom_gabor_3 (RandomGabor) (None, 28, 28, 4)         1626      \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 14, 14, 4)         0         \n_________________________________________________________________\nglobal_average_pooling2d_1 ( (None, 4)                 0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                50        \n=================================================================\nTotal params: 1,676\nTrainable params: 76\nNon-trainable params: 1,600\n_________________________________________________________________\n\n\n\nconfig = {\n    \"epochs\":5,\n    \"batch_size\":64,\n}\n\n\nwandb.init(project=\"Testing\",\n           config=config)\nconfig = wandb.config\n\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: Currently logged in as: jorgvt (use `wandb login --relogin` to force relogin)\n\n\nwandb version 0.13.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.12.11\n\n\nRun data is saved locally in /media/disk/users-muten/vitojor/flayers/Notebooks/wandb/run-20220921_125140-lk0tpm9i\n\n\nSyncing run smooth-donkey-1 to Weights & Biases (docs)\n\n\n\nmodel.fit(X_train, Y_train, epochs=config.epochs, batch_size=config.batch_size, callbacks=[GaborLayerLogger()])\n\n\nwandb.finish()\n\n\n\n\nWaiting for W&B process to finish... (success)."
  },
  {
    "objectID": "constraints.html",
    "href": "constraints.html",
    "title": "Constraints",
    "section": "",
    "text": "source\n\nPositive\n\n Positive ()\n\nConstraint to force positive weights in a layer."
  },
  {
    "objectID": "Experiments/03_multiple_randomgabor.html",
    "href": "Experiments/03_multiple_randomgabor.html",
    "title": "Multiple Random Gabor experiment",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom einops import rearrange, repeat\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import mnist\n\nfrom flayers.layers import RandomGabor"
  },
  {
    "objectID": "Experiments/03_multiple_randomgabor.html#data-loading",
    "href": "Experiments/03_multiple_randomgabor.html#data-loading",
    "title": "Multiple Random Gabor experiment",
    "section": "Data loading",
    "text": "Data loading\n\nWe will be using MNIST for a simple and quick test.\n\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\nX_train = repeat(X_train, \"b h w ->  b h w c\", c=1)/255.0\nX_test = repeat(X_test, \"b h w ->  b h w c\", c=1)/255.0\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\n((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
  },
  {
    "objectID": "Experiments/03_multiple_randomgabor.html#definition-of-simple-model",
    "href": "Experiments/03_multiple_randomgabor.html#definition-of-simple-model",
    "title": "Multiple Random Gabor experiment",
    "section": "Definition of simple model",
    "text": "Definition of simple model\n\nmodel = tf.keras.Sequential([\n    RandomGabor(n_gabors=4, size=20, input_shape=(28,28,1)),\n    layers.MaxPool2D(2),\n    RandomGabor(n_gabors=4, size=20),\n    layers.MaxPool2D(2),\n    RandomGabor(n_gabors=4, size=20),\n    layers.MaxPool2D(2),\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\n2022-09-20 12:47:21.299667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5435 MB memory:  -> device: 0, name: NVIDIA GeForce GTX TITAN Black, pci bus id: 0000:83:00.0, compute capability: 3.5\n2022-09-20 12:47:23.872491: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-09-20 12:47:24.116626: I tensorflow/core/util/cuda_solvers.cc:180] Creating CudaSolver handles for stream 0x5566d7fd5170\n\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nrandom_gabor_1 (RandomGabor) (None, 28, 28, 4)         1626      \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 14, 14, 4)         0         \n_________________________________________________________________\nrandom_gabor_3 (RandomGabor) (None, 14, 14, 4)         1626      \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 7, 7, 4)           0         \n_________________________________________________________________\nrandom_gabor_5 (RandomGabor) (None, 7, 7, 4)           1626      \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 3, 3, 4)           0         \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 4)                 0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                50        \n=================================================================\nTotal params: 4,928\nTrainable params: 128\nNon-trainable params: 4,800\n_________________________________________________________________\n\n\nWe can show the initial Gabor filters:\n\nmodel.layers[0].show_filters()\n\n\n\n\n\nmodel.layers[2].show_filters()\n\n\n\n\n\nmodel.layers[4].show_filters()\n\n\n\n\n\nhistory = model.fit(X_train, Y_train, batch_size=128, epochs=2, validation_split=0.2)\n\nEpoch 1/2\n\n\n2022-09-20 12:47:43.857300: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n2022-09-20 12:47:44.350354: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n\n\n375/375 [==============================] - 51s 96ms/step - loss: 2.2364 - accuracy: 0.1946 - val_loss: 2.0396 - val_accuracy: 0.2788\nEpoch 2/2\n375/375 [==============================] - 36s 96ms/step - loss: 1.9230 - accuracy: 0.3473 - val_loss: 1.7555 - val_accuracy: 0.4182\n\n\nShowing the training dynamics:\n\nfig, axes = plt.subplots(1,2, figsize=(9,4))\naxes[0].plot(history.history['loss'][1:], label=\"Train\")\naxes[0].plot(history.history['val_loss'][1:], label=\"Val\")\naxes[1].plot(history.history['accuracy'], label=\"Train\")\naxes[1].plot(history.history['val_accuracy'], label=\"Val\")\nplt.legend()\nplt.show()\n\n\n\n\nCalculate the metrics in the test set:\n\nmodel.evaluate(X_test, Y_test, batch_size=128)\n\n79/79 [==============================] - 0s 4ms/step - loss: 1.7527 - accuracy: 0.4212\n\n\n[1.752727746963501, 0.4212000072002411]\n\n\nWe can visualize the gabor filters after the training process:\n\nmodel.layers[0].show_filters()\n\n\n\n\n\nmodel.layers[2].show_filters()\n\n\n\n\n\nmodel.layers[4].show_filters()"
  },
  {
    "objectID": "Experiments/01_randomgabor.html",
    "href": "Experiments/01_randomgabor.html",
    "title": "Random Gabor experiment",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom einops import rearrange, repeat\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import mnist\n\nfrom flayers.layers import RandomGabor"
  },
  {
    "objectID": "Experiments/01_randomgabor.html#data-loading",
    "href": "Experiments/01_randomgabor.html#data-loading",
    "title": "Random Gabor experiment",
    "section": "Data loading",
    "text": "Data loading\n\nWe will be using MNIST for a simple and quick test.\n\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\nX_train = repeat(X_train, \"b h w ->  b h w c\", c=1)/255.0\nX_test = repeat(X_test, \"b h w ->  b h w c\", c=1)/255.0\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\n((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
  },
  {
    "objectID": "Experiments/01_randomgabor.html#definition-of-simple-model",
    "href": "Experiments/01_randomgabor.html#definition-of-simple-model",
    "title": "Random Gabor experiment",
    "section": "Definition of simple model",
    "text": "Definition of simple model\n\nmodel = tf.keras.Sequential([\n    RandomGabor(n_gabors=4, size=20, input_shape=(28,28,1)),\n    # layers.Conv2D(32, 3, input_shape=(28,28,1)),\n    layers.MaxPool2D(2),\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\n2022-09-18 23:02:18.375631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5290 MB memory:  -> device: 0, name: NVIDIA GeForce GTX TITAN Black, pci bus id: 0000:83:00.0, compute capability: 3.5\n2022-09-18 23:02:21.015730: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-09-18 23:02:21.299468: I tensorflow/core/util/cuda_solvers.cc:180] Creating CudaSolver handles for stream 0x5595438552e0\n\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nrandom_gabor (RandomGabor)   (None, 28, 28, 4)         1626      \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 14, 14, 4)         0         \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 4)                 0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                50        \n=================================================================\nTotal params: 1,676\nTrainable params: 76\nNon-trainable params: 1,600\n_________________________________________________________________\n\n\nWe can show the initial Gabor filters:\n\nmodel.layers[0].show_filters()\n\n\n\n\n\nhistory = model.fit(X_train, Y_train, batch_size=128, epochs=1, validation_split=0.2)\n\n2022-09-18 23:02:30.164449: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n2022-09-18 23:02:30.665826: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n\n\n375/375 [==============================] - 21s 38ms/step - loss: 2.3070 - accuracy: 0.1080 - val_loss: 2.2878 - val_accuracy: 0.1509\n\n\nWe can visualize the gabor filters after the training process:\n\nmodel.layers[0].show_filters()"
  },
  {
    "objectID": "Experiments/00_gaborlayer.html",
    "href": "Experiments/00_gaborlayer.html",
    "title": "Gabor layer experiment",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom einops import rearrange, repeat\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import mnist\n\nfrom flayers.layers import GaborLayer"
  },
  {
    "objectID": "Experiments/00_gaborlayer.html#data-loading",
    "href": "Experiments/00_gaborlayer.html#data-loading",
    "title": "Gabor layer experiment",
    "section": "Data loading",
    "text": "Data loading\n\nWe will be using MNIST for a simple and quick test.\n\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\nX_train = repeat(X_train, \"b h w ->  b h w c\", c=1)/255.0\nX_test = repeat(X_test, \"b h w ->  b h w c\", c=1)/255.0\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\n((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
  },
  {
    "objectID": "Experiments/00_gaborlayer.html#definition-of-simple-model",
    "href": "Experiments/00_gaborlayer.html#definition-of-simple-model",
    "title": "Gabor layer experiment",
    "section": "Definition of simple model",
    "text": "Definition of simple model\n\nn_gabors = 4\nsigma_i = [0.1, 0.2]*2\nsigma_j = [0.2, 0.1]*2\nfreq = [10, 10]*2\ntheta = [0, np.pi/2]*2\nrot_theta = [0, 0]*2\nsigma_theta = [0, 0]*2\n\n\nmodel = tf.keras.Sequential([\n    GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n               theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20, input_shape=(28,28,1)),\n    layers.MaxPool2D(2),\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ngabor_layer_1 (GaborLayer)   (None, 28, 28, 4)         1626      \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 14, 14, 4)         0         \n_________________________________________________________________\nglobal_average_pooling2d_1 ( (None, 4)                 0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                50        \n=================================================================\nTotal params: 1,676\nTrainable params: 76\nNon-trainable params: 1,600\n_________________________________________________________________\n\n\nWe can show the initial Gabor filters:\n\nmodel.layers[0].show_filters()\n\n\n\n\n\nhistory = model.fit(X_train, Y_train, batch_size=128, epochs=1, validation_split=0.2)\n\n375/375 [==============================] - 17s 33ms/step - loss: 2.2897 - accuracy: 0.1481 - val_loss: 2.2693 - val_accuracy: 0.1996\n\n\nWe can visualize the gabor filters after the training process:\n\nmodel.layers[0].show_filters()\n\n\n\n\nWe can even check the atributes of the layer to inspect the change in the initial parameters:\n\nmodel.layers[0].theta.numpy()*180/np.pi\n\narray([-1.3238928, 90.20859  , -2.235493 , 88.482956 ], dtype=float32)\n\n\n\nmodel.layers[0].rot_theta.numpy()*180/np.pi\n\narray([-1.3243861 ,  0.27530777,  2.329611  , -1.5420123 ], dtype=float32)\n\n\n\nmodel.layers[0].sigma_theta.numpy()*180/np.pi\n\narray([-1.3520143, -4.6940336,  5.102634 , -1.5307682], dtype=float32)"
  },
  {
    "objectID": "Experiments/05_multiple_randomgabor_relu.html",
    "href": "Experiments/05_multiple_randomgabor_relu.html",
    "title": "Multiple Random Gabor experiment",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom einops import rearrange, repeat\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import mnist\n\nfrom flayers.layers import RandomGabor"
  },
  {
    "objectID": "Experiments/05_multiple_randomgabor_relu.html#data-loading",
    "href": "Experiments/05_multiple_randomgabor_relu.html#data-loading",
    "title": "Multiple Random Gabor experiment",
    "section": "Data loading",
    "text": "Data loading\n\nWe will be using MNIST for a simple and quick test.\n\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\nX_train = repeat(X_train, \"b h w ->  b h w c\", c=1)/255.0\nX_test = repeat(X_test, \"b h w ->  b h w c\", c=1)/255.0\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\n((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
  },
  {
    "objectID": "Experiments/05_multiple_randomgabor_relu.html#definition-of-simple-model",
    "href": "Experiments/05_multiple_randomgabor_relu.html#definition-of-simple-model",
    "title": "Multiple Random Gabor experiment",
    "section": "Definition of simple model",
    "text": "Definition of simple model\n\nmodel = tf.keras.Sequential([\n    RandomGabor(n_gabors=4, size=20, input_shape=(28,28,1)),\n    layers.ReLU(),\n    layers.MaxPool2D(2),\n    RandomGabor(n_gabors=4, size=10),\n    layers.ReLU(),\n    layers.MaxPool2D(2),\n    RandomGabor(n_gabors=4, size=5),\n    layers.ReLU(),\n    layers.MaxPool2D(2),\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\n2022-09-08 12:06:36.641715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5435 MB memory:  -> device: 0, name: NVIDIA GeForce GTX TITAN Black, pci bus id: 0000:83:00.0, compute capability: 3.5\n\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nrandom_gabor (RandomGabor)   (None, 28, 28, 4)         26        \n_________________________________________________________________\nre_lu (ReLU)                 (None, 28, 28, 4)         0         \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 14, 14, 4)         0         \n_________________________________________________________________\nrandom_gabor_1 (RandomGabor) (None, 14, 14, 4)         26        \n_________________________________________________________________\nre_lu_1 (ReLU)               (None, 14, 14, 4)         0         \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 7, 7, 4)           0         \n_________________________________________________________________\nrandom_gabor_2 (RandomGabor) (None, 7, 7, 4)           26        \n_________________________________________________________________\nre_lu_2 (ReLU)               (None, 7, 7, 4)           0         \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 3, 3, 4)           0         \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 4)                 0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                50        \n=================================================================\nTotal params: 128\nTrainable params: 128\nNon-trainable params: 0\n_________________________________________________________________\n\n\nWe can show the initial Gabor filters:\n\nmodel.layers[0].show_filters()\n\n2022-09-08 12:06:40.240924: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-09-08 12:06:40.486401: I tensorflow/core/util/cuda_solvers.cc:180] Creating CudaSolver handles for stream 0x55a3b2715f00\n\n\n\n\n\n\nmodel.layers[3].show_filters()\n\n\n\n\n\nmodel.layers[6].show_filters()\n\n\n\n\n\nhistory = model.fit(X_train, Y_train, batch_size=128, epochs=100, validation_split=0.2)\n\nEpoch 1/100\n\n\n2022-09-08 12:07:26.699074: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n2022-09-08 12:07:27.154195: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n\n\n375/375 [==============================] - 44s 83ms/step - loss: 72.2522 - accuracy: 0.1398 - val_loss: 2.7318 - val_accuracy: 0.1739\nEpoch 2/100\n375/375 [==============================] - 29s 77ms/step - loss: 2.5426 - accuracy: 0.1832 - val_loss: 2.3943 - val_accuracy: 0.1903\nEpoch 3/100\n375/375 [==============================] - 30s 81ms/step - loss: 2.3536 - accuracy: 0.1959 - val_loss: 2.2703 - val_accuracy: 0.2134\nEpoch 4/100\n375/375 [==============================] - 29s 76ms/step - loss: 2.2619 - accuracy: 0.2111 - val_loss: 2.2346 - val_accuracy: 0.2205\nEpoch 5/100\n375/375 [==============================] - 29s 76ms/step - loss: 2.1907 - accuracy: 0.2223 - val_loss: 2.1368 - val_accuracy: 0.2283\nEpoch 6/100\n375/375 [==============================] - 29s 76ms/step - loss: 2.1296 - accuracy: 0.2389 - val_loss: 2.0727 - val_accuracy: 0.2532\nEpoch 7/100\n375/375 [==============================] - 29s 77ms/step - loss: 2.0777 - accuracy: 0.2534 - val_loss: 2.0683 - val_accuracy: 0.2573\nEpoch 8/100\n375/375 [==============================] - 29s 77ms/step - loss: 2.0325 - accuracy: 0.2734 - val_loss: 1.9766 - val_accuracy: 0.2992\nEpoch 9/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.9893 - accuracy: 0.2948 - val_loss: 1.9291 - val_accuracy: 0.3220\nEpoch 10/100\n375/375 [==============================] - 29s 76ms/step - loss: 1.9523 - accuracy: 0.3144 - val_loss: 1.8903 - val_accuracy: 0.3455\nEpoch 11/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.9158 - accuracy: 0.3333 - val_loss: 1.8665 - val_accuracy: 0.3576\nEpoch 12/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.8832 - accuracy: 0.3468 - val_loss: 1.8170 - val_accuracy: 0.3795\nEpoch 13/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.8468 - accuracy: 0.3625 - val_loss: 1.7824 - val_accuracy: 0.3868\nEpoch 14/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.8113 - accuracy: 0.3763 - val_loss: 1.7485 - val_accuracy: 0.3966\nEpoch 15/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.7753 - accuracy: 0.3881 - val_loss: 1.7290 - val_accuracy: 0.4121\nEpoch 16/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.7420 - accuracy: 0.4003 - val_loss: 1.6898 - val_accuracy: 0.4283\nEpoch 17/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.6988 - accuracy: 0.4162 - val_loss: 1.6385 - val_accuracy: 0.4499\nEpoch 18/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.6605 - accuracy: 0.4361 - val_loss: 1.5921 - val_accuracy: 0.4667\nEpoch 19/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.6065 - accuracy: 0.4590 - val_loss: 1.5227 - val_accuracy: 0.4949\nEpoch 20/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.5208 - accuracy: 0.4929 - val_loss: 1.4289 - val_accuracy: 0.5296\nEpoch 21/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.4161 - accuracy: 0.5218 - val_loss: 1.3047 - val_accuracy: 0.5573\nEpoch 22/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.3149 - accuracy: 0.5512 - val_loss: 1.2797 - val_accuracy: 0.5631\nEpoch 23/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.2306 - accuracy: 0.5781 - val_loss: 1.1715 - val_accuracy: 0.5974\nEpoch 24/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.1721 - accuracy: 0.6013 - val_loss: 1.0977 - val_accuracy: 0.6281\nEpoch 25/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.1540 - accuracy: 0.6081 - val_loss: 1.0598 - val_accuracy: 0.6478\nEpoch 26/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.1052 - accuracy: 0.6251 - val_loss: 1.0746 - val_accuracy: 0.6286\nEpoch 27/100\n375/375 [==============================] - 29s 76ms/step - loss: 1.0857 - accuracy: 0.6344 - val_loss: 1.0419 - val_accuracy: 0.6498\nEpoch 28/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.0805 - accuracy: 0.6365 - val_loss: 1.0425 - val_accuracy: 0.6453\nEpoch 29/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.0480 - accuracy: 0.6491 - val_loss: 1.0290 - val_accuracy: 0.6475\nEpoch 30/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.0522 - accuracy: 0.6472 - val_loss: 0.9822 - val_accuracy: 0.6736\nEpoch 31/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.0398 - accuracy: 0.6549 - val_loss: 0.9516 - val_accuracy: 0.6887\nEpoch 32/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.0281 - accuracy: 0.6577 - val_loss: 0.9446 - val_accuracy: 0.6825\nEpoch 33/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.0285 - accuracy: 0.6566 - val_loss: 0.9623 - val_accuracy: 0.6671\nEpoch 34/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.0102 - accuracy: 0.6639 - val_loss: 0.9425 - val_accuracy: 0.6833\nEpoch 35/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.0110 - accuracy: 0.6634 - val_loss: 1.0055 - val_accuracy: 0.6603\nEpoch 36/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.0061 - accuracy: 0.6651 - val_loss: 0.9460 - val_accuracy: 0.6768\nEpoch 37/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.0029 - accuracy: 0.6631 - val_loss: 0.9528 - val_accuracy: 0.6789\nEpoch 38/100\n375/375 [==============================] - 29s 76ms/step - loss: 1.0100 - accuracy: 0.6606 - val_loss: 1.0218 - val_accuracy: 0.6573\nEpoch 39/100\n375/375 [==============================] - 29s 76ms/step - loss: 0.9972 - accuracy: 0.6646 - val_loss: 1.0173 - val_accuracy: 0.6571\nEpoch 40/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.0011 - accuracy: 0.6624 - val_loss: 0.9416 - val_accuracy: 0.6853\nEpoch 41/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9854 - accuracy: 0.6705 - val_loss: 0.9618 - val_accuracy: 0.6740\nEpoch 42/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9939 - accuracy: 0.6640 - val_loss: 0.9237 - val_accuracy: 0.6837\nEpoch 43/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9993 - accuracy: 0.6624 - val_loss: 0.9294 - val_accuracy: 0.6919\nEpoch 44/100\n375/375 [==============================] - 29s 77ms/step - loss: 0.9923 - accuracy: 0.6644 - val_loss: 0.9168 - val_accuracy: 0.6864\nEpoch 45/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9947 - accuracy: 0.6643 - val_loss: 0.9236 - val_accuracy: 0.6819\nEpoch 46/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9823 - accuracy: 0.6699 - val_loss: 0.9321 - val_accuracy: 0.6806\nEpoch 47/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9965 - accuracy: 0.6628 - val_loss: 0.9290 - val_accuracy: 0.6889\nEpoch 48/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9854 - accuracy: 0.6664 - val_loss: 0.9273 - val_accuracy: 0.6888\nEpoch 49/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9810 - accuracy: 0.6697 - val_loss: 0.9430 - val_accuracy: 0.6790\nEpoch 50/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9804 - accuracy: 0.6695 - val_loss: 0.9362 - val_accuracy: 0.6916\nEpoch 51/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9837 - accuracy: 0.6665 - val_loss: 0.9167 - val_accuracy: 0.6879\nEpoch 52/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9890 - accuracy: 0.6671 - val_loss: 0.9152 - val_accuracy: 0.6937\nEpoch 53/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9709 - accuracy: 0.6731 - val_loss: 0.9280 - val_accuracy: 0.6895\nEpoch 54/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9793 - accuracy: 0.6698 - val_loss: 0.9112 - val_accuracy: 0.6961\nEpoch 55/100\n375/375 [==============================] - 29s 77ms/step - loss: 0.9699 - accuracy: 0.6737 - val_loss: 0.9613 - val_accuracy: 0.6847\nEpoch 56/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9747 - accuracy: 0.6721 - val_loss: 0.9234 - val_accuracy: 0.7009\nEpoch 57/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9744 - accuracy: 0.6719 - val_loss: 0.9092 - val_accuracy: 0.6988\nEpoch 58/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9682 - accuracy: 0.6736 - val_loss: 0.9387 - val_accuracy: 0.6836\nEpoch 59/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9770 - accuracy: 0.6707 - val_loss: 0.9507 - val_accuracy: 0.6712\nEpoch 60/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9757 - accuracy: 0.6717 - val_loss: 0.8992 - val_accuracy: 0.7005\nEpoch 61/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9748 - accuracy: 0.6733 - val_loss: 0.9299 - val_accuracy: 0.6841\nEpoch 62/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9720 - accuracy: 0.6718 - val_loss: 0.9042 - val_accuracy: 0.7007\nEpoch 63/100\n375/375 [==============================] - 29s 76ms/step - loss: 0.9681 - accuracy: 0.6752 - val_loss: 0.9029 - val_accuracy: 0.6945\nEpoch 64/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9641 - accuracy: 0.6769 - val_loss: 0.9030 - val_accuracy: 0.6973\nEpoch 65/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9641 - accuracy: 0.6763 - val_loss: 0.9218 - val_accuracy: 0.6927\nEpoch 66/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9630 - accuracy: 0.6777 - val_loss: 0.9043 - val_accuracy: 0.6955\nEpoch 67/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9764 - accuracy: 0.6725 - val_loss: 0.9018 - val_accuracy: 0.6957\nEpoch 68/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9608 - accuracy: 0.6779 - val_loss: 0.8989 - val_accuracy: 0.7021\nEpoch 69/100\n375/375 [==============================] - 29s 76ms/step - loss: 0.9610 - accuracy: 0.6790 - val_loss: 0.9225 - val_accuracy: 0.6985\nEpoch 70/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9566 - accuracy: 0.6799 - val_loss: 0.9383 - val_accuracy: 0.6965\nEpoch 71/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9670 - accuracy: 0.6740 - val_loss: 0.9634 - val_accuracy: 0.6783\nEpoch 72/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9768 - accuracy: 0.6704 - val_loss: 0.8960 - val_accuracy: 0.6963\nEpoch 73/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9547 - accuracy: 0.6817 - val_loss: 0.8929 - val_accuracy: 0.7088\nEpoch 74/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9583 - accuracy: 0.6801 - val_loss: 0.9462 - val_accuracy: 0.6812\nEpoch 75/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9512 - accuracy: 0.6834 - val_loss: 0.9067 - val_accuracy: 0.6951\nEpoch 76/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9681 - accuracy: 0.6751 - val_loss: 0.9263 - val_accuracy: 0.6942\nEpoch 77/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9467 - accuracy: 0.6843 - val_loss: 0.8915 - val_accuracy: 0.7018\nEpoch 78/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9539 - accuracy: 0.6815 - val_loss: 0.9162 - val_accuracy: 0.6876\nEpoch 79/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9466 - accuracy: 0.6836 - val_loss: 0.8996 - val_accuracy: 0.7031\nEpoch 80/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9515 - accuracy: 0.6822 - val_loss: 0.8824 - val_accuracy: 0.7060\nEpoch 81/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9513 - accuracy: 0.6832 - val_loss: 0.9449 - val_accuracy: 0.6869\nEpoch 82/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9485 - accuracy: 0.6830 - val_loss: 0.9464 - val_accuracy: 0.6816\nEpoch 83/100\n375/375 [==============================] - 29s 76ms/step - loss: 0.9471 - accuracy: 0.6840 - val_loss: 0.8862 - val_accuracy: 0.7048\nEpoch 84/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9470 - accuracy: 0.6851 - val_loss: 0.8803 - val_accuracy: 0.7063\nEpoch 85/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9455 - accuracy: 0.6842 - val_loss: 0.8786 - val_accuracy: 0.7119\nEpoch 86/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9478 - accuracy: 0.6850 - val_loss: 0.8852 - val_accuracy: 0.7023\nEpoch 87/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9485 - accuracy: 0.6830 - val_loss: 0.8809 - val_accuracy: 0.7080\nEpoch 88/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9559 - accuracy: 0.6818 - val_loss: 0.8808 - val_accuracy: 0.7091\nEpoch 89/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9559 - accuracy: 0.6814 - val_loss: 0.8960 - val_accuracy: 0.7004\nEpoch 90/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9468 - accuracy: 0.6835 - val_loss: 0.9058 - val_accuracy: 0.6975\nEpoch 91/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9457 - accuracy: 0.6834 - val_loss: 0.9527 - val_accuracy: 0.6716\nEpoch 92/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9540 - accuracy: 0.6804 - val_loss: 0.8798 - val_accuracy: 0.7125\nEpoch 93/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9497 - accuracy: 0.6837 - val_loss: 1.0125 - val_accuracy: 0.6543\nEpoch 94/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9473 - accuracy: 0.6834 - val_loss: 0.9361 - val_accuracy: 0.6777\nEpoch 95/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9494 - accuracy: 0.6825 - val_loss: 0.8761 - val_accuracy: 0.7042\nEpoch 96/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9420 - accuracy: 0.6860 - val_loss: 0.8932 - val_accuracy: 0.7014\nEpoch 97/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9460 - accuracy: 0.6827 - val_loss: 0.8747 - val_accuracy: 0.7082\nEpoch 98/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9389 - accuracy: 0.6847 - val_loss: 0.9072 - val_accuracy: 0.6951\nEpoch 99/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9416 - accuracy: 0.6837 - val_loss: 0.8890 - val_accuracy: 0.7036\nEpoch 100/100\n375/375 [==============================] - 28s 73ms/step - loss: 0.9349 - accuracy: 0.6880 - val_loss: 0.9311 - val_accuracy: 0.6885\n\n\nShowing the training dynamics:\n\nfig, axes = plt.subplots(1,2, figsize=(9,4))\naxes[0].plot(history.history['loss'][1:], label=\"Train\")\naxes[0].plot(history.history['val_loss'][1:], label=\"Val\")\naxes[1].plot(history.history['accuracy'], label=\"Train\")\naxes[1].plot(history.history['val_accuracy'], label=\"Val\")\nplt.legend()\nplt.show()\n\n\n\n\nCalculate the metrics in the test set:\n\nmodel.evaluate(X_test, Y_test, batch_size=128)\n\n79/79 [==============================] - 2s 30ms/step - loss: 0.9478 - accuracy: 0.6835\n\n\n[0.9478453993797302, 0.6834999918937683]\n\n\nWe can visualize the gabor filters after the training process:\n\nmodel.layers[0].show_filters()\n\n\n\n\n\nmodel.layers[3].show_filters()\n\n\n\n\n\nmodel.layers[6].show_filters()"
  },
  {
    "objectID": "Experiments/04_multiple_gaborlayer_relu.html",
    "href": "Experiments/04_multiple_gaborlayer_relu.html",
    "title": "Gabor layer experiment",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom einops import rearrange, repeat\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import mnist\n\nfrom flayers.layers import GaborLayer"
  },
  {
    "objectID": "Experiments/04_multiple_gaborlayer_relu.html#data-loading",
    "href": "Experiments/04_multiple_gaborlayer_relu.html#data-loading",
    "title": "Gabor layer experiment",
    "section": "Data loading",
    "text": "Data loading\n\nWe will be using MNIST for a simple and quick test.\n\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\nX_train = repeat(X_train, \"b h w ->  b h w c\", c=1)/255.0\nX_test = repeat(X_test, \"b h w ->  b h w c\", c=1)/255.0\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\n((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
  },
  {
    "objectID": "Experiments/04_multiple_gaborlayer_relu.html#definition-of-simple-model",
    "href": "Experiments/04_multiple_gaborlayer_relu.html#definition-of-simple-model",
    "title": "Gabor layer experiment",
    "section": "Definition of simple model",
    "text": "Definition of simple model\n\nn_gabors = 4\nsigma_i = [0.1, 0.2, 0.1, 0.1]\nsigma_j = [0.2, 0.1, 0.2, 0.2]\nfreq = [10, 10]*2\ntheta = [0, np.pi/2, np.pi/4, -np.pi/4]\nrot_theta = [0, 0]*2\nsigma_theta = [0, 0, np.pi/4, -np.pi/4]\n\n\nmodel = tf.keras.Sequential([\n    GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n               theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20, input_shape=(28,28,1)),\n    layers.ReLU(),\n    layers.MaxPool2D(2),\n    GaborLayer(n_gabors=n_gabors, size=10, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n               theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=10),\n    layers.ReLU(),\n    layers.MaxPool2D(2),\n    GaborLayer(n_gabors=n_gabors, size=5, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n               theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=5),\n    layers.ReLU(),\n    layers.MaxPool2D(2),\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ngabor_layer_3 (GaborLayer)   (None, 28, 28, 4)         26        \n_________________________________________________________________\nre_lu_3 (ReLU)               (None, 28, 28, 4)         0         \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 14, 14, 4)         0         \n_________________________________________________________________\ngabor_layer_4 (GaborLayer)   (None, 14, 14, 4)         26        \n_________________________________________________________________\nre_lu_4 (ReLU)               (None, 14, 14, 4)         0         \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 7, 7, 4)           0         \n_________________________________________________________________\ngabor_layer_5 (GaborLayer)   (None, 7, 7, 4)           26        \n_________________________________________________________________\nre_lu_5 (ReLU)               (None, 7, 7, 4)           0         \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 3, 3, 4)           0         \n_________________________________________________________________\nglobal_average_pooling2d_1 ( (None, 4)                 0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                50        \n=================================================================\nTotal params: 128\nTrainable params: 128\nNon-trainable params: 0\n_________________________________________________________________\n\n\nWe can show the initial Gabor filters:\n\nmodel.layers[0].show_filters()\n\n2022-09-08 12:04:47.616859: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-09-08 12:04:47.838710: I tensorflow/core/util/cuda_solvers.cc:180] Creating CudaSolver handles for stream 0x5600333a86c0\n\n\n\n\n\n\nmodel.layers[3].show_filters()\n\n\n\n\n\nmodel.layers[6].show_filters()\n\n\n\n\n\nhistory = model.fit(X_train, Y_train, batch_size=128, epochs=100, validation_split=0.2)\n\nEpoch 1/100\n\n\n2022-09-08 12:05:58.537282: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n2022-09-08 12:05:58.998947: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n\n\n375/375 [==============================] - 45s 88ms/step - loss: 985.9783 - accuracy: 0.1111 - val_loss: 3.0740 - val_accuracy: 0.0936\nEpoch 2/100\n375/375 [==============================] - 32s 85ms/step - loss: 2.5712 - accuracy: 0.0968 - val_loss: 2.4051 - val_accuracy: 0.0983\nEpoch 3/100\n375/375 [==============================] - 32s 84ms/step - loss: 2.3618 - accuracy: 0.1037 - val_loss: 2.3399 - val_accuracy: 0.1082\nEpoch 4/100\n375/375 [==============================] - 33s 87ms/step - loss: 2.3222 - accuracy: 0.1124 - val_loss: 2.3124 - val_accuracy: 0.1216\nEpoch 5/100\n375/375 [==============================] - 32s 86ms/step - loss: 2.3025 - accuracy: 0.1255 - val_loss: 2.2956 - val_accuracy: 0.1308\nEpoch 6/100\n375/375 [==============================] - 33s 87ms/step - loss: 2.2889 - accuracy: 0.1331 - val_loss: 2.2823 - val_accuracy: 0.1339\nEpoch 7/100\n375/375 [==============================] - 31s 84ms/step - loss: 2.2780 - accuracy: 0.1406 - val_loss: 2.2732 - val_accuracy: 0.1412\nEpoch 8/100\n375/375 [==============================] - 32s 84ms/step - loss: 2.2704 - accuracy: 0.1438 - val_loss: 2.2659 - val_accuracy: 0.1460\nEpoch 9/100\n375/375 [==============================] - 31s 84ms/step - loss: 2.2643 - accuracy: 0.1460 - val_loss: 2.2597 - val_accuracy: 0.1444\nEpoch 10/100\n375/375 [==============================] - 32s 85ms/step - loss: 2.2591 - accuracy: 0.1466 - val_loss: 2.2548 - val_accuracy: 0.1482\nEpoch 11/100\n375/375 [==============================] - 32s 84ms/step - loss: 2.2529 - accuracy: 0.1501 - val_loss: 2.2484 - val_accuracy: 0.1501\nEpoch 12/100\n375/375 [==============================] - 32s 84ms/step - loss: 2.2463 - accuracy: 0.1546 - val_loss: 2.2382 - val_accuracy: 0.1579\nEpoch 13/100\n375/375 [==============================] - 31s 84ms/step - loss: 2.2391 - accuracy: 0.1558 - val_loss: 2.2323 - val_accuracy: 0.1558\nEpoch 14/100\n375/375 [==============================] - 32s 84ms/step - loss: 2.2319 - accuracy: 0.1632 - val_loss: 2.2225 - val_accuracy: 0.1595\nEpoch 15/100\n375/375 [==============================] - 31s 83ms/step - loss: 2.2251 - accuracy: 0.1623 - val_loss: 2.2168 - val_accuracy: 0.1688\nEpoch 16/100\n375/375 [==============================] - 31s 84ms/step - loss: 2.2159 - accuracy: 0.1707 - val_loss: 2.2077 - val_accuracy: 0.1709\nEpoch 17/100\n375/375 [==============================] - 31s 83ms/step - loss: 2.2050 - accuracy: 0.1780 - val_loss: 2.1958 - val_accuracy: 0.1867\nEpoch 18/100\n375/375 [==============================] - 31s 82ms/step - loss: 2.1919 - accuracy: 0.1858 - val_loss: 2.1867 - val_accuracy: 0.1929\nEpoch 19/100\n375/375 [==============================] - 31s 81ms/step - loss: 2.1812 - accuracy: 0.1945 - val_loss: 2.1729 - val_accuracy: 0.1979\nEpoch 20/100\n375/375 [==============================] - 31s 83ms/step - loss: 2.1656 - accuracy: 0.2046 - val_loss: 2.1579 - val_accuracy: 0.2122\nEpoch 21/100\n375/375 [==============================] - 31s 82ms/step - loss: 2.1526 - accuracy: 0.2124 - val_loss: 2.1398 - val_accuracy: 0.2209\nEpoch 22/100\n375/375 [==============================] - 31s 84ms/step - loss: 2.1356 - accuracy: 0.2198 - val_loss: 2.1211 - val_accuracy: 0.2302\nEpoch 23/100\n375/375 [==============================] - 31s 83ms/step - loss: 2.1097 - accuracy: 0.2297 - val_loss: 2.1363 - val_accuracy: 0.1929\nEpoch 24/100\n375/375 [==============================] - 31s 82ms/step - loss: 2.0828 - accuracy: 0.2349 - val_loss: 2.0603 - val_accuracy: 0.2377\nEpoch 25/100\n375/375 [==============================] - 31s 82ms/step - loss: 2.0613 - accuracy: 0.2377 - val_loss: 2.0356 - val_accuracy: 0.2533\nEpoch 26/100\n375/375 [==============================] - 31s 83ms/step - loss: 2.0246 - accuracy: 0.2483 - val_loss: 2.0052 - val_accuracy: 0.2559\nEpoch 27/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.9939 - accuracy: 0.2626 - val_loss: 2.0181 - val_accuracy: 0.2536\nEpoch 28/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.9504 - accuracy: 0.2867 - val_loss: 1.9345 - val_accuracy: 0.2952\nEpoch 29/100\n375/375 [==============================] - 30s 81ms/step - loss: 1.9102 - accuracy: 0.2981 - val_loss: 1.8928 - val_accuracy: 0.3027\nEpoch 30/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.8791 - accuracy: 0.3022 - val_loss: 1.8656 - val_accuracy: 0.2966\nEpoch 31/100\n375/375 [==============================] - 31s 81ms/step - loss: 1.8436 - accuracy: 0.3085 - val_loss: 1.8246 - val_accuracy: 0.3078\nEpoch 32/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.8176 - accuracy: 0.3138 - val_loss: 1.7932 - val_accuracy: 0.3173\nEpoch 33/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.7868 - accuracy: 0.3216 - val_loss: 1.7776 - val_accuracy: 0.3264\nEpoch 34/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.7210 - accuracy: 0.3379 - val_loss: 1.6930 - val_accuracy: 0.3298\nEpoch 35/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.6528 - accuracy: 0.3550 - val_loss: 1.6709 - val_accuracy: 0.3572\nEpoch 36/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.6281 - accuracy: 0.3638 - val_loss: 1.6007 - val_accuracy: 0.3685\nEpoch 37/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.5943 - accuracy: 0.3723 - val_loss: 1.5715 - val_accuracy: 0.3918\nEpoch 38/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.5836 - accuracy: 0.3803 - val_loss: 1.5498 - val_accuracy: 0.3907\nEpoch 39/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.5560 - accuracy: 0.3921 - val_loss: 1.5089 - val_accuracy: 0.4304\nEpoch 40/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.5430 - accuracy: 0.4028 - val_loss: 1.4976 - val_accuracy: 0.4376\nEpoch 41/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.5249 - accuracy: 0.4141 - val_loss: 1.4813 - val_accuracy: 0.4534\nEpoch 42/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.4983 - accuracy: 0.4308 - val_loss: 1.4671 - val_accuracy: 0.4387\nEpoch 43/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.4679 - accuracy: 0.4423 - val_loss: 1.4519 - val_accuracy: 0.4453\nEpoch 44/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.4707 - accuracy: 0.4391 - val_loss: 1.3944 - val_accuracy: 0.4703\nEpoch 45/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.4273 - accuracy: 0.4564 - val_loss: 1.5095 - val_accuracy: 0.4157\nEpoch 46/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.3460 - accuracy: 0.4915 - val_loss: 1.2557 - val_accuracy: 0.5315\nEpoch 47/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.2945 - accuracy: 0.5171 - val_loss: 1.2103 - val_accuracy: 0.5444\nEpoch 48/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.2753 - accuracy: 0.5259 - val_loss: 1.3190 - val_accuracy: 0.5141\nEpoch 49/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.2489 - accuracy: 0.5384 - val_loss: 1.2835 - val_accuracy: 0.5042\nEpoch 50/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.2157 - accuracy: 0.5519 - val_loss: 1.1760 - val_accuracy: 0.5705\nEpoch 51/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.2159 - accuracy: 0.5510 - val_loss: 1.1809 - val_accuracy: 0.5684\nEpoch 52/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.2119 - accuracy: 0.5555 - val_loss: 1.1244 - val_accuracy: 0.5897\nEpoch 53/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1933 - accuracy: 0.5614 - val_loss: 1.1595 - val_accuracy: 0.5832\nEpoch 54/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.2085 - accuracy: 0.5562 - val_loss: 1.2195 - val_accuracy: 0.5597\nEpoch 55/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1991 - accuracy: 0.5624 - val_loss: 1.1500 - val_accuracy: 0.5898\nEpoch 56/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1975 - accuracy: 0.5623 - val_loss: 1.1141 - val_accuracy: 0.5968\nEpoch 57/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1818 - accuracy: 0.5651 - val_loss: 1.1461 - val_accuracy: 0.5820\nEpoch 58/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1922 - accuracy: 0.5619 - val_loss: 1.1506 - val_accuracy: 0.5747\nEpoch 59/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.2001 - accuracy: 0.5591 - val_loss: 1.1239 - val_accuracy: 0.5916\nEpoch 60/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1768 - accuracy: 0.5722 - val_loss: 1.1599 - val_accuracy: 0.5850\nEpoch 61/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1618 - accuracy: 0.5755 - val_loss: 1.0917 - val_accuracy: 0.6029\nEpoch 62/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1562 - accuracy: 0.5796 - val_loss: 1.1027 - val_accuracy: 0.5997\nEpoch 63/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1704 - accuracy: 0.5736 - val_loss: 1.0901 - val_accuracy: 0.6047\nEpoch 64/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1763 - accuracy: 0.5681 - val_loss: 1.1084 - val_accuracy: 0.6019\nEpoch 65/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1516 - accuracy: 0.5791 - val_loss: 1.1424 - val_accuracy: 0.5817\nEpoch 66/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1623 - accuracy: 0.5738 - val_loss: 1.0913 - val_accuracy: 0.5972\nEpoch 67/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1638 - accuracy: 0.5762 - val_loss: 1.1198 - val_accuracy: 0.6037\nEpoch 68/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1481 - accuracy: 0.5801 - val_loss: 1.0738 - val_accuracy: 0.6120\nEpoch 69/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1545 - accuracy: 0.5778 - val_loss: 1.2973 - val_accuracy: 0.5236\nEpoch 70/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1556 - accuracy: 0.5770 - val_loss: 1.1003 - val_accuracy: 0.6025\nEpoch 71/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1413 - accuracy: 0.5808 - val_loss: 1.1655 - val_accuracy: 0.5612\nEpoch 72/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1646 - accuracy: 0.5732 - val_loss: 1.0869 - val_accuracy: 0.6077\nEpoch 73/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1467 - accuracy: 0.5826 - val_loss: 1.1171 - val_accuracy: 0.5973\nEpoch 74/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1394 - accuracy: 0.5827 - val_loss: 1.0873 - val_accuracy: 0.6067\nEpoch 75/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1287 - accuracy: 0.5866 - val_loss: 1.0773 - val_accuracy: 0.6004\nEpoch 76/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1397 - accuracy: 0.5833 - val_loss: 1.1152 - val_accuracy: 0.5881\nEpoch 77/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1492 - accuracy: 0.5792 - val_loss: 1.1067 - val_accuracy: 0.5840\nEpoch 78/100\n375/375 [==============================] - 30s 81ms/step - loss: 1.1314 - accuracy: 0.5841 - val_loss: 1.0981 - val_accuracy: 0.5865\nEpoch 79/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1389 - accuracy: 0.5816 - val_loss: 1.0705 - val_accuracy: 0.6087\nEpoch 80/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1331 - accuracy: 0.5823 - val_loss: 1.0890 - val_accuracy: 0.5955\nEpoch 81/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1568 - accuracy: 0.5715 - val_loss: 1.0918 - val_accuracy: 0.6032\nEpoch 82/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1627 - accuracy: 0.5715 - val_loss: 1.0989 - val_accuracy: 0.6002\nEpoch 83/100\n375/375 [==============================] - 30s 81ms/step - loss: 1.1402 - accuracy: 0.5812 - val_loss: 1.0879 - val_accuracy: 0.6016\nEpoch 84/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1329 - accuracy: 0.5858 - val_loss: 1.0934 - val_accuracy: 0.6027\nEpoch 85/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1283 - accuracy: 0.5911 - val_loss: 1.0936 - val_accuracy: 0.6072\nEpoch 86/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1244 - accuracy: 0.5931 - val_loss: 1.1160 - val_accuracy: 0.5979\nEpoch 87/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1222 - accuracy: 0.5947 - val_loss: 1.0462 - val_accuracy: 0.6208\nEpoch 88/100\n375/375 [==============================] - 30s 81ms/step - loss: 1.1086 - accuracy: 0.5978 - val_loss: 1.0793 - val_accuracy: 0.6053\nEpoch 89/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1146 - accuracy: 0.5970 - val_loss: 1.0564 - val_accuracy: 0.6127\nEpoch 90/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1012 - accuracy: 0.6016 - val_loss: 1.1103 - val_accuracy: 0.6007\nEpoch 91/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1043 - accuracy: 0.6010 - val_loss: 1.0442 - val_accuracy: 0.6267\nEpoch 92/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1101 - accuracy: 0.6013 - val_loss: 1.1222 - val_accuracy: 0.5767\nEpoch 93/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.0980 - accuracy: 0.6058 - val_loss: 1.0606 - val_accuracy: 0.6108\nEpoch 94/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1073 - accuracy: 0.5988 - val_loss: 1.0959 - val_accuracy: 0.5970\nEpoch 95/100\n375/375 [==============================] - 32s 85ms/step - loss: 1.0962 - accuracy: 0.6022 - val_loss: 1.0617 - val_accuracy: 0.6246\nEpoch 96/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.1002 - accuracy: 0.6028 - val_loss: 1.0339 - val_accuracy: 0.6304\nEpoch 97/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.0971 - accuracy: 0.6040 - val_loss: 1.0482 - val_accuracy: 0.6246\nEpoch 98/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.0995 - accuracy: 0.6049 - val_loss: 1.0905 - val_accuracy: 0.6219\nEpoch 99/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.0929 - accuracy: 0.6069 - val_loss: 1.0765 - val_accuracy: 0.6123\nEpoch 100/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.0936 - accuracy: 0.6077 - val_loss: 1.0671 - val_accuracy: 0.6162\n\n\nShowing the training dynamics:\n\nfig, axes = plt.subplots(1,2, figsize=(9,4))\naxes[0].plot(history.history['loss'][1:], label=\"Train\")\naxes[0].plot(history.history['val_loss'][1:], label=\"Val\")\naxes[1].plot(history.history['accuracy'], label=\"Train\")\naxes[1].plot(history.history['val_accuracy'], label=\"Val\")\nplt.legend()\nplt.show()\n\n\n\n\nCalculate the metrics in the test set:\n\nmodel.evaluate(X_test, Y_test, batch_size=128)\n\n79/79 [==============================] - 3s 33ms/step - loss: 1.0718 - accuracy: 0.6153\n\n\n[1.071818232536316, 0.6152999997138977]\n\n\nWe can visualize the gabor filters after the training process:\n\nmodel.layers[0].show_filters()\n\n\n\n\n\nmodel.layers[2].show_filters()\n\nAttributeError: 'MaxPooling2D' object has no attribute 'show_filters'\n\n\n\nmodel.layers[4].show_filters()\n\n\n\n\nWe can even check the atributes of the layer to inspect the change in the initial parameters:\n\nmodel.layers[0].theta.numpy()*180/np.pi\n\narray([  0.38321877,  90.76502   ,  45.756405  , -47.088924  ],\n      dtype=float32)\n\n\n\nmodel.layers[0].rot_theta.numpy()*180/np.pi\n\narray([ 0.39840868, -0.8525029 , -0.04723335, -1.0895499 ], dtype=float32)\n\n\n\nmodel.layers[0].sigma_theta.numpy()*180/np.pi\n\narray([-45.138485,   9.870471,  55.021404, -44.676727], dtype=float32)"
  },
  {
    "objectID": "Experiments/02_multiple_gaborlayer.html",
    "href": "Experiments/02_multiple_gaborlayer.html",
    "title": "Gabor layer experiment",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom einops import rearrange, repeat\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import mnist\n\nfrom flayers.layers import GaborLayer"
  },
  {
    "objectID": "Experiments/02_multiple_gaborlayer.html#data-loading",
    "href": "Experiments/02_multiple_gaborlayer.html#data-loading",
    "title": "Gabor layer experiment",
    "section": "Data loading",
    "text": "Data loading\n\nWe will be using MNIST for a simple and quick test.\n\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\nX_train = repeat(X_train, \"b h w ->  b h w c\", c=1)/255.0\nX_test = repeat(X_test, \"b h w ->  b h w c\", c=1)/255.0\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\n((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
  },
  {
    "objectID": "Experiments/02_multiple_gaborlayer.html#definition-of-simple-model",
    "href": "Experiments/02_multiple_gaborlayer.html#definition-of-simple-model",
    "title": "Gabor layer experiment",
    "section": "Definition of simple model",
    "text": "Definition of simple model\n\nn_gabors = 4\nsigma_i = [0.1, 0.2, 0.1, 0.1]\nsigma_j = [0.2, 0.1, 0.2, 0.2]\nfreq = [10, 10]*2\ntheta = [0, np.pi/2, np.pi/4, -np.pi/4]\nrot_theta = [0, 0]*2\nsigma_theta = [0, 0, np.pi/4, -np.pi/4]\n\n\nmodel = tf.keras.Sequential([\n    GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n               theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20, input_shape=(28,28,1)),\n    layers.MaxPool2D(2),\n    GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n               theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20),\n    layers.MaxPool2D(2),\n    GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n               theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20),\n    layers.MaxPool2D(2),\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\n2022-09-20 12:45:08.047414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5435 MB memory:  -> device: 0, name: NVIDIA GeForce GTX TITAN Black, pci bus id: 0000:83:00.0, compute capability: 3.5\n2022-09-20 12:45:10.647320: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-09-20 12:45:10.896105: I tensorflow/core/util/cuda_solvers.cc:180] Creating CudaSolver handles for stream 0x56262da5ad20\n\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ngabor_layer (GaborLayer)     (None, 28, 28, 4)         1626      \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 14, 14, 4)         0         \n_________________________________________________________________\ngabor_layer_1 (GaborLayer)   (None, 14, 14, 4)         1626      \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 7, 7, 4)           0         \n_________________________________________________________________\ngabor_layer_2 (GaborLayer)   (None, 7, 7, 4)           1626      \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 3, 3, 4)           0         \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 4)                 0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                50        \n=================================================================\nTotal params: 4,928\nTrainable params: 128\nNon-trainable params: 4,800\n_________________________________________________________________\n\n\nWe can show the initial Gabor filters:\n\nmodel.layers[0].show_filters()\n\n\n\n\n\nhistory = model.fit(X_train, Y_train, batch_size=128, epochs=2, validation_split=0.2)\n\nEpoch 1/2\n\n\n2022-09-20 12:45:31.290944: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n2022-09-20 12:45:31.798301: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n\n\n375/375 [==============================] - 53s 96ms/step - loss: 2.2773 - accuracy: 0.1696 - val_loss: 2.2126 - val_accuracy: 0.2162\nEpoch 2/2\n375/375 [==============================] - 35s 95ms/step - loss: 2.1034 - accuracy: 0.2343 - val_loss: 1.9671 - val_accuracy: 0.2958\n\n\nShowing the training dynamics:\n\nfig, axes = plt.subplots(1,2, figsize=(9,4))\naxes[0].plot(history.history['loss'][1:], label=\"Train\")\naxes[0].plot(history.history['val_loss'][1:], label=\"Val\")\naxes[1].plot(history.history['accuracy'], label=\"Train\")\naxes[1].plot(history.history['val_accuracy'], label=\"Val\")\nplt.legend()\nplt.show()\n\n\n\n\nCalculate the metrics in the test set:\n\nmodel.evaluate(X_test, Y_test, batch_size=128)\n\n79/79 [==============================] - 0s 4ms/step - loss: 1.9803 - accuracy: 0.2908\n\n\n[1.9802665710449219, 0.290800005197525]\n\n\nWe can visualize the gabor filters after the training process:\n\nmodel.layers[0].show_filters()\n\n\n\n\n\nmodel.layers[2].show_filters()\n\n\n\n\n\nmodel.layers[4].show_filters()\n\n\n\n\nWe can even check the atributes of the layer to inspect the change in the initial parameters:\n\nmodel.layers[0].theta.numpy()*180/np.pi\n\narray([ -2.305845,  87.13862 ,  43.73978 , -44.11806 ], dtype=float32)\n\n\n\nmodel.layers[0].rot_theta.numpy()*180/np.pi\n\narray([ 1.1697422 , -2.6115904 ,  0.03874239,  0.905848  ], dtype=float32)\n\n\n\nmodel.layers[0].sigma_theta.numpy()*180/np.pi\n\narray([ 11.119123,   8.056805,  59.319393, -73.5566  ], dtype=float32)"
  },
  {
    "objectID": "layers.html",
    "href": "layers.html",
    "title": "Layers",
    "section": "",
    "text": "Convolutional layer that forces a functional Gabor form for its filters. Every parameter of the Gabor can be learnt."
  },
  {
    "objectID": "layers.html#managing-dtype",
    "href": "layers.html#managing-dtype",
    "title": "Layers",
    "section": "Managing dtype",
    "text": "Managing dtype\nTensorflow is a bit picky when it comes to dtype, so it can be useful to define a function that will ensure that every parameter is casted to the same dtype:\n\na, b = tf.convert_to_tensor(1), tf.convert_to_tensor(1.1)\nprint(a.dtype, b.dtype)\n# assert a.dtype != b.dtype\n\n<dtype: 'int32'> <dtype: 'float32'>\n\n\n2022-09-22 10:36:50.004800: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n2022-09-22 10:36:50.004871: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: megatron\n2022-09-22 10:36:50.004890: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: megatron\n2022-09-22 10:36:50.005005: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.57.2\n2022-09-22 10:36:50.005051: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.57.2\n2022-09-22 10:36:50.005066: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.57.2\n\n\n\nc, d = cast_all(a, b)\nprint(c.dtype, d.dtype)\nassert c.dtype == d.dtype\n\n<dtype: 'float32'> <dtype: 'float32'>"
  },
  {
    "objectID": "layers.html#creating-a-gabor-filter-in-tensorflow",
    "href": "layers.html#creating-a-gabor-filter-in-tensorflow",
    "title": "Layers",
    "section": "Creating a Gabor filter in TensorFlow",
    "text": "Creating a Gabor filter in TensorFlow\nFirst of all we need to be able to generate Gabor filters as Tensorflow Tensor:\n\nsource\n\ngabor_2d_tf\n\n gabor_2d_tf (i, j, imean, jmean, sigma_i, sigma_j, freq, theta,\n              sigma_theta)\n\n\n\n\n\nDetails\n\n\n\n\ni\nHorizontal domain\n\n\nj\nVertical domain\n\n\nimean\nHorizontal mean\n\n\njmean\nVertical mean\n\n\nsigma_i\nHorizontal width\n\n\nsigma_j\nVertical width\n\n\nfreq\nFrecuency of the filter\n\n\ntheta\nAngle of the filter\n\n\nsigma_theta\nWidth of the angle?? Rotation of the domain??\n\n\n\n\nsource\n\n\ncreate_gabor_rot_tf\n\n create_gabor_rot_tf (Nrows, Ncols, imean, jmean, sigma_i, sigma_j, freq,\n                      theta, rot_theta, sigma_theta, fs)\n\nCreates a rotated Gabor filter with the input parameters.\n\n\n\n\nDetails\n\n\n\n\nNrows\nNumber of horizontal pixels\n\n\nNcols\nNumber of vertical pixels\n\n\nimean\nHorizontal mean (in degrees)\n\n\njmean\nVertical mean (in degrees)\n\n\nsigma_i\nHorizontal width (in degrees)\n\n\nsigma_j\nVertical width (in degrees)\n\n\nfreq\nFrequency\n\n\ntheta\nAngle\n\n\nrot_theta\nRotation of the domain??\n\n\nsigma_theta\nWidth of the angle?? Rotation of the domain??\n\n\nfs\nSampling frequency\n\n\n\n\ngabor = create_gabor_rot_tf(Nrows=20, Ncols=20, imean=0.5, jmean=0.5, sigma_i=0.1, sigma_j=0.1, freq=10, theta=0, rot_theta=0, sigma_theta=0, fs=20)\nplt.imshow(gabor)\nplt.show()\n\n2022-09-22 10:36:51.315618: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)"
  },
  {
    "objectID": "layers.html#creating-a-set-of-gabor-filters",
    "href": "layers.html#creating-a-set-of-gabor-filters",
    "title": "Layers",
    "section": "Creating a set of Gabor filters",
    "text": "Creating a set of Gabor filters\n\nIt can be a little bit tricky to translate plain Python for loops into tf.function.\n\nIn plain Python, if we wanted to create a set of filters we could initialize an empty array or list and fill it with the different filters generated inside a for loop, but we can’t do that inside a tf.function because Tensorflow tries to build the computational graph and starts to nest graphs inside graphs and the performance is terrible. Luckily for us, they implement a tf.TensorArray that can be used inside a tf.function to this effect.\n\nsource\n\ncreate_multiple_different_rot_gabor_tf\n\n create_multiple_different_rot_gabor_tf (n_gabors, Nrows, Ncols, imean,\n                                         jmean, sigma_i:list,\n                                         sigma_j:list, freq:list,\n                                         theta:list, rot_theta:list,\n                                         sigma_theta:list, fs,\n                                         normalize:bool=True)\n\nCreates a set of Gabor filters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_gabors\n\n\nNumber of Gabor filters we want to create.\n\n\nNrows\n\n\nNumber of horizontal pixels.\n\n\nNcols\n\n\nNumber of vertical pixels.\n\n\nimean\n\n\nHorizontal mean (in degrees).\n\n\njmean\n\n\nVertical mean (in degrees).\n\n\nsigma_i\nlist\n\nHorizontal width (in degrees).\n\n\nsigma_j\nlist\n\nVertical width (in degrees).\n\n\nfreq\nlist\n\nFrequency.\n\n\ntheta\nlist\n\nAngle.\n\n\nrot_theta\nlist\n\nRotation of the domain??\n\n\nsigma_theta\nlist\n\nWidth of the angle?? Rotation of the domain??\n\n\nfs\n\n\nSampling frequency.\n\n\nnormalize\nbool\nTrue\nWether to normalize (and divide by n_gabors) or not the Gabors.\n\n\n\n\nn_gabors = 4\ngabors = create_multiple_different_rot_gabor_tf(n_gabors=n_gabors, Nrows=20, Ncols=20, imean=0.5, jmean=0.5, sigma_i=[0.1]*n_gabors, sigma_j=[0.1]*n_gabors, freq=[10]*n_gabors, \n                                                theta=[0]*n_gabors, rot_theta=[0]*n_gabors, sigma_theta=[0]*n_gabors, fs=20)\ngabors.shape\n\n2022-09-22 10:36:53.962292: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at functional_ops.cc:374 : Internal: No function library\n2022-09-22 10:36:53.963533: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at functional_ops.cc:374 : Internal: No function library\n2022-09-22 10:36:53.964886: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at functional_ops.cc:374 : Internal: No function library\n\n\nTensorShape([4, 20, 20])\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\nfor gabor_filter, ax in zip(gabors, axes.ravel()):\n    ax.imshow(gabor_filter)\nplt.show()\n\n\n\n\nWe can, as well, change the parameters of the Gabor filters independently:\n\nn_gabors = 4\nsigma_i = [0.1, 0.2, 0.3, 0.4]\nsigma_j = [0.1, 0.2, 0.3, 0.4]\nfreq = [10, 20, 30, 40]\ntheta = [0, 45, 90, 135]\nrot_theta = [0, 45, 90, 135]\nsigma_theta = [0, 45, 90, 135]\ngabors = create_multiple_different_rot_gabor_tf(n_gabors=n_gabors, Nrows=20, Ncols=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n                                                theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20)\ngabors.shape\n\n2022-09-22 10:36:55.606241: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at functional_ops.cc:374 : Internal: No function library\n2022-09-22 10:36:55.607478: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at functional_ops.cc:374 : Internal: No function library\n2022-09-22 10:36:55.608616: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at functional_ops.cc:374 : Internal: No function library\n\n\nTensorShape([4, 20, 20])\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\nfor gabor_filter, ax in zip(gabors, axes.ravel()):\n    ax.imshow(gabor_filter)\nplt.show()"
  },
  {
    "objectID": "layers.html#gabor-layer-1",
    "href": "layers.html#gabor-layer-1",
    "title": "Layers",
    "section": "Gabor layer",
    "text": "Gabor layer\n\nGabor layer with pre-defined values.\n\nThe Gabor filters are stored in the GaborLayer.filters attribute and while they are re-calculated at each step of training (as should be), they are not re-calculated during inference. At inference time, the last calculated filters are used.\n\nsource\n\nGaborLayer\n\n GaborLayer (*args, **kwargs)\n\nPre-initialized Gabor layer that is trainable through backpropagation.\n\na = GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n               theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20, input_shape=(28,28,1))\n# a.filters.shape\n\n\nsource\n\n\nGaborLayer.call\n\n GaborLayer.call (inputs, training=False)\n\nBuild a set of filters from the stored values and convolve them with the input.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninputs\n\n\nInputs to the layer.\n\n\ntraining\nbool\nFalse\nFlag indicating if we are training the layer or using it for inference.\n\n\n\nIt’s important to check if we can use it in a model:\n\nn_gabors = 4\nsigma_i = [0.1, 0.2]*2\nsigma_j = [0.2, 0.1]*2\nfreq = [10, 10]*2\ntheta = [0, np.pi/2]*2\nrot_theta = [0, 0]*2\nsigma_theta = [0, 0]*2\n\n\nmodel = tf.keras.Sequential([\n    GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n               theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20, input_shape=(28,28,1)),\n    layers.MaxPool2D(2),\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ngabor_layer_2 (GaborLayer)   (None, 28, 28, 4)         1626      \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 14, 14, 4)         0         \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 4)                 0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                50        \n=================================================================\nTotal params: 1,676\nTrainable params: 76\nNon-trainable params: 1,600\n_________________________________________________________________\n\n\n\nn_gabors = 4\nsigma_i = [0.1, 0.2, 0.3, 0.4]\nsigma_j = [0.1, 0.2, 0.3, 0.4]\nfreq = [10, 20, 30, 40]\ntheta = [0, 45, 90, 135]\nrot_theta = [0, 45, 90, 135]\nsigma_theta = [0, 45, 90, 135]\n\ngaborlayer = GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n                        theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20)\nsample_input = np.random.uniform(0, 1, size=(1, 256, 256, 1))\nsample_output = gaborlayer(sample_input).numpy()\nassert sample_input.shape[1:3] == sample_output.shape[1:3]\nassert sample_output.shape[-1] == 4\nfig, axes = plt.subplots(1,2)\naxes[0].imshow(sample_input.squeeze())\naxes[1].imshow(sample_output.squeeze())\naxes[0].set_title(\"Input\")\naxes[1].set_title(\"Output\")\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\nIt can be helpful to implement a method to show the Gabor filters that are being used by the layer:\n\nsource\n\n\nGaborLayer.show_filters\n\n GaborLayer.show_filters ()\n\nCalculates and plots the filters corresponding to the stored parameters.\n\ngaborlayer = GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n                        theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20)\ngaborlayer.show_filters()\n\nWARNING:tensorflow:5 out of the last 5 calls to <function create_multiple_different_rot_gabor_tf> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\n\n\n\nWe can check that the parameters are trainable and thus the gradient is propagated properly:\n\ngaborlayer = GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n                        theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20)\n\n\ngaborlayer = GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n                        theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20)\ngaborlayer.build(())\nwith tf.GradientTape() as tape:\n    output = gaborlayer(sample_input, training=True)\n    loss = output - output**2\ngradients = tape.gradient(loss, gaborlayer.trainable_variables)\ngradients\n\nWARNING:tensorflow:6 out of the last 6 calls to <function create_multiple_different_rot_gabor_tf> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\n[<tf.Tensor: shape=(), dtype=float32, numpy=-43654416.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=11970161.0>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=\n array([-4.18656562e+04, -8.09927920e+07, -1.14410117e+05, -1.23048696e+08],\n       dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=\n array([-4.3216855e+04, -8.0995704e+07, -1.1109711e+05, -1.2311187e+08],\n       dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=\n array([-8.3397627e+00, -2.5924567e+05, -1.8548600e+03, -6.3047660e+06],\n       dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=\n array([ 1.5471164e+00, -1.0965000e+03,  2.2201016e+03, -8.2523200e+05],\n       dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=\n array([-1.5463333e+00,  1.0965000e+03, -2.2200566e+03,  8.2523475e+05],\n       dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.        ,  0.5       , -0.00195312, -0.5       ], dtype=float32)>]"
  },
  {
    "objectID": "layers.html#random-initialize-a-simple-set-of-gabor-filters",
    "href": "layers.html#random-initialize-a-simple-set-of-gabor-filters",
    "title": "Layers",
    "section": "Random initialize a simple set of Gabor filters",
    "text": "Random initialize a simple set of Gabor filters\n\nInsted of defining ourselves the initial values, we can randomly initialize them. This can speed up our testing.\n\n\nsource\n\ncreate_simple_random_set\n\n create_simple_random_set (n_gabors, size)\n\nCreates a simple set of randomly initialized squared Gabor filters.\n\n\n\n\nDetails\n\n\n\n\nn_gabors\nNumber of Gabor filters we want to create.\n\n\nsize\nSize of the Gabor (they will be square).\n\n\n\n\ngabors = create_simple_random_set(n_gabors=4, size=20)\ngabors.shape\n\nTensorShape([4, 20, 20])\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\nfor gabor_filter, ax in zip(gabors, axes.ravel()):\n    ax.imshow(gabor_filter)\nplt.show()"
  },
  {
    "objectID": "layers.html#randomgabor",
    "href": "layers.html#randomgabor",
    "title": "Layers",
    "section": "RandomGabor",
    "text": "RandomGabor\n\nActually, we can define a different layer that initializes almost all of its parameters randomly by inhereting from GaborLayer.\n\n\nsource\n\nRandomGabor\n\n RandomGabor (*args, **kwargs)\n\nRandomly initialized Gabor layer that is trainable through backpropagation.\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\n\nIt’s important to check if we can use it in a model:\n\nmodel = tf.keras.Sequential([\n    RandomGabor(n_gabors=n_gabors, size=20, input_shape=(28,28,1)),\n    layers.MaxPool2D(2),\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nrandom_gabor_3 (RandomGabor) (None, 28, 28, 4)         1626      \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 14, 14, 4)         0         \n_________________________________________________________________\nglobal_average_pooling2d_1 ( (None, 4)                 0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                50        \n=================================================================\nTotal params: 1,676\nTrainable params: 76\nNon-trainable params: 1,600\n_________________________________________________________________\n\n\nWe can check that the .call() method from GaborLayer still works:\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\nsample_input = np.random.uniform(0, 1, size=(1, 256, 256, 1))\nsample_output = gaborlayer(sample_input).numpy()\nassert sample_input.shape[1:3] == sample_output.shape[1:3]\nassert sample_output.shape[-1] == 4\nfig, axes = plt.subplots(1,2)\naxes[0].imshow(sample_input.squeeze())\naxes[1].imshow(sample_output.squeeze())\naxes[0].set_title(\"Input\")\naxes[1].set_title(\"Output\")\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\nEnsure that we can still plot its filters:\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\ngaborlayer.show_filters()\n\n\n\n\nWe can check that the parameters are trainable and thus the gradient is propagated properly:\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\nwith tf.GradientTape() as tape:\n    output = gaborlayer(sample_input, training=True)\n    loss = output - output**2\ngradients = tape.gradient(loss, gaborlayer.trainable_variables)\ngradients\n\n[<tf.Tensor: shape=(), dtype=float32, numpy=-14807149.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=310030.06>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-10065.641, -68741.8  ,  -8747.457, -81285.984], dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-17256.184, -22176.14 , -55413.055, -26464.76 ], dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=array([  7505.675, -50937.2  ,  -7013.109,  12654.501], dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=\n array([   43336.797, -1755171.4  ,    85998.625, -1342648.1  ],\n       dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ -43530.617, 1856712.   ,  -82486.6  , 1354445.6  ], dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=\n array([    193.8518, -101540.625 ,   -3512.0667,  -11797.089 ],\n       dtype=float32)>]"
  }
]