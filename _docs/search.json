[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "flayers",
    "section": "",
    "text": "git clone https://github.com/Jorgvt/flayers.git\ncd flayers\npip install -e .\npip install flayers"
  },
  {
    "objectID": "Experiments/03_multiple_randomgabor.html",
    "href": "Experiments/03_multiple_randomgabor.html",
    "title": "Multiple Random Gabor experiment",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom einops import rearrange, repeat\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import mnist\n\nfrom flayers.layers import RandomGabor"
  },
  {
    "objectID": "Experiments/03_multiple_randomgabor.html#data-loading",
    "href": "Experiments/03_multiple_randomgabor.html#data-loading",
    "title": "Multiple Random Gabor experiment",
    "section": "Data loading",
    "text": "Data loading\n\nWe will be using MNIST for a simple and quick test.\n\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\nX_train = repeat(X_train, \"b h w ->  b h w c\", c=1)/255.0\nX_test = repeat(X_test, \"b h w ->  b h w c\", c=1)/255.0\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\n((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
  },
  {
    "objectID": "Experiments/03_multiple_randomgabor.html#definition-of-simple-model",
    "href": "Experiments/03_multiple_randomgabor.html#definition-of-simple-model",
    "title": "Multiple Random Gabor experiment",
    "section": "Definition of simple model",
    "text": "Definition of simple model\n\nmodel = tf.keras.Sequential([\n    RandomGabor(n_gabors=4, size=20, input_shape=(28,28,1)),\n    layers.MaxPool2D(2),\n    RandomGabor(n_gabors=4, size=20),\n    layers.MaxPool2D(2),\n    RandomGabor(n_gabors=4, size=20),\n    layers.MaxPool2D(2),\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\n2022-09-08 10:28:47.259611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5435 MB memory:  -> device: 0, name: NVIDIA GeForce GTX TITAN Black, pci bus id: 0000:83:00.0, compute capability: 3.5\n\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nrandom_gabor (RandomGabor)   (None, 28, 28, 4)         26        \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 14, 14, 4)         0         \n_________________________________________________________________\nrandom_gabor_1 (RandomGabor) (None, 14, 14, 4)         26        \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 7, 7, 4)           0         \n_________________________________________________________________\nrandom_gabor_2 (RandomGabor) (None, 7, 7, 4)           26        \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 3, 3, 4)           0         \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 4)                 0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                50        \n=================================================================\nTotal params: 128\nTrainable params: 128\nNon-trainable params: 0\n_________________________________________________________________\n\n\nWe can show the initial Gabor filters:\n\nmodel.layers[0].show_filters()\n\n2022-09-08 10:28:50.838435: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-09-08 10:28:51.081995: I tensorflow/core/util/cuda_solvers.cc:180] Creating CudaSolver handles for stream 0x5626d6503e40\n\n\n\n\n\n\nmodel.layers[2].show_filters()\n\n\n\n\n\nmodel.layers[4].show_filters()\n\n\n\n\n\nhistory = model.fit(X_train, Y_train, batch_size=128, epochs=100, validation_split=0.2)\n\nEpoch 1/100\n\n\n2022-09-08 10:29:07.139561: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n2022-09-08 10:29:07.594658: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n\n\n375/375 [==============================] - 42s 79ms/step - loss: 653.6666 - accuracy: 0.1011 - val_loss: 59.0548 - val_accuracy: 0.0952\nEpoch 2/100\n375/375 [==============================] - 27s 73ms/step - loss: 13.6545 - accuracy: 0.1205 - val_loss: 5.5856 - val_accuracy: 0.1410\nEpoch 3/100\n375/375 [==============================] - 28s 73ms/step - loss: 4.9277 - accuracy: 0.1479 - val_loss: 4.3816 - val_accuracy: 0.1614\nEpoch 4/100\n375/375 [==============================] - 28s 74ms/step - loss: 4.1624 - accuracy: 0.1656 - val_loss: 3.8244 - val_accuracy: 0.1743\nEpoch 5/100\n375/375 [==============================] - 28s 73ms/step - loss: 3.7332 - accuracy: 0.1778 - val_loss: 3.4878 - val_accuracy: 0.1872\nEpoch 6/100\n375/375 [==============================] - 28s 76ms/step - loss: 3.4179 - accuracy: 0.1899 - val_loss: 3.2097 - val_accuracy: 0.2020\nEpoch 7/100\n375/375 [==============================] - 29s 77ms/step - loss: 3.1593 - accuracy: 0.1995 - val_loss: 2.9968 - val_accuracy: 0.2154\nEpoch 8/100\n375/375 [==============================] - 28s 75ms/step - loss: 2.9079 - accuracy: 0.2098 - val_loss: 2.7633 - val_accuracy: 0.2223\nEpoch 9/100\n375/375 [==============================] - 28s 74ms/step - loss: 2.7068 - accuracy: 0.2185 - val_loss: 2.5888 - val_accuracy: 0.2322\nEpoch 10/100\n375/375 [==============================] - 28s 75ms/step - loss: 2.5298 - accuracy: 0.2324 - val_loss: 2.4435 - val_accuracy: 0.2457\nEpoch 11/100\n375/375 [==============================] - 30s 79ms/step - loss: 2.3969 - accuracy: 0.2448 - val_loss: 2.3201 - val_accuracy: 0.2608\nEpoch 12/100\n375/375 [==============================] - 30s 79ms/step - loss: 2.2839 - accuracy: 0.2598 - val_loss: 2.2312 - val_accuracy: 0.2732\nEpoch 13/100\n375/375 [==============================] - 29s 78ms/step - loss: 2.1891 - accuracy: 0.2735 - val_loss: 2.1281 - val_accuracy: 0.2907\nEpoch 14/100\n375/375 [==============================] - 28s 75ms/step - loss: 2.0928 - accuracy: 0.2936 - val_loss: 2.0378 - val_accuracy: 0.3104\nEpoch 15/100\n375/375 [==============================] - 28s 74ms/step - loss: 2.0037 - accuracy: 0.3146 - val_loss: 1.9308 - val_accuracy: 0.3312\nEpoch 16/100\n375/375 [==============================] - 28s 74ms/step - loss: 1.9189 - accuracy: 0.3294 - val_loss: 1.8500 - val_accuracy: 0.3526\nEpoch 17/100\n375/375 [==============================] - 28s 74ms/step - loss: 1.8512 - accuracy: 0.3484 - val_loss: 1.7926 - val_accuracy: 0.3664\nEpoch 18/100\n375/375 [==============================] - 27s 73ms/step - loss: 1.8007 - accuracy: 0.3591 - val_loss: 1.7349 - val_accuracy: 0.3783\nEpoch 19/100\n375/375 [==============================] - 28s 74ms/step - loss: 1.7604 - accuracy: 0.3727 - val_loss: 1.7042 - val_accuracy: 0.3894\nEpoch 20/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.7277 - accuracy: 0.3843 - val_loss: 1.6782 - val_accuracy: 0.4116\nEpoch 21/100\n375/375 [==============================] - 28s 74ms/step - loss: 1.6941 - accuracy: 0.4007 - val_loss: 1.6352 - val_accuracy: 0.4244\nEpoch 22/100\n375/375 [==============================] - 28s 74ms/step - loss: 1.6631 - accuracy: 0.4174 - val_loss: 1.6047 - val_accuracy: 0.4425\nEpoch 23/100\n375/375 [==============================] - 28s 74ms/step - loss: 1.6350 - accuracy: 0.4287 - val_loss: 1.5722 - val_accuracy: 0.4591\nEpoch 24/100\n375/375 [==============================] - 28s 74ms/step - loss: 1.6128 - accuracy: 0.4395 - val_loss: 1.5530 - val_accuracy: 0.4661\nEpoch 25/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.5929 - accuracy: 0.4456 - val_loss: 1.5292 - val_accuracy: 0.4757\nEpoch 26/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.5705 - accuracy: 0.4547 - val_loss: 1.4995 - val_accuracy: 0.4823\nEpoch 27/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.5462 - accuracy: 0.4650 - val_loss: 1.4941 - val_accuracy: 0.4862\nEpoch 28/100\n375/375 [==============================] - 28s 74ms/step - loss: 1.5229 - accuracy: 0.4738 - val_loss: 1.4692 - val_accuracy: 0.4949\nEpoch 29/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.4977 - accuracy: 0.4862 - val_loss: 1.4285 - val_accuracy: 0.5090\nEpoch 30/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.4749 - accuracy: 0.4936 - val_loss: 1.4088 - val_accuracy: 0.5207\nEpoch 31/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.4462 - accuracy: 0.5045 - val_loss: 1.3710 - val_accuracy: 0.5249\nEpoch 32/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.4082 - accuracy: 0.5179 - val_loss: 1.3316 - val_accuracy: 0.5429\nEpoch 33/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.3805 - accuracy: 0.5292 - val_loss: 1.3333 - val_accuracy: 0.5463\nEpoch 34/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.3593 - accuracy: 0.5375 - val_loss: 1.2883 - val_accuracy: 0.5616\nEpoch 35/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.3404 - accuracy: 0.5429 - val_loss: 1.2852 - val_accuracy: 0.5618\nEpoch 36/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.3197 - accuracy: 0.5533 - val_loss: 1.2530 - val_accuracy: 0.5723\nEpoch 37/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.2879 - accuracy: 0.5671 - val_loss: 1.2333 - val_accuracy: 0.5827\nEpoch 38/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.2627 - accuracy: 0.5740 - val_loss: 1.1988 - val_accuracy: 0.5962\nEpoch 39/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.2437 - accuracy: 0.5809 - val_loss: 1.2039 - val_accuracy: 0.6038\nEpoch 40/100\n375/375 [==============================] - 28s 74ms/step - loss: 1.2247 - accuracy: 0.5883 - val_loss: 1.1786 - val_accuracy: 0.6095\nEpoch 41/100\n375/375 [==============================] - 29s 76ms/step - loss: 1.2114 - accuracy: 0.5945 - val_loss: 1.1565 - val_accuracy: 0.6105\nEpoch 42/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.1927 - accuracy: 0.5995 - val_loss: 1.1365 - val_accuracy: 0.6252\nEpoch 43/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.1662 - accuracy: 0.6095 - val_loss: 1.1077 - val_accuracy: 0.6388\nEpoch 44/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.1345 - accuracy: 0.6243 - val_loss: 1.0795 - val_accuracy: 0.6497\nEpoch 45/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.1041 - accuracy: 0.6346 - val_loss: 1.0438 - val_accuracy: 0.6623\nEpoch 46/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.0734 - accuracy: 0.6469 - val_loss: 1.0259 - val_accuracy: 0.6618\nEpoch 47/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.0530 - accuracy: 0.6536 - val_loss: 0.9971 - val_accuracy: 0.6688\nEpoch 48/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.0360 - accuracy: 0.6601 - val_loss: 0.9740 - val_accuracy: 0.6775\nEpoch 49/100\n375/375 [==============================] - 28s 74ms/step - loss: 1.0161 - accuracy: 0.6678 - val_loss: 0.9597 - val_accuracy: 0.6853\nEpoch 50/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9993 - accuracy: 0.6732 - val_loss: 0.9633 - val_accuracy: 0.6887\nEpoch 51/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9878 - accuracy: 0.6767 - val_loss: 0.9213 - val_accuracy: 0.6963\nEpoch 52/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9693 - accuracy: 0.6815 - val_loss: 0.9150 - val_accuracy: 0.6963\nEpoch 53/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9585 - accuracy: 0.6836 - val_loss: 0.9063 - val_accuracy: 0.6981\nEpoch 54/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9441 - accuracy: 0.6908 - val_loss: 0.9066 - val_accuracy: 0.6985\nEpoch 55/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9385 - accuracy: 0.6916 - val_loss: 0.8810 - val_accuracy: 0.7100\nEpoch 56/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9334 - accuracy: 0.6955 - val_loss: 0.8661 - val_accuracy: 0.7214\nEpoch 57/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9226 - accuracy: 0.6990 - val_loss: 0.8689 - val_accuracy: 0.7185\nEpoch 58/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9202 - accuracy: 0.6984 - val_loss: 0.8601 - val_accuracy: 0.7228\nEpoch 59/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9218 - accuracy: 0.6974 - val_loss: 0.8632 - val_accuracy: 0.7227\nEpoch 60/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9160 - accuracy: 0.6986 - val_loss: 0.8603 - val_accuracy: 0.7258\nEpoch 61/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9138 - accuracy: 0.7012 - val_loss: 0.8545 - val_accuracy: 0.7217\nEpoch 62/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9109 - accuracy: 0.7016 - val_loss: 0.8630 - val_accuracy: 0.7179\nEpoch 63/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9140 - accuracy: 0.7019 - val_loss: 0.8653 - val_accuracy: 0.7178\nEpoch 64/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9090 - accuracy: 0.7015 - val_loss: 0.8450 - val_accuracy: 0.7284\nEpoch 65/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9056 - accuracy: 0.7034 - val_loss: 0.8478 - val_accuracy: 0.7280\nEpoch 66/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9069 - accuracy: 0.7032 - val_loss: 0.8601 - val_accuracy: 0.7239\nEpoch 67/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9063 - accuracy: 0.7036 - val_loss: 0.8545 - val_accuracy: 0.7168\nEpoch 68/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9046 - accuracy: 0.7050 - val_loss: 0.8630 - val_accuracy: 0.7277\nEpoch 69/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.8998 - accuracy: 0.7072 - val_loss: 0.8557 - val_accuracy: 0.7254\nEpoch 70/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9087 - accuracy: 0.7030 - val_loss: 0.8442 - val_accuracy: 0.7303\nEpoch 71/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8981 - accuracy: 0.7069 - val_loss: 0.8607 - val_accuracy: 0.7179\nEpoch 72/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8987 - accuracy: 0.7067 - val_loss: 0.8549 - val_accuracy: 0.7214\nEpoch 73/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.8953 - accuracy: 0.7085 - val_loss: 0.8374 - val_accuracy: 0.7283\nEpoch 74/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8923 - accuracy: 0.7100 - val_loss: 0.8314 - val_accuracy: 0.7324\nEpoch 75/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8956 - accuracy: 0.7086 - val_loss: 0.8693 - val_accuracy: 0.7130\nEpoch 76/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8970 - accuracy: 0.7095 - val_loss: 0.8335 - val_accuracy: 0.7287\nEpoch 77/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8931 - accuracy: 0.7080 - val_loss: 0.8619 - val_accuracy: 0.7103\nEpoch 78/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8907 - accuracy: 0.7111 - val_loss: 0.8452 - val_accuracy: 0.7318\nEpoch 79/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8899 - accuracy: 0.7101 - val_loss: 0.8369 - val_accuracy: 0.7251\nEpoch 80/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8924 - accuracy: 0.7097 - val_loss: 0.8366 - val_accuracy: 0.7333\nEpoch 81/100\n375/375 [==============================] - 29s 76ms/step - loss: 0.8910 - accuracy: 0.7108 - val_loss: 0.8712 - val_accuracy: 0.7125\nEpoch 82/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8852 - accuracy: 0.7119 - val_loss: 0.8849 - val_accuracy: 0.7233\nEpoch 83/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.8891 - accuracy: 0.7113 - val_loss: 0.8336 - val_accuracy: 0.7329\nEpoch 84/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8858 - accuracy: 0.7141 - val_loss: 0.8644 - val_accuracy: 0.7207\nEpoch 85/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.8861 - accuracy: 0.7127 - val_loss: 0.8476 - val_accuracy: 0.7191\nEpoch 86/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8840 - accuracy: 0.7122 - val_loss: 0.8387 - val_accuracy: 0.7294\nEpoch 87/100\n375/375 [==============================] - 29s 77ms/step - loss: 0.8846 - accuracy: 0.7122 - val_loss: 0.8459 - val_accuracy: 0.7304\nEpoch 88/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.8829 - accuracy: 0.7134 - val_loss: 0.8384 - val_accuracy: 0.7262\nEpoch 89/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8864 - accuracy: 0.7113 - val_loss: 0.8607 - val_accuracy: 0.7198\nEpoch 90/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8854 - accuracy: 0.7124 - val_loss: 0.8565 - val_accuracy: 0.7194\nEpoch 91/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8861 - accuracy: 0.7124 - val_loss: 0.8266 - val_accuracy: 0.7332\nEpoch 92/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8858 - accuracy: 0.7121 - val_loss: 0.8546 - val_accuracy: 0.7209\nEpoch 93/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8913 - accuracy: 0.7106 - val_loss: 0.8341 - val_accuracy: 0.7286\nEpoch 94/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8796 - accuracy: 0.7159 - val_loss: 0.8439 - val_accuracy: 0.7284\nEpoch 95/100\n375/375 [==============================] - 29s 76ms/step - loss: 0.8777 - accuracy: 0.7164 - val_loss: 0.9002 - val_accuracy: 0.7197\nEpoch 96/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.8847 - accuracy: 0.7136 - val_loss: 0.8253 - val_accuracy: 0.7365\nEpoch 97/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.8764 - accuracy: 0.7174 - val_loss: 0.8461 - val_accuracy: 0.7309\nEpoch 98/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8807 - accuracy: 0.7151 - val_loss: 0.8299 - val_accuracy: 0.7341\nEpoch 99/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.8789 - accuracy: 0.7160 - val_loss: 0.8203 - val_accuracy: 0.7394\nEpoch 100/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.8808 - accuracy: 0.7144 - val_loss: 0.8302 - val_accuracy: 0.7424\n\n\nShowing the training dynamics:\n\nfig, axes = plt.subplots(1,2, figsize=(9,4))\naxes[0].plot(history.history['loss'][1:], label=\"Train\")\naxes[0].plot(history.history['val_loss'][1:], label=\"Val\")\naxes[1].plot(history.history['accuracy'], label=\"Train\")\naxes[1].plot(history.history['val_accuracy'], label=\"Val\")\nplt.legend()\nplt.show()\n\n\n\n\nCalculate the metrics in the test set:\n\nmodel.evaluate(X_test, Y_test, batch_size=128)\n\n79/79 [==============================] - 2s 29ms/step - loss: 0.8396 - accuracy: 0.7444\n\n\n[0.8396355509757996, 0.7444000244140625]\n\n\nWe can visualize the gabor filters after the training process:\n\nmodel.layers[0].show_filters()\n\n\n\n\n\nmodel.layers[2].show_filters()\n\n\n\n\n\nmodel.layers[4].show_filters()"
  },
  {
    "objectID": "Experiments/01_randomgabor.html",
    "href": "Experiments/01_randomgabor.html",
    "title": "Random Gabor experiment",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom einops import rearrange, repeat\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import mnist\n\nfrom flayers.layers import RandomGabor"
  },
  {
    "objectID": "Experiments/01_randomgabor.html#data-loading",
    "href": "Experiments/01_randomgabor.html#data-loading",
    "title": "Random Gabor experiment",
    "section": "Data loading",
    "text": "Data loading\n\nWe will be using MNIST for a simple and quick test.\n\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\nX_train = repeat(X_train, \"b h w ->  b h w c\", c=1)/255.0\nX_test = repeat(X_test, \"b h w ->  b h w c\", c=1)/255.0\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\n((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
  },
  {
    "objectID": "Experiments/01_randomgabor.html#definition-of-simple-model",
    "href": "Experiments/01_randomgabor.html#definition-of-simple-model",
    "title": "Random Gabor experiment",
    "section": "Definition of simple model",
    "text": "Definition of simple model\n\nmodel = tf.keras.Sequential([\n    RandomGabor(n_gabors=4, size=20, input_shape=(28,28,1)),\n    # layers.Conv2D(32, 3, input_shape=(28,28,1)),\n    layers.MaxPool2D(2),\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\n2022-09-06 11:22:17.064899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2373 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 780 Ti, pci bus id: 0000:02:00.0, compute capability: 3.5\n2022-09-06 11:22:17.065914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 151 MB memory:  -> device: 1, name: Tesla K40m, pci bus id: 0000:03:00.0, compute capability: 3.5\n2022-09-06 11:22:17.067063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 5435 MB memory:  -> device: 2, name: NVIDIA GeForce GTX TITAN Black, pci bus id: 0000:83:00.0, compute capability: 3.5\n2022-09-06 11:22:17.068686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 5435 MB memory:  -> device: 3, name: NVIDIA GeForce GTX TITAN Black, pci bus id: 0000:84:00.0, compute capability: 3.5\n\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nrandom_gabor (RandomGabor)   (None, 28, 28, 4)         26        \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 14, 14, 4)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 784)               0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                7850      \n=================================================================\nTotal params: 7,876\nTrainable params: 7,876\nNon-trainable params: 0\n_________________________________________________________________\n\n\nWe can show the initial Gabor filters:\n\nmodel.layers[0].show_filters()\n\n2022-09-06 11:22:19.809341: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-09-06 11:22:20.052736: I tensorflow/core/util/cuda_solvers.cc:180] Creating CudaSolver handles for stream 0x563ffaaf38c0\n\n\n\n\n\n\nhistory = model.fit(X_train, Y_train, batch_size=128, epochs=5)\n\nEpoch 1/5\n\n\n2022-09-06 11:22:26.673075: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n2022-09-06 11:22:27.122762: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n\n\n469/469 [==============================] - 18s 28ms/step - loss: 1.2855 - accuracy: 0.7523\nEpoch 2/5\n469/469 [==============================] - 13s 28ms/step - loss: 0.2952 - accuracy: 0.9105\nEpoch 3/5\n469/469 [==============================] - 13s 28ms/step - loss: 0.2307 - accuracy: 0.9303\nEpoch 4/5\n469/469 [==============================] - 13s 28ms/step - loss: 0.1971 - accuracy: 0.9409\nEpoch 5/5\n469/469 [==============================] - 13s 28ms/step - loss: 0.1745 - accuracy: 0.9478\n\n\nWe can visualize the gabor filters after the training process:\n\nmodel.layers[0].show_filters()"
  },
  {
    "objectID": "Experiments/00_gaborlayer.html",
    "href": "Experiments/00_gaborlayer.html",
    "title": "Gabor layer experiment",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom einops import rearrange, repeat\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import mnist\n\nfrom flayers.layers import GaborLayer"
  },
  {
    "objectID": "Experiments/00_gaborlayer.html#data-loading",
    "href": "Experiments/00_gaborlayer.html#data-loading",
    "title": "Gabor layer experiment",
    "section": "Data loading",
    "text": "Data loading\n\nWe will be using MNIST for a simple and quick test.\n\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\nX_train = repeat(X_train, \"b h w ->  b h w c\", c=1)/255.0\nX_test = repeat(X_test, \"b h w ->  b h w c\", c=1)/255.0\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\n((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
  },
  {
    "objectID": "Experiments/00_gaborlayer.html#definition-of-simple-model",
    "href": "Experiments/00_gaborlayer.html#definition-of-simple-model",
    "title": "Gabor layer experiment",
    "section": "Definition of simple model",
    "text": "Definition of simple model\n\nn_gabors = 4\nsigma_i = [0.1, 0.2]*2\nsigma_j = [0.2, 0.1]*2\nfreq = [10, 10]*2\ntheta = [0, np.pi/2]*2\nrot_theta = [0, 0]*2\nsigma_theta = [0, 0]*2\n\n\nmodel = tf.keras.Sequential([\n    GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n               theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20, input_shape=(28,28,1)),\n    layers.MaxPool2D(2),\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\nModel: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ngabor_layer_5 (GaborLayer)   (None, 28, 28, 4)         26        \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 14, 14, 4)         0         \n_________________________________________________________________\nglobal_average_pooling2d_4 ( (None, 4)                 0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 10)                50        \n=================================================================\nTotal params: 76\nTrainable params: 76\nNon-trainable params: 0\n_________________________________________________________________\n\n\nWe can show the initial Gabor filters:\n\nmodel.layers[0].show_filters()\n\n\n\n\n\nhistory = model.fit(X_train, Y_train, batch_size=128, epochs=5)\n\nEpoch 1/5\n469/469 [==============================] - 17s 28ms/step - loss: 2.3227 - accuracy: 0.1088\nEpoch 2/5\n469/469 [==============================] - 13s 28ms/step - loss: 2.2425 - accuracy: 0.1522\nEpoch 3/5\n469/469 [==============================] - 13s 28ms/step - loss: 1.9368 - accuracy: 0.3259\nEpoch 4/5\n469/469 [==============================] - 13s 28ms/step - loss: 1.7845 - accuracy: 0.3729\nEpoch 5/5\n469/469 [==============================] - 13s 28ms/step - loss: 1.7038 - accuracy: 0.3982\n\n\nWe can visualize the gabor filters after the training process:\n\nmodel.layers[0].show_filters()\n\n\n\n\nWe can even check the atributes of the layer to inspect the change in the initial parameters:\n\nmodel.layers[0].theta.numpy()*180/np.pi\n\narray([ 1.1418808e-02,  9.1543541e+01, -3.3004951e+00,  9.3963242e+01],\n      dtype=float32)\n\n\n\nmodel.layers[0].rot_theta.numpy()*180/np.pi\n\narray([-0.66917044, -1.573621  ,  4.7601485 , -2.2281935 ], dtype=float32)\n\n\n\nmodel.layers[0].sigma_theta.numpy()*180/np.pi\n\narray([ 12.963239,  16.60231 , -19.626122,  -2.529321], dtype=float32)"
  },
  {
    "objectID": "Experiments/05_multiple_randomgabor_relu.html",
    "href": "Experiments/05_multiple_randomgabor_relu.html",
    "title": "Multiple Random Gabor experiment",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom einops import rearrange, repeat\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import mnist\n\nfrom flayers.layers import RandomGabor"
  },
  {
    "objectID": "Experiments/05_multiple_randomgabor_relu.html#data-loading",
    "href": "Experiments/05_multiple_randomgabor_relu.html#data-loading",
    "title": "Multiple Random Gabor experiment",
    "section": "Data loading",
    "text": "Data loading\n\nWe will be using MNIST for a simple and quick test.\n\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\nX_train = repeat(X_train, \"b h w ->  b h w c\", c=1)/255.0\nX_test = repeat(X_test, \"b h w ->  b h w c\", c=1)/255.0\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\n((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
  },
  {
    "objectID": "Experiments/05_multiple_randomgabor_relu.html#definition-of-simple-model",
    "href": "Experiments/05_multiple_randomgabor_relu.html#definition-of-simple-model",
    "title": "Multiple Random Gabor experiment",
    "section": "Definition of simple model",
    "text": "Definition of simple model\n\nmodel = tf.keras.Sequential([\n    RandomGabor(n_gabors=4, size=20, input_shape=(28,28,1)),\n    layers.ReLU(),\n    layers.MaxPool2D(2),\n    RandomGabor(n_gabors=4, size=10),\n    layers.ReLU(),\n    layers.MaxPool2D(2),\n    RandomGabor(n_gabors=4, size=5),\n    layers.ReLU(),\n    layers.MaxPool2D(2),\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\n2022-09-08 12:06:36.641715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5435 MB memory:  -> device: 0, name: NVIDIA GeForce GTX TITAN Black, pci bus id: 0000:83:00.0, compute capability: 3.5\n\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nrandom_gabor (RandomGabor)   (None, 28, 28, 4)         26        \n_________________________________________________________________\nre_lu (ReLU)                 (None, 28, 28, 4)         0         \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 14, 14, 4)         0         \n_________________________________________________________________\nrandom_gabor_1 (RandomGabor) (None, 14, 14, 4)         26        \n_________________________________________________________________\nre_lu_1 (ReLU)               (None, 14, 14, 4)         0         \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 7, 7, 4)           0         \n_________________________________________________________________\nrandom_gabor_2 (RandomGabor) (None, 7, 7, 4)           26        \n_________________________________________________________________\nre_lu_2 (ReLU)               (None, 7, 7, 4)           0         \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 3, 3, 4)           0         \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 4)                 0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                50        \n=================================================================\nTotal params: 128\nTrainable params: 128\nNon-trainable params: 0\n_________________________________________________________________\n\n\nWe can show the initial Gabor filters:\n\nmodel.layers[0].show_filters()\n\n2022-09-08 12:06:40.240924: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-09-08 12:06:40.486401: I tensorflow/core/util/cuda_solvers.cc:180] Creating CudaSolver handles for stream 0x55a3b2715f00\n\n\n\n\n\n\nmodel.layers[3].show_filters()\n\n\n\n\n\nmodel.layers[6].show_filters()\n\n\n\n\n\nhistory = model.fit(X_train, Y_train, batch_size=128, epochs=100, validation_split=0.2)\n\nEpoch 1/100\n\n\n2022-09-08 12:07:26.699074: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n2022-09-08 12:07:27.154195: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n\n\n375/375 [==============================] - 44s 83ms/step - loss: 72.2522 - accuracy: 0.1398 - val_loss: 2.7318 - val_accuracy: 0.1739\nEpoch 2/100\n375/375 [==============================] - 29s 77ms/step - loss: 2.5426 - accuracy: 0.1832 - val_loss: 2.3943 - val_accuracy: 0.1903\nEpoch 3/100\n375/375 [==============================] - 30s 81ms/step - loss: 2.3536 - accuracy: 0.1959 - val_loss: 2.2703 - val_accuracy: 0.2134\nEpoch 4/100\n375/375 [==============================] - 29s 76ms/step - loss: 2.2619 - accuracy: 0.2111 - val_loss: 2.2346 - val_accuracy: 0.2205\nEpoch 5/100\n375/375 [==============================] - 29s 76ms/step - loss: 2.1907 - accuracy: 0.2223 - val_loss: 2.1368 - val_accuracy: 0.2283\nEpoch 6/100\n375/375 [==============================] - 29s 76ms/step - loss: 2.1296 - accuracy: 0.2389 - val_loss: 2.0727 - val_accuracy: 0.2532\nEpoch 7/100\n375/375 [==============================] - 29s 77ms/step - loss: 2.0777 - accuracy: 0.2534 - val_loss: 2.0683 - val_accuracy: 0.2573\nEpoch 8/100\n375/375 [==============================] - 29s 77ms/step - loss: 2.0325 - accuracy: 0.2734 - val_loss: 1.9766 - val_accuracy: 0.2992\nEpoch 9/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.9893 - accuracy: 0.2948 - val_loss: 1.9291 - val_accuracy: 0.3220\nEpoch 10/100\n375/375 [==============================] - 29s 76ms/step - loss: 1.9523 - accuracy: 0.3144 - val_loss: 1.8903 - val_accuracy: 0.3455\nEpoch 11/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.9158 - accuracy: 0.3333 - val_loss: 1.8665 - val_accuracy: 0.3576\nEpoch 12/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.8832 - accuracy: 0.3468 - val_loss: 1.8170 - val_accuracy: 0.3795\nEpoch 13/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.8468 - accuracy: 0.3625 - val_loss: 1.7824 - val_accuracy: 0.3868\nEpoch 14/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.8113 - accuracy: 0.3763 - val_loss: 1.7485 - val_accuracy: 0.3966\nEpoch 15/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.7753 - accuracy: 0.3881 - val_loss: 1.7290 - val_accuracy: 0.4121\nEpoch 16/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.7420 - accuracy: 0.4003 - val_loss: 1.6898 - val_accuracy: 0.4283\nEpoch 17/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.6988 - accuracy: 0.4162 - val_loss: 1.6385 - val_accuracy: 0.4499\nEpoch 18/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.6605 - accuracy: 0.4361 - val_loss: 1.5921 - val_accuracy: 0.4667\nEpoch 19/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.6065 - accuracy: 0.4590 - val_loss: 1.5227 - val_accuracy: 0.4949\nEpoch 20/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.5208 - accuracy: 0.4929 - val_loss: 1.4289 - val_accuracy: 0.5296\nEpoch 21/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.4161 - accuracy: 0.5218 - val_loss: 1.3047 - val_accuracy: 0.5573\nEpoch 22/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.3149 - accuracy: 0.5512 - val_loss: 1.2797 - val_accuracy: 0.5631\nEpoch 23/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.2306 - accuracy: 0.5781 - val_loss: 1.1715 - val_accuracy: 0.5974\nEpoch 24/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.1721 - accuracy: 0.6013 - val_loss: 1.0977 - val_accuracy: 0.6281\nEpoch 25/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.1540 - accuracy: 0.6081 - val_loss: 1.0598 - val_accuracy: 0.6478\nEpoch 26/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.1052 - accuracy: 0.6251 - val_loss: 1.0746 - val_accuracy: 0.6286\nEpoch 27/100\n375/375 [==============================] - 29s 76ms/step - loss: 1.0857 - accuracy: 0.6344 - val_loss: 1.0419 - val_accuracy: 0.6498\nEpoch 28/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.0805 - accuracy: 0.6365 - val_loss: 1.0425 - val_accuracy: 0.6453\nEpoch 29/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.0480 - accuracy: 0.6491 - val_loss: 1.0290 - val_accuracy: 0.6475\nEpoch 30/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.0522 - accuracy: 0.6472 - val_loss: 0.9822 - val_accuracy: 0.6736\nEpoch 31/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.0398 - accuracy: 0.6549 - val_loss: 0.9516 - val_accuracy: 0.6887\nEpoch 32/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.0281 - accuracy: 0.6577 - val_loss: 0.9446 - val_accuracy: 0.6825\nEpoch 33/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.0285 - accuracy: 0.6566 - val_loss: 0.9623 - val_accuracy: 0.6671\nEpoch 34/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.0102 - accuracy: 0.6639 - val_loss: 0.9425 - val_accuracy: 0.6833\nEpoch 35/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.0110 - accuracy: 0.6634 - val_loss: 1.0055 - val_accuracy: 0.6603\nEpoch 36/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.0061 - accuracy: 0.6651 - val_loss: 0.9460 - val_accuracy: 0.6768\nEpoch 37/100\n375/375 [==============================] - 28s 75ms/step - loss: 1.0029 - accuracy: 0.6631 - val_loss: 0.9528 - val_accuracy: 0.6789\nEpoch 38/100\n375/375 [==============================] - 29s 76ms/step - loss: 1.0100 - accuracy: 0.6606 - val_loss: 1.0218 - val_accuracy: 0.6573\nEpoch 39/100\n375/375 [==============================] - 29s 76ms/step - loss: 0.9972 - accuracy: 0.6646 - val_loss: 1.0173 - val_accuracy: 0.6571\nEpoch 40/100\n375/375 [==============================] - 28s 76ms/step - loss: 1.0011 - accuracy: 0.6624 - val_loss: 0.9416 - val_accuracy: 0.6853\nEpoch 41/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9854 - accuracy: 0.6705 - val_loss: 0.9618 - val_accuracy: 0.6740\nEpoch 42/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9939 - accuracy: 0.6640 - val_loss: 0.9237 - val_accuracy: 0.6837\nEpoch 43/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9993 - accuracy: 0.6624 - val_loss: 0.9294 - val_accuracy: 0.6919\nEpoch 44/100\n375/375 [==============================] - 29s 77ms/step - loss: 0.9923 - accuracy: 0.6644 - val_loss: 0.9168 - val_accuracy: 0.6864\nEpoch 45/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9947 - accuracy: 0.6643 - val_loss: 0.9236 - val_accuracy: 0.6819\nEpoch 46/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9823 - accuracy: 0.6699 - val_loss: 0.9321 - val_accuracy: 0.6806\nEpoch 47/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9965 - accuracy: 0.6628 - val_loss: 0.9290 - val_accuracy: 0.6889\nEpoch 48/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9854 - accuracy: 0.6664 - val_loss: 0.9273 - val_accuracy: 0.6888\nEpoch 49/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9810 - accuracy: 0.6697 - val_loss: 0.9430 - val_accuracy: 0.6790\nEpoch 50/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9804 - accuracy: 0.6695 - val_loss: 0.9362 - val_accuracy: 0.6916\nEpoch 51/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9837 - accuracy: 0.6665 - val_loss: 0.9167 - val_accuracy: 0.6879\nEpoch 52/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9890 - accuracy: 0.6671 - val_loss: 0.9152 - val_accuracy: 0.6937\nEpoch 53/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9709 - accuracy: 0.6731 - val_loss: 0.9280 - val_accuracy: 0.6895\nEpoch 54/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9793 - accuracy: 0.6698 - val_loss: 0.9112 - val_accuracy: 0.6961\nEpoch 55/100\n375/375 [==============================] - 29s 77ms/step - loss: 0.9699 - accuracy: 0.6737 - val_loss: 0.9613 - val_accuracy: 0.6847\nEpoch 56/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9747 - accuracy: 0.6721 - val_loss: 0.9234 - val_accuracy: 0.7009\nEpoch 57/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9744 - accuracy: 0.6719 - val_loss: 0.9092 - val_accuracy: 0.6988\nEpoch 58/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9682 - accuracy: 0.6736 - val_loss: 0.9387 - val_accuracy: 0.6836\nEpoch 59/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9770 - accuracy: 0.6707 - val_loss: 0.9507 - val_accuracy: 0.6712\nEpoch 60/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9757 - accuracy: 0.6717 - val_loss: 0.8992 - val_accuracy: 0.7005\nEpoch 61/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9748 - accuracy: 0.6733 - val_loss: 0.9299 - val_accuracy: 0.6841\nEpoch 62/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9720 - accuracy: 0.6718 - val_loss: 0.9042 - val_accuracy: 0.7007\nEpoch 63/100\n375/375 [==============================] - 29s 76ms/step - loss: 0.9681 - accuracy: 0.6752 - val_loss: 0.9029 - val_accuracy: 0.6945\nEpoch 64/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9641 - accuracy: 0.6769 - val_loss: 0.9030 - val_accuracy: 0.6973\nEpoch 65/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9641 - accuracy: 0.6763 - val_loss: 0.9218 - val_accuracy: 0.6927\nEpoch 66/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9630 - accuracy: 0.6777 - val_loss: 0.9043 - val_accuracy: 0.6955\nEpoch 67/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9764 - accuracy: 0.6725 - val_loss: 0.9018 - val_accuracy: 0.6957\nEpoch 68/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9608 - accuracy: 0.6779 - val_loss: 0.8989 - val_accuracy: 0.7021\nEpoch 69/100\n375/375 [==============================] - 29s 76ms/step - loss: 0.9610 - accuracy: 0.6790 - val_loss: 0.9225 - val_accuracy: 0.6985\nEpoch 70/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9566 - accuracy: 0.6799 - val_loss: 0.9383 - val_accuracy: 0.6965\nEpoch 71/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9670 - accuracy: 0.6740 - val_loss: 0.9634 - val_accuracy: 0.6783\nEpoch 72/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9768 - accuracy: 0.6704 - val_loss: 0.8960 - val_accuracy: 0.6963\nEpoch 73/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9547 - accuracy: 0.6817 - val_loss: 0.8929 - val_accuracy: 0.7088\nEpoch 74/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9583 - accuracy: 0.6801 - val_loss: 0.9462 - val_accuracy: 0.6812\nEpoch 75/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9512 - accuracy: 0.6834 - val_loss: 0.9067 - val_accuracy: 0.6951\nEpoch 76/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9681 - accuracy: 0.6751 - val_loss: 0.9263 - val_accuracy: 0.6942\nEpoch 77/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9467 - accuracy: 0.6843 - val_loss: 0.8915 - val_accuracy: 0.7018\nEpoch 78/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9539 - accuracy: 0.6815 - val_loss: 0.9162 - val_accuracy: 0.6876\nEpoch 79/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9466 - accuracy: 0.6836 - val_loss: 0.8996 - val_accuracy: 0.7031\nEpoch 80/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9515 - accuracy: 0.6822 - val_loss: 0.8824 - val_accuracy: 0.7060\nEpoch 81/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9513 - accuracy: 0.6832 - val_loss: 0.9449 - val_accuracy: 0.6869\nEpoch 82/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9485 - accuracy: 0.6830 - val_loss: 0.9464 - val_accuracy: 0.6816\nEpoch 83/100\n375/375 [==============================] - 29s 76ms/step - loss: 0.9471 - accuracy: 0.6840 - val_loss: 0.8862 - val_accuracy: 0.7048\nEpoch 84/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9470 - accuracy: 0.6851 - val_loss: 0.8803 - val_accuracy: 0.7063\nEpoch 85/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9455 - accuracy: 0.6842 - val_loss: 0.8786 - val_accuracy: 0.7119\nEpoch 86/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9478 - accuracy: 0.6850 - val_loss: 0.8852 - val_accuracy: 0.7023\nEpoch 87/100\n375/375 [==============================] - 28s 76ms/step - loss: 0.9485 - accuracy: 0.6830 - val_loss: 0.8809 - val_accuracy: 0.7080\nEpoch 88/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9559 - accuracy: 0.6818 - val_loss: 0.8808 - val_accuracy: 0.7091\nEpoch 89/100\n375/375 [==============================] - 28s 75ms/step - loss: 0.9559 - accuracy: 0.6814 - val_loss: 0.8960 - val_accuracy: 0.7004\nEpoch 90/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9468 - accuracy: 0.6835 - val_loss: 0.9058 - val_accuracy: 0.6975\nEpoch 91/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9457 - accuracy: 0.6834 - val_loss: 0.9527 - val_accuracy: 0.6716\nEpoch 92/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9540 - accuracy: 0.6804 - val_loss: 0.8798 - val_accuracy: 0.7125\nEpoch 93/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9497 - accuracy: 0.6837 - val_loss: 1.0125 - val_accuracy: 0.6543\nEpoch 94/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9473 - accuracy: 0.6834 - val_loss: 0.9361 - val_accuracy: 0.6777\nEpoch 95/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9494 - accuracy: 0.6825 - val_loss: 0.8761 - val_accuracy: 0.7042\nEpoch 96/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9420 - accuracy: 0.6860 - val_loss: 0.8932 - val_accuracy: 0.7014\nEpoch 97/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9460 - accuracy: 0.6827 - val_loss: 0.8747 - val_accuracy: 0.7082\nEpoch 98/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9389 - accuracy: 0.6847 - val_loss: 0.9072 - val_accuracy: 0.6951\nEpoch 99/100\n375/375 [==============================] - 28s 74ms/step - loss: 0.9416 - accuracy: 0.6837 - val_loss: 0.8890 - val_accuracy: 0.7036\nEpoch 100/100\n375/375 [==============================] - 28s 73ms/step - loss: 0.9349 - accuracy: 0.6880 - val_loss: 0.9311 - val_accuracy: 0.6885\n\n\nShowing the training dynamics:\n\nfig, axes = plt.subplots(1,2, figsize=(9,4))\naxes[0].plot(history.history['loss'][1:], label=\"Train\")\naxes[0].plot(history.history['val_loss'][1:], label=\"Val\")\naxes[1].plot(history.history['accuracy'], label=\"Train\")\naxes[1].plot(history.history['val_accuracy'], label=\"Val\")\nplt.legend()\nplt.show()\n\n\n\n\nCalculate the metrics in the test set:\n\nmodel.evaluate(X_test, Y_test, batch_size=128)\n\n79/79 [==============================] - 2s 30ms/step - loss: 0.9478 - accuracy: 0.6835\n\n\n[0.9478453993797302, 0.6834999918937683]\n\n\nWe can visualize the gabor filters after the training process:\n\nmodel.layers[0].show_filters()\n\n\n\n\n\nmodel.layers[3].show_filters()\n\n\n\n\n\nmodel.layers[6].show_filters()"
  },
  {
    "objectID": "Experiments/04_multiple_gaborlayer_relu.html",
    "href": "Experiments/04_multiple_gaborlayer_relu.html",
    "title": "Gabor layer experiment",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom einops import rearrange, repeat\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import mnist\n\nfrom flayers.layers import GaborLayer"
  },
  {
    "objectID": "Experiments/04_multiple_gaborlayer_relu.html#data-loading",
    "href": "Experiments/04_multiple_gaborlayer_relu.html#data-loading",
    "title": "Gabor layer experiment",
    "section": "Data loading",
    "text": "Data loading\n\nWe will be using MNIST for a simple and quick test.\n\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\nX_train = repeat(X_train, \"b h w ->  b h w c\", c=1)/255.0\nX_test = repeat(X_test, \"b h w ->  b h w c\", c=1)/255.0\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\n((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
  },
  {
    "objectID": "Experiments/04_multiple_gaborlayer_relu.html#definition-of-simple-model",
    "href": "Experiments/04_multiple_gaborlayer_relu.html#definition-of-simple-model",
    "title": "Gabor layer experiment",
    "section": "Definition of simple model",
    "text": "Definition of simple model\n\nn_gabors = 4\nsigma_i = [0.1, 0.2, 0.1, 0.1]\nsigma_j = [0.2, 0.1, 0.2, 0.2]\nfreq = [10, 10]*2\ntheta = [0, np.pi/2, np.pi/4, -np.pi/4]\nrot_theta = [0, 0]*2\nsigma_theta = [0, 0, np.pi/4, -np.pi/4]\n\n\nmodel = tf.keras.Sequential([\n    GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n               theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20, input_shape=(28,28,1)),\n    layers.ReLU(),\n    layers.MaxPool2D(2),\n    GaborLayer(n_gabors=n_gabors, size=10, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n               theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=10),\n    layers.ReLU(),\n    layers.MaxPool2D(2),\n    GaborLayer(n_gabors=n_gabors, size=5, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n               theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=5),\n    layers.ReLU(),\n    layers.MaxPool2D(2),\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ngabor_layer_3 (GaborLayer)   (None, 28, 28, 4)         26        \n_________________________________________________________________\nre_lu_3 (ReLU)               (None, 28, 28, 4)         0         \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 14, 14, 4)         0         \n_________________________________________________________________\ngabor_layer_4 (GaborLayer)   (None, 14, 14, 4)         26        \n_________________________________________________________________\nre_lu_4 (ReLU)               (None, 14, 14, 4)         0         \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 7, 7, 4)           0         \n_________________________________________________________________\ngabor_layer_5 (GaborLayer)   (None, 7, 7, 4)           26        \n_________________________________________________________________\nre_lu_5 (ReLU)               (None, 7, 7, 4)           0         \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 3, 3, 4)           0         \n_________________________________________________________________\nglobal_average_pooling2d_1 ( (None, 4)                 0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                50        \n=================================================================\nTotal params: 128\nTrainable params: 128\nNon-trainable params: 0\n_________________________________________________________________\n\n\nWe can show the initial Gabor filters:\n\nmodel.layers[0].show_filters()\n\n2022-09-08 12:04:47.616859: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-09-08 12:04:47.838710: I tensorflow/core/util/cuda_solvers.cc:180] Creating CudaSolver handles for stream 0x5600333a86c0\n\n\n\n\n\n\nmodel.layers[3].show_filters()\n\n\n\n\n\nmodel.layers[6].show_filters()\n\n\n\n\n\nhistory = model.fit(X_train, Y_train, batch_size=128, epochs=100, validation_split=0.2)\n\nEpoch 1/100\n\n\n2022-09-08 12:05:58.537282: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n2022-09-08 12:05:58.998947: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n\n\n375/375 [==============================] - 45s 88ms/step - loss: 985.9783 - accuracy: 0.1111 - val_loss: 3.0740 - val_accuracy: 0.0936\nEpoch 2/100\n375/375 [==============================] - 32s 85ms/step - loss: 2.5712 - accuracy: 0.0968 - val_loss: 2.4051 - val_accuracy: 0.0983\nEpoch 3/100\n375/375 [==============================] - 32s 84ms/step - loss: 2.3618 - accuracy: 0.1037 - val_loss: 2.3399 - val_accuracy: 0.1082\nEpoch 4/100\n375/375 [==============================] - 33s 87ms/step - loss: 2.3222 - accuracy: 0.1124 - val_loss: 2.3124 - val_accuracy: 0.1216\nEpoch 5/100\n375/375 [==============================] - 32s 86ms/step - loss: 2.3025 - accuracy: 0.1255 - val_loss: 2.2956 - val_accuracy: 0.1308\nEpoch 6/100\n375/375 [==============================] - 33s 87ms/step - loss: 2.2889 - accuracy: 0.1331 - val_loss: 2.2823 - val_accuracy: 0.1339\nEpoch 7/100\n375/375 [==============================] - 31s 84ms/step - loss: 2.2780 - accuracy: 0.1406 - val_loss: 2.2732 - val_accuracy: 0.1412\nEpoch 8/100\n375/375 [==============================] - 32s 84ms/step - loss: 2.2704 - accuracy: 0.1438 - val_loss: 2.2659 - val_accuracy: 0.1460\nEpoch 9/100\n375/375 [==============================] - 31s 84ms/step - loss: 2.2643 - accuracy: 0.1460 - val_loss: 2.2597 - val_accuracy: 0.1444\nEpoch 10/100\n375/375 [==============================] - 32s 85ms/step - loss: 2.2591 - accuracy: 0.1466 - val_loss: 2.2548 - val_accuracy: 0.1482\nEpoch 11/100\n375/375 [==============================] - 32s 84ms/step - loss: 2.2529 - accuracy: 0.1501 - val_loss: 2.2484 - val_accuracy: 0.1501\nEpoch 12/100\n375/375 [==============================] - 32s 84ms/step - loss: 2.2463 - accuracy: 0.1546 - val_loss: 2.2382 - val_accuracy: 0.1579\nEpoch 13/100\n375/375 [==============================] - 31s 84ms/step - loss: 2.2391 - accuracy: 0.1558 - val_loss: 2.2323 - val_accuracy: 0.1558\nEpoch 14/100\n375/375 [==============================] - 32s 84ms/step - loss: 2.2319 - accuracy: 0.1632 - val_loss: 2.2225 - val_accuracy: 0.1595\nEpoch 15/100\n375/375 [==============================] - 31s 83ms/step - loss: 2.2251 - accuracy: 0.1623 - val_loss: 2.2168 - val_accuracy: 0.1688\nEpoch 16/100\n375/375 [==============================] - 31s 84ms/step - loss: 2.2159 - accuracy: 0.1707 - val_loss: 2.2077 - val_accuracy: 0.1709\nEpoch 17/100\n375/375 [==============================] - 31s 83ms/step - loss: 2.2050 - accuracy: 0.1780 - val_loss: 2.1958 - val_accuracy: 0.1867\nEpoch 18/100\n375/375 [==============================] - 31s 82ms/step - loss: 2.1919 - accuracy: 0.1858 - val_loss: 2.1867 - val_accuracy: 0.1929\nEpoch 19/100\n375/375 [==============================] - 31s 81ms/step - loss: 2.1812 - accuracy: 0.1945 - val_loss: 2.1729 - val_accuracy: 0.1979\nEpoch 20/100\n375/375 [==============================] - 31s 83ms/step - loss: 2.1656 - accuracy: 0.2046 - val_loss: 2.1579 - val_accuracy: 0.2122\nEpoch 21/100\n375/375 [==============================] - 31s 82ms/step - loss: 2.1526 - accuracy: 0.2124 - val_loss: 2.1398 - val_accuracy: 0.2209\nEpoch 22/100\n375/375 [==============================] - 31s 84ms/step - loss: 2.1356 - accuracy: 0.2198 - val_loss: 2.1211 - val_accuracy: 0.2302\nEpoch 23/100\n375/375 [==============================] - 31s 83ms/step - loss: 2.1097 - accuracy: 0.2297 - val_loss: 2.1363 - val_accuracy: 0.1929\nEpoch 24/100\n375/375 [==============================] - 31s 82ms/step - loss: 2.0828 - accuracy: 0.2349 - val_loss: 2.0603 - val_accuracy: 0.2377\nEpoch 25/100\n375/375 [==============================] - 31s 82ms/step - loss: 2.0613 - accuracy: 0.2377 - val_loss: 2.0356 - val_accuracy: 0.2533\nEpoch 26/100\n375/375 [==============================] - 31s 83ms/step - loss: 2.0246 - accuracy: 0.2483 - val_loss: 2.0052 - val_accuracy: 0.2559\nEpoch 27/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.9939 - accuracy: 0.2626 - val_loss: 2.0181 - val_accuracy: 0.2536\nEpoch 28/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.9504 - accuracy: 0.2867 - val_loss: 1.9345 - val_accuracy: 0.2952\nEpoch 29/100\n375/375 [==============================] - 30s 81ms/step - loss: 1.9102 - accuracy: 0.2981 - val_loss: 1.8928 - val_accuracy: 0.3027\nEpoch 30/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.8791 - accuracy: 0.3022 - val_loss: 1.8656 - val_accuracy: 0.2966\nEpoch 31/100\n375/375 [==============================] - 31s 81ms/step - loss: 1.8436 - accuracy: 0.3085 - val_loss: 1.8246 - val_accuracy: 0.3078\nEpoch 32/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.8176 - accuracy: 0.3138 - val_loss: 1.7932 - val_accuracy: 0.3173\nEpoch 33/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.7868 - accuracy: 0.3216 - val_loss: 1.7776 - val_accuracy: 0.3264\nEpoch 34/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.7210 - accuracy: 0.3379 - val_loss: 1.6930 - val_accuracy: 0.3298\nEpoch 35/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.6528 - accuracy: 0.3550 - val_loss: 1.6709 - val_accuracy: 0.3572\nEpoch 36/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.6281 - accuracy: 0.3638 - val_loss: 1.6007 - val_accuracy: 0.3685\nEpoch 37/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.5943 - accuracy: 0.3723 - val_loss: 1.5715 - val_accuracy: 0.3918\nEpoch 38/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.5836 - accuracy: 0.3803 - val_loss: 1.5498 - val_accuracy: 0.3907\nEpoch 39/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.5560 - accuracy: 0.3921 - val_loss: 1.5089 - val_accuracy: 0.4304\nEpoch 40/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.5430 - accuracy: 0.4028 - val_loss: 1.4976 - val_accuracy: 0.4376\nEpoch 41/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.5249 - accuracy: 0.4141 - val_loss: 1.4813 - val_accuracy: 0.4534\nEpoch 42/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.4983 - accuracy: 0.4308 - val_loss: 1.4671 - val_accuracy: 0.4387\nEpoch 43/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.4679 - accuracy: 0.4423 - val_loss: 1.4519 - val_accuracy: 0.4453\nEpoch 44/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.4707 - accuracy: 0.4391 - val_loss: 1.3944 - val_accuracy: 0.4703\nEpoch 45/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.4273 - accuracy: 0.4564 - val_loss: 1.5095 - val_accuracy: 0.4157\nEpoch 46/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.3460 - accuracy: 0.4915 - val_loss: 1.2557 - val_accuracy: 0.5315\nEpoch 47/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.2945 - accuracy: 0.5171 - val_loss: 1.2103 - val_accuracy: 0.5444\nEpoch 48/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.2753 - accuracy: 0.5259 - val_loss: 1.3190 - val_accuracy: 0.5141\nEpoch 49/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.2489 - accuracy: 0.5384 - val_loss: 1.2835 - val_accuracy: 0.5042\nEpoch 50/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.2157 - accuracy: 0.5519 - val_loss: 1.1760 - val_accuracy: 0.5705\nEpoch 51/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.2159 - accuracy: 0.5510 - val_loss: 1.1809 - val_accuracy: 0.5684\nEpoch 52/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.2119 - accuracy: 0.5555 - val_loss: 1.1244 - val_accuracy: 0.5897\nEpoch 53/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1933 - accuracy: 0.5614 - val_loss: 1.1595 - val_accuracy: 0.5832\nEpoch 54/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.2085 - accuracy: 0.5562 - val_loss: 1.2195 - val_accuracy: 0.5597\nEpoch 55/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1991 - accuracy: 0.5624 - val_loss: 1.1500 - val_accuracy: 0.5898\nEpoch 56/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1975 - accuracy: 0.5623 - val_loss: 1.1141 - val_accuracy: 0.5968\nEpoch 57/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1818 - accuracy: 0.5651 - val_loss: 1.1461 - val_accuracy: 0.5820\nEpoch 58/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1922 - accuracy: 0.5619 - val_loss: 1.1506 - val_accuracy: 0.5747\nEpoch 59/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.2001 - accuracy: 0.5591 - val_loss: 1.1239 - val_accuracy: 0.5916\nEpoch 60/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1768 - accuracy: 0.5722 - val_loss: 1.1599 - val_accuracy: 0.5850\nEpoch 61/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1618 - accuracy: 0.5755 - val_loss: 1.0917 - val_accuracy: 0.6029\nEpoch 62/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1562 - accuracy: 0.5796 - val_loss: 1.1027 - val_accuracy: 0.5997\nEpoch 63/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1704 - accuracy: 0.5736 - val_loss: 1.0901 - val_accuracy: 0.6047\nEpoch 64/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1763 - accuracy: 0.5681 - val_loss: 1.1084 - val_accuracy: 0.6019\nEpoch 65/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1516 - accuracy: 0.5791 - val_loss: 1.1424 - val_accuracy: 0.5817\nEpoch 66/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1623 - accuracy: 0.5738 - val_loss: 1.0913 - val_accuracy: 0.5972\nEpoch 67/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1638 - accuracy: 0.5762 - val_loss: 1.1198 - val_accuracy: 0.6037\nEpoch 68/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1481 - accuracy: 0.5801 - val_loss: 1.0738 - val_accuracy: 0.6120\nEpoch 69/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1545 - accuracy: 0.5778 - val_loss: 1.2973 - val_accuracy: 0.5236\nEpoch 70/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1556 - accuracy: 0.5770 - val_loss: 1.1003 - val_accuracy: 0.6025\nEpoch 71/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1413 - accuracy: 0.5808 - val_loss: 1.1655 - val_accuracy: 0.5612\nEpoch 72/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1646 - accuracy: 0.5732 - val_loss: 1.0869 - val_accuracy: 0.6077\nEpoch 73/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1467 - accuracy: 0.5826 - val_loss: 1.1171 - val_accuracy: 0.5973\nEpoch 74/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1394 - accuracy: 0.5827 - val_loss: 1.0873 - val_accuracy: 0.6067\nEpoch 75/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1287 - accuracy: 0.5866 - val_loss: 1.0773 - val_accuracy: 0.6004\nEpoch 76/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1397 - accuracy: 0.5833 - val_loss: 1.1152 - val_accuracy: 0.5881\nEpoch 77/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1492 - accuracy: 0.5792 - val_loss: 1.1067 - val_accuracy: 0.5840\nEpoch 78/100\n375/375 [==============================] - 30s 81ms/step - loss: 1.1314 - accuracy: 0.5841 - val_loss: 1.0981 - val_accuracy: 0.5865\nEpoch 79/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1389 - accuracy: 0.5816 - val_loss: 1.0705 - val_accuracy: 0.6087\nEpoch 80/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1331 - accuracy: 0.5823 - val_loss: 1.0890 - val_accuracy: 0.5955\nEpoch 81/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1568 - accuracy: 0.5715 - val_loss: 1.0918 - val_accuracy: 0.6032\nEpoch 82/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1627 - accuracy: 0.5715 - val_loss: 1.0989 - val_accuracy: 0.6002\nEpoch 83/100\n375/375 [==============================] - 30s 81ms/step - loss: 1.1402 - accuracy: 0.5812 - val_loss: 1.0879 - val_accuracy: 0.6016\nEpoch 84/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1329 - accuracy: 0.5858 - val_loss: 1.0934 - val_accuracy: 0.6027\nEpoch 85/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1283 - accuracy: 0.5911 - val_loss: 1.0936 - val_accuracy: 0.6072\nEpoch 86/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1244 - accuracy: 0.5931 - val_loss: 1.1160 - val_accuracy: 0.5979\nEpoch 87/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1222 - accuracy: 0.5947 - val_loss: 1.0462 - val_accuracy: 0.6208\nEpoch 88/100\n375/375 [==============================] - 30s 81ms/step - loss: 1.1086 - accuracy: 0.5978 - val_loss: 1.0793 - val_accuracy: 0.6053\nEpoch 89/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1146 - accuracy: 0.5970 - val_loss: 1.0564 - val_accuracy: 0.6127\nEpoch 90/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1012 - accuracy: 0.6016 - val_loss: 1.1103 - val_accuracy: 0.6007\nEpoch 91/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1043 - accuracy: 0.6010 - val_loss: 1.0442 - val_accuracy: 0.6267\nEpoch 92/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1101 - accuracy: 0.6013 - val_loss: 1.1222 - val_accuracy: 0.5767\nEpoch 93/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.0980 - accuracy: 0.6058 - val_loss: 1.0606 - val_accuracy: 0.6108\nEpoch 94/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1073 - accuracy: 0.5988 - val_loss: 1.0959 - val_accuracy: 0.5970\nEpoch 95/100\n375/375 [==============================] - 32s 85ms/step - loss: 1.0962 - accuracy: 0.6022 - val_loss: 1.0617 - val_accuracy: 0.6246\nEpoch 96/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.1002 - accuracy: 0.6028 - val_loss: 1.0339 - val_accuracy: 0.6304\nEpoch 97/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.0971 - accuracy: 0.6040 - val_loss: 1.0482 - val_accuracy: 0.6246\nEpoch 98/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.0995 - accuracy: 0.6049 - val_loss: 1.0905 - val_accuracy: 0.6219\nEpoch 99/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.0929 - accuracy: 0.6069 - val_loss: 1.0765 - val_accuracy: 0.6123\nEpoch 100/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.0936 - accuracy: 0.6077 - val_loss: 1.0671 - val_accuracy: 0.6162\n\n\nShowing the training dynamics:\n\nfig, axes = plt.subplots(1,2, figsize=(9,4))\naxes[0].plot(history.history['loss'][1:], label=\"Train\")\naxes[0].plot(history.history['val_loss'][1:], label=\"Val\")\naxes[1].plot(history.history['accuracy'], label=\"Train\")\naxes[1].plot(history.history['val_accuracy'], label=\"Val\")\nplt.legend()\nplt.show()\n\n\n\n\nCalculate the metrics in the test set:\n\nmodel.evaluate(X_test, Y_test, batch_size=128)\n\n79/79 [==============================] - 3s 33ms/step - loss: 1.0718 - accuracy: 0.6153\n\n\n[1.071818232536316, 0.6152999997138977]\n\n\nWe can visualize the gabor filters after the training process:\n\nmodel.layers[0].show_filters()\n\n\n\n\n\nmodel.layers[2].show_filters()\n\nAttributeError: 'MaxPooling2D' object has no attribute 'show_filters'\n\n\n\nmodel.layers[4].show_filters()\n\n\n\n\nWe can even check the atributes of the layer to inspect the change in the initial parameters:\n\nmodel.layers[0].theta.numpy()*180/np.pi\n\narray([  0.38321877,  90.76502   ,  45.756405  , -47.088924  ],\n      dtype=float32)\n\n\n\nmodel.layers[0].rot_theta.numpy()*180/np.pi\n\narray([ 0.39840868, -0.8525029 , -0.04723335, -1.0895499 ], dtype=float32)\n\n\n\nmodel.layers[0].sigma_theta.numpy()*180/np.pi\n\narray([-45.138485,   9.870471,  55.021404, -44.676727], dtype=float32)"
  },
  {
    "objectID": "Experiments/02_multiple_gaborlayer.html",
    "href": "Experiments/02_multiple_gaborlayer.html",
    "title": "Gabor layer experiment",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom einops import rearrange, repeat\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import mnist\n\nfrom flayers.layers import GaborLayer"
  },
  {
    "objectID": "Experiments/02_multiple_gaborlayer.html#data-loading",
    "href": "Experiments/02_multiple_gaborlayer.html#data-loading",
    "title": "Gabor layer experiment",
    "section": "Data loading",
    "text": "Data loading\n\nWe will be using MNIST for a simple and quick test.\n\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\nX_train = repeat(X_train, \"b h w ->  b h w c\", c=1)/255.0\nX_test = repeat(X_test, \"b h w ->  b h w c\", c=1)/255.0\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\n((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
  },
  {
    "objectID": "Experiments/02_multiple_gaborlayer.html#definition-of-simple-model",
    "href": "Experiments/02_multiple_gaborlayer.html#definition-of-simple-model",
    "title": "Gabor layer experiment",
    "section": "Definition of simple model",
    "text": "Definition of simple model\n\nn_gabors = 4\nsigma_i = [0.1, 0.2, 0.1, 0.1]\nsigma_j = [0.2, 0.1, 0.2, 0.2]\nfreq = [10, 10]*2\ntheta = [0, np.pi/2, np.pi/4, -np.pi/4]\nrot_theta = [0, 0]*2\nsigma_theta = [0, 0, np.pi/4, -np.pi/4]\n\n\nmodel = tf.keras.Sequential([\n    GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n               theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20, input_shape=(28,28,1)),\n    layers.MaxPool2D(2),\n    GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n               theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20),\n    layers.MaxPool2D(2),\n    GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n               theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20),\n    layers.MaxPool2D(2),\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\n2022-09-08 10:34:16.837755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10793 MB memory:  -> device: 0, name: Tesla K40m, pci bus id: 0000:03:00.0, compute capability: 3.5\n\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ngabor_layer (GaborLayer)     (None, 28, 28, 4)         26        \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 14, 14, 4)         0         \n_________________________________________________________________\ngabor_layer_1 (GaborLayer)   (None, 14, 14, 4)         26        \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 7, 7, 4)           0         \n_________________________________________________________________\ngabor_layer_2 (GaborLayer)   (None, 7, 7, 4)           26        \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 3, 3, 4)           0         \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 4)                 0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                50        \n=================================================================\nTotal params: 128\nTrainable params: 128\nNon-trainable params: 0\n_________________________________________________________________\n\n\nWe can show the initial Gabor filters:\n\nmodel.layers[0].show_filters()\n\n2022-09-08 10:34:20.497150: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-09-08 10:34:20.716222: I tensorflow/core/util/cuda_solvers.cc:180] Creating CudaSolver handles for stream 0x55ce0961be70\n\n\n\n\n\n\nhistory = model.fit(X_train, Y_train, batch_size=128, epochs=100, validation_split=0.2)\n\nEpoch 1/100\n\n\n2022-09-08 10:34:35.055817: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n2022-09-08 10:34:35.517682: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n\n\n375/375 [==============================] - 48s 93ms/step - loss: 53.1879 - accuracy: 0.1049 - val_loss: 7.3154 - val_accuracy: 0.1034\nEpoch 2/100\n375/375 [==============================] - 32s 86ms/step - loss: 5.3019 - accuracy: 0.1095 - val_loss: 3.9903 - val_accuracy: 0.1092\nEpoch 3/100\n375/375 [==============================] - 32s 84ms/step - loss: 3.3966 - accuracy: 0.1096 - val_loss: 2.9894 - val_accuracy: 0.1128\nEpoch 4/100\n375/375 [==============================] - 32s 85ms/step - loss: 2.8143 - accuracy: 0.1138 - val_loss: 2.6610 - val_accuracy: 0.1093\nEpoch 5/100\n375/375 [==============================] - 31s 83ms/step - loss: 2.6076 - accuracy: 0.1096 - val_loss: 2.5345 - val_accuracy: 0.1093\nEpoch 6/100\n375/375 [==============================] - 32s 84ms/step - loss: 2.5094 - accuracy: 0.1115 - val_loss: 2.4872 - val_accuracy: 0.1034\nEpoch 7/100\n375/375 [==============================] - 32s 85ms/step - loss: 2.4797 - accuracy: 0.1102 - val_loss: 2.4527 - val_accuracy: 0.1149\nEpoch 8/100\n375/375 [==============================] - 31s 84ms/step - loss: 2.4442 - accuracy: 0.1089 - val_loss: 2.4189 - val_accuracy: 0.0995\nEpoch 9/100\n375/375 [==============================] - 32s 84ms/step - loss: 2.4111 - accuracy: 0.1136 - val_loss: 2.4106 - val_accuracy: 0.1101\nEpoch 10/100\n375/375 [==============================] - 31s 82ms/step - loss: 2.3790 - accuracy: 0.1223 - val_loss: 2.4078 - val_accuracy: 0.1067\nEpoch 11/100\n375/375 [==============================] - 31s 84ms/step - loss: 2.3550 - accuracy: 0.1175 - val_loss: 2.3210 - val_accuracy: 0.1312\nEpoch 12/100\n375/375 [==============================] - 31s 84ms/step - loss: 2.3327 - accuracy: 0.1205 - val_loss: 2.2847 - val_accuracy: 0.1443\nEpoch 13/100\n375/375 [==============================] - 31s 83ms/step - loss: 2.3041 - accuracy: 0.1376 - val_loss: 2.2463 - val_accuracy: 0.1593\nEpoch 14/100\n375/375 [==============================] - 32s 84ms/step - loss: 2.2641 - accuracy: 0.1545 - val_loss: 2.2457 - val_accuracy: 0.1663\nEpoch 15/100\n375/375 [==============================] - 32s 84ms/step - loss: 2.2258 - accuracy: 0.1766 - val_loss: 2.1766 - val_accuracy: 0.1941\nEpoch 16/100\n375/375 [==============================] - 31s 83ms/step - loss: 2.2026 - accuracy: 0.1923 - val_loss: 2.1463 - val_accuracy: 0.1999\nEpoch 17/100\n375/375 [==============================] - 31s 83ms/step - loss: 2.1474 - accuracy: 0.2165 - val_loss: 2.1283 - val_accuracy: 0.2132\nEpoch 18/100\n375/375 [==============================] - 31s 84ms/step - loss: 2.1159 - accuracy: 0.2290 - val_loss: 2.1280 - val_accuracy: 0.2051\nEpoch 19/100\n375/375 [==============================] - 31s 83ms/step - loss: 2.0732 - accuracy: 0.2499 - val_loss: 2.0583 - val_accuracy: 0.2422\nEpoch 20/100\n375/375 [==============================] - 31s 83ms/step - loss: 2.0469 - accuracy: 0.2644 - val_loss: 2.0433 - val_accuracy: 0.2635\nEpoch 21/100\n375/375 [==============================] - 31s 84ms/step - loss: 2.0072 - accuracy: 0.2793 - val_loss: 1.9514 - val_accuracy: 0.3022\nEpoch 22/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.9715 - accuracy: 0.2937 - val_loss: 1.9205 - val_accuracy: 0.3167\nEpoch 23/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.9412 - accuracy: 0.3064 - val_loss: 1.9068 - val_accuracy: 0.3210\nEpoch 24/100\n375/375 [==============================] - 32s 84ms/step - loss: 1.9011 - accuracy: 0.3265 - val_loss: 1.9031 - val_accuracy: 0.3142\nEpoch 25/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.8606 - accuracy: 0.3424 - val_loss: 1.8336 - val_accuracy: 0.3472\nEpoch 26/100\n375/375 [==============================] - 32s 85ms/step - loss: 1.8083 - accuracy: 0.3580 - val_loss: 1.7510 - val_accuracy: 0.3872\nEpoch 27/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.7367 - accuracy: 0.3804 - val_loss: 1.6704 - val_accuracy: 0.4014\nEpoch 28/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.6567 - accuracy: 0.4089 - val_loss: 1.6104 - val_accuracy: 0.4158\nEpoch 29/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.6048 - accuracy: 0.4255 - val_loss: 1.6363 - val_accuracy: 0.4035\nEpoch 30/100\n375/375 [==============================] - 32s 84ms/step - loss: 1.5808 - accuracy: 0.4336 - val_loss: 1.5532 - val_accuracy: 0.4347\nEpoch 31/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.5811 - accuracy: 0.4301 - val_loss: 1.5456 - val_accuracy: 0.4372\nEpoch 32/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.5584 - accuracy: 0.4395 - val_loss: 1.5234 - val_accuracy: 0.4523\nEpoch 33/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.5462 - accuracy: 0.4460 - val_loss: 1.5257 - val_accuracy: 0.4435\nEpoch 34/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.5447 - accuracy: 0.4480 - val_loss: 1.5121 - val_accuracy: 0.4565\nEpoch 35/100\n375/375 [==============================] - 32s 84ms/step - loss: 1.5296 - accuracy: 0.4507 - val_loss: 1.4809 - val_accuracy: 0.4688\nEpoch 36/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.5185 - accuracy: 0.4546 - val_loss: 1.4982 - val_accuracy: 0.4582\nEpoch 37/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.5169 - accuracy: 0.4600 - val_loss: 1.5149 - val_accuracy: 0.4624\nEpoch 38/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.5138 - accuracy: 0.4566 - val_loss: 1.4959 - val_accuracy: 0.4627\nEpoch 39/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.5086 - accuracy: 0.4624 - val_loss: 1.4718 - val_accuracy: 0.4706\nEpoch 40/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.4964 - accuracy: 0.4683 - val_loss: 1.4679 - val_accuracy: 0.4793\nEpoch 41/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.4886 - accuracy: 0.4685 - val_loss: 1.4985 - val_accuracy: 0.4667\nEpoch 42/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.4891 - accuracy: 0.4676 - val_loss: 1.4734 - val_accuracy: 0.4684\nEpoch 43/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.4848 - accuracy: 0.4669 - val_loss: 1.4741 - val_accuracy: 0.4692\nEpoch 44/100\n375/375 [==============================] - 32s 84ms/step - loss: 1.4766 - accuracy: 0.4696 - val_loss: 1.4268 - val_accuracy: 0.4825\nEpoch 45/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.4670 - accuracy: 0.4722 - val_loss: 1.4451 - val_accuracy: 0.4750\nEpoch 46/100\n375/375 [==============================] - 32s 84ms/step - loss: 1.4540 - accuracy: 0.4767 - val_loss: 1.4312 - val_accuracy: 0.4805\nEpoch 47/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.4428 - accuracy: 0.4812 - val_loss: 1.4503 - val_accuracy: 0.4914\nEpoch 48/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.4278 - accuracy: 0.4873 - val_loss: 1.4017 - val_accuracy: 0.4925\nEpoch 49/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.4165 - accuracy: 0.4905 - val_loss: 1.4208 - val_accuracy: 0.4890\nEpoch 50/100\n375/375 [==============================] - 32s 85ms/step - loss: 1.4010 - accuracy: 0.4993 - val_loss: 1.4191 - val_accuracy: 0.4840\nEpoch 51/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.4121 - accuracy: 0.4986 - val_loss: 1.4070 - val_accuracy: 0.4935\nEpoch 52/100\n375/375 [==============================] - 32s 84ms/step - loss: 1.3885 - accuracy: 0.5073 - val_loss: 1.3196 - val_accuracy: 0.5315\nEpoch 53/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.3459 - accuracy: 0.5233 - val_loss: 1.2969 - val_accuracy: 0.5447\nEpoch 54/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.3271 - accuracy: 0.5345 - val_loss: 1.2875 - val_accuracy: 0.5379\nEpoch 55/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.2776 - accuracy: 0.5522 - val_loss: 1.2711 - val_accuracy: 0.5491\nEpoch 56/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.2435 - accuracy: 0.5665 - val_loss: 1.1660 - val_accuracy: 0.5891\nEpoch 57/100\n375/375 [==============================] - 32s 84ms/step - loss: 1.2121 - accuracy: 0.5772 - val_loss: 1.2255 - val_accuracy: 0.5617\nEpoch 58/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.1980 - accuracy: 0.5801 - val_loss: 1.1337 - val_accuracy: 0.6062\nEpoch 59/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1790 - accuracy: 0.5876 - val_loss: 1.1356 - val_accuracy: 0.6012\nEpoch 60/100\n375/375 [==============================] - 32s 84ms/step - loss: 1.1755 - accuracy: 0.5922 - val_loss: 1.1313 - val_accuracy: 0.6167\nEpoch 61/100\n375/375 [==============================] - 32s 84ms/step - loss: 1.1651 - accuracy: 0.5947 - val_loss: 1.0896 - val_accuracy: 0.6202\nEpoch 62/100\n375/375 [==============================] - 32s 85ms/step - loss: 1.1618 - accuracy: 0.5923 - val_loss: 1.1167 - val_accuracy: 0.6120\nEpoch 63/100\n375/375 [==============================] - 32s 84ms/step - loss: 1.1650 - accuracy: 0.5919 - val_loss: 1.1560 - val_accuracy: 0.5888\nEpoch 64/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1635 - accuracy: 0.5930 - val_loss: 1.1428 - val_accuracy: 0.5931\nEpoch 65/100\n375/375 [==============================] - 32s 84ms/step - loss: 1.1531 - accuracy: 0.5959 - val_loss: 1.1195 - val_accuracy: 0.6190\nEpoch 66/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.1429 - accuracy: 0.5994 - val_loss: 1.1284 - val_accuracy: 0.6047\nEpoch 67/100\n375/375 [==============================] - 32s 85ms/step - loss: 1.1529 - accuracy: 0.5981 - val_loss: 1.0952 - val_accuracy: 0.5988\nEpoch 68/100\n375/375 [==============================] - 32s 85ms/step - loss: 1.1473 - accuracy: 0.5980 - val_loss: 1.1129 - val_accuracy: 0.6061\nEpoch 69/100\n375/375 [==============================] - 32s 84ms/step - loss: 1.1499 - accuracy: 0.5992 - val_loss: 1.1366 - val_accuracy: 0.5915\nEpoch 70/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.1427 - accuracy: 0.6019 - val_loss: 1.1129 - val_accuracy: 0.6050\nEpoch 71/100\n375/375 [==============================] - 32s 84ms/step - loss: 1.1423 - accuracy: 0.6016 - val_loss: 1.0861 - val_accuracy: 0.6235\nEpoch 72/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1442 - accuracy: 0.6015 - val_loss: 1.0953 - val_accuracy: 0.6167\nEpoch 73/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1404 - accuracy: 0.6027 - val_loss: 1.0821 - val_accuracy: 0.6258\nEpoch 74/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.1381 - accuracy: 0.6028 - val_loss: 1.0807 - val_accuracy: 0.6184\nEpoch 75/100\n375/375 [==============================] - 32s 84ms/step - loss: 1.1371 - accuracy: 0.6028 - val_loss: 1.1134 - val_accuracy: 0.6093\nEpoch 76/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.1368 - accuracy: 0.6026 - val_loss: 1.0980 - val_accuracy: 0.6192\nEpoch 77/100\n375/375 [==============================] - 32s 84ms/step - loss: 1.1358 - accuracy: 0.6011 - val_loss: 1.0954 - val_accuracy: 0.6200\nEpoch 78/100\n375/375 [==============================] - 32s 84ms/step - loss: 1.1355 - accuracy: 0.6033 - val_loss: 1.0864 - val_accuracy: 0.6213\nEpoch 79/100\n375/375 [==============================] - 31s 84ms/step - loss: 1.1341 - accuracy: 0.6044 - val_loss: 1.0845 - val_accuracy: 0.6324\nEpoch 80/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1281 - accuracy: 0.6066 - val_loss: 1.0840 - val_accuracy: 0.6214\nEpoch 81/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1289 - accuracy: 0.6078 - val_loss: 1.0778 - val_accuracy: 0.6225\nEpoch 82/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1215 - accuracy: 0.6140 - val_loss: 1.0911 - val_accuracy: 0.6226\nEpoch 83/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1226 - accuracy: 0.6105 - val_loss: 1.0986 - val_accuracy: 0.6091\nEpoch 84/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1257 - accuracy: 0.6101 - val_loss: 1.0672 - val_accuracy: 0.6291\nEpoch 85/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1243 - accuracy: 0.6114 - val_loss: 1.1310 - val_accuracy: 0.5987\nEpoch 86/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1177 - accuracy: 0.6140 - val_loss: 1.1025 - val_accuracy: 0.6118\nEpoch 87/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1271 - accuracy: 0.6094 - val_loss: 1.0622 - val_accuracy: 0.6336\nEpoch 88/100\n375/375 [==============================] - 31s 82ms/step - loss: 1.1154 - accuracy: 0.6149 - val_loss: 1.0563 - val_accuracy: 0.6302\nEpoch 89/100\n375/375 [==============================] - 31s 83ms/step - loss: 1.1144 - accuracy: 0.6124 - val_loss: 1.0903 - val_accuracy: 0.6252\nEpoch 90/100\n375/375 [==============================] - 32s 86ms/step - loss: 1.1130 - accuracy: 0.6158 - val_loss: 1.0602 - val_accuracy: 0.6350\nEpoch 91/100\n375/375 [==============================] - 32s 86ms/step - loss: 1.1144 - accuracy: 0.6123 - val_loss: 1.1094 - val_accuracy: 0.6087\nEpoch 92/100\n375/375 [==============================] - 32s 86ms/step - loss: 1.1163 - accuracy: 0.6133 - val_loss: 1.0637 - val_accuracy: 0.6292\nEpoch 93/100\n375/375 [==============================] - 33s 87ms/step - loss: 1.1192 - accuracy: 0.6123 - val_loss: 1.0625 - val_accuracy: 0.6440\nEpoch 94/100\n375/375 [==============================] - 32s 86ms/step - loss: 1.1050 - accuracy: 0.6199 - val_loss: 1.1102 - val_accuracy: 0.6156\nEpoch 95/100\n375/375 [==============================] - 32s 86ms/step - loss: 1.1138 - accuracy: 0.6153 - val_loss: 1.0942 - val_accuracy: 0.6236\nEpoch 96/100\n375/375 [==============================] - 32s 86ms/step - loss: 1.1144 - accuracy: 0.6158 - val_loss: 1.0695 - val_accuracy: 0.6322\nEpoch 97/100\n375/375 [==============================] - 32s 86ms/step - loss: 1.1093 - accuracy: 0.6160 - val_loss: 1.0768 - val_accuracy: 0.6174\nEpoch 98/100\n375/375 [==============================] - 32s 86ms/step - loss: 1.1074 - accuracy: 0.6148 - val_loss: 1.0555 - val_accuracy: 0.6313\nEpoch 99/100\n375/375 [==============================] - 32s 86ms/step - loss: 1.1076 - accuracy: 0.6152 - val_loss: 1.0380 - val_accuracy: 0.6418\nEpoch 100/100\n375/375 [==============================] - 33s 87ms/step - loss: 1.1100 - accuracy: 0.6158 - val_loss: 1.0540 - val_accuracy: 0.6360\n\n\nShowing the training dynamics:\n\nfig, axes = plt.subplots(1,2, figsize=(9,4))\naxes[0].plot(history.history['loss'][1:], label=\"Train\")\naxes[0].plot(history.history['val_loss'][1:], label=\"Val\")\naxes[1].plot(history.history['accuracy'], label=\"Train\")\naxes[1].plot(history.history['val_accuracy'], label=\"Val\")\nplt.legend()\nplt.show()\n\n\n\n\nCalculate the metrics in the test set:\n\nmodel.evaluate(X_test, Y_test, batch_size=128)\n\n79/79 [==============================] - 3s 36ms/step - loss: 1.0928 - accuracy: 0.6304\n\n\n[1.0928494930267334, 0.6304000020027161]\n\n\nWe can visualize the gabor filters after the training process:\n\nmodel.layers[0].show_filters()\n\n\n\n\n\nmodel.layers[2].show_filters()\n\n\n\n\n\nmodel.layers[4].show_filters()\n\n\n\n\nWe can even check the atributes of the layer to inspect the change in the initial parameters:\n\nmodel.layers[0].theta.numpy()*180/np.pi\n\narray([  0.38321877,  90.76502   ,  45.756405  , -47.088924  ],\n      dtype=float32)\n\n\n\nmodel.layers[0].rot_theta.numpy()*180/np.pi\n\narray([ 0.39840868, -0.8525029 , -0.04723335, -1.0895499 ], dtype=float32)\n\n\n\nmodel.layers[0].sigma_theta.numpy()*180/np.pi\n\narray([-45.138485,   9.870471,  55.021404, -44.676727], dtype=float32)"
  },
  {
    "objectID": "layers.html",
    "href": "layers.html",
    "title": "Layers",
    "section": "",
    "text": "Convolutional layer that forces a functional Gabor form for its filters. Every parameter of the Gabor can be learnt."
  },
  {
    "objectID": "layers.html#managing-dtype",
    "href": "layers.html#managing-dtype",
    "title": "Layers",
    "section": "Managing dtype",
    "text": "Managing dtype\nTensorflow is a bit picky when it comes to dtype, so it can be useful to define a function that will ensure that every parameter is casted to the same dtype:\n\na, b = tf.convert_to_tensor(1), tf.convert_to_tensor(1.1)\nprint(a.dtype, b.dtype)\n# assert a.dtype != b.dtype\n\n<dtype: 'int32'> <dtype: 'float32'>\n\n\n2022-09-14 10:15:15.285025: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n2022-09-14 10:15:15.285095: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: megatron\n2022-09-14 10:15:15.285109: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: megatron\n2022-09-14 10:15:15.285246: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.57.2\n2022-09-14 10:15:15.285298: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.57.2\n2022-09-14 10:15:15.285310: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.57.2\n\n\n\nc, d = cast_all(a, b)\nprint(c.dtype, d.dtype)\nassert c.dtype == d.dtype\n\n<dtype: 'float32'> <dtype: 'float32'>"
  },
  {
    "objectID": "layers.html#creating-a-gabor-filter-in-tensorflow",
    "href": "layers.html#creating-a-gabor-filter-in-tensorflow",
    "title": "Layers",
    "section": "Creating a Gabor filter in TensorFlow",
    "text": "Creating a Gabor filter in TensorFlow\nFirst of all we need to be able to generate Gabor filters as Tensorflow Tensor:\n\nsource\n\ngabor_2d_tf\n\n gabor_2d_tf (i, j, imean, jmean, sigma_i, sigma_j, freq, theta,\n              sigma_theta)\n\n\n\n\n\nDetails\n\n\n\n\ni\nHorizontal domain\n\n\nj\nVertical domain\n\n\nimean\nHorizontal mean\n\n\njmean\nVertical mean\n\n\nsigma_i\nHorizontal width\n\n\nsigma_j\nVertical width\n\n\nfreq\nFrecuency of the filter\n\n\ntheta\nAngle of the filter\n\n\nsigma_theta\nWidth of the angle?? Rotation of the domain??\n\n\n\n\nsource\n\n\ncreate_gabor_rot_tf\n\n create_gabor_rot_tf (Nrows, Ncols, imean, jmean, sigma_i, sigma_j, freq,\n                      theta, rot_theta, sigma_theta, fs)\n\nCreates a rotated Gabor filter with the input parameters.\n\n\n\n\nDetails\n\n\n\n\nNrows\nNumber of horizontal pixels\n\n\nNcols\nNumber of vertical pixels\n\n\nimean\nHorizontal mean (in degrees)\n\n\njmean\nVertical mean (in degrees)\n\n\nsigma_i\nHorizontal width (in degrees)\n\n\nsigma_j\nVertical width (in degrees)\n\n\nfreq\nFrequency\n\n\ntheta\nAngle\n\n\nrot_theta\nRotation of the domain??\n\n\nsigma_theta\nWidth of the angle?? Rotation of the domain??\n\n\nfs\nSampling frequency\n\n\n\n\ngabor = create_gabor_rot_tf(Nrows=20, Ncols=20, imean=0.5, jmean=0.5, sigma_i=0.1, sigma_j=0.1, freq=10, theta=0, rot_theta=0, sigma_theta=0, fs=20)\nplt.imshow(gabor)\nplt.show()\n\n2022-09-14 10:15:16.577480: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)"
  },
  {
    "objectID": "layers.html#creating-a-set-of-gabor-filters",
    "href": "layers.html#creating-a-set-of-gabor-filters",
    "title": "Layers",
    "section": "Creating a set of Gabor filters",
    "text": "Creating a set of Gabor filters\n\nIt can be a little bit tricky to translate plain Python for loops into tf.function.\n\nIn plain Python, if we wanted to create a set of filters we could initialize an empty array or list and fill it with the different filters generated inside a for loop, but we cant do that inside a tf.function because Tensorflow tries to build the computational graph and starts to nest graphs inside graphs and the performance is terrible. Luckily for us, they implement a tf.TensorArray that can be used inside a tf.function to this effect.\n\nsource\n\ncreate_multiple_different_rot_gabor_tf\n\n create_multiple_different_rot_gabor_tf (n_gabors, Nrows, Ncols, imean,\n                                         jmean, sigma_i:list,\n                                         sigma_j:list, freq:list,\n                                         theta:list, rot_theta:list,\n                                         sigma_theta:list, fs,\n                                         normalize:bool=True)\n\nCreates a set of Gabor filters.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_gabors\n\n\nNumber of Gabor filters we want to create.\n\n\nNrows\n\n\nNumber of horizontal pixels.\n\n\nNcols\n\n\nNumber of vertical pixels.\n\n\nimean\n\n\nHorizontal mean (in degrees).\n\n\njmean\n\n\nVertical mean (in degrees).\n\n\nsigma_i\nlist\n\nHorizontal width (in degrees).\n\n\nsigma_j\nlist\n\nVertical width (in degrees).\n\n\nfreq\nlist\n\nFrequency.\n\n\ntheta\nlist\n\nAngle.\n\n\nrot_theta\nlist\n\nRotation of the domain??\n\n\nsigma_theta\nlist\n\nWidth of the angle?? Rotation of the domain??\n\n\nfs\n\n\nSampling frequency.\n\n\nnormalize\nbool\nTrue\nWether to normalize (and divide by n_gabors) or not the Gabors.\n\n\n\n\nn_gabors = 4\ngabors = create_multiple_different_rot_gabor_tf(n_gabors=n_gabors, Nrows=20, Ncols=20, imean=0.5, jmean=0.5, sigma_i=[0.1]*n_gabors, sigma_j=[0.1]*n_gabors, freq=[10]*n_gabors, \n                                                theta=[0]*n_gabors, rot_theta=[0]*n_gabors, sigma_theta=[0]*n_gabors, fs=20)\ngabors.shape\n\n2022-09-14 10:15:18.653420: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at functional_ops.cc:374 : Internal: No function library\n2022-09-14 10:15:18.654478: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at functional_ops.cc:374 : Internal: No function library\n2022-09-14 10:15:18.655449: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at functional_ops.cc:374 : Internal: No function library\n\n\nTensorShape([4, 20, 20])\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\nfor gabor_filter, ax in zip(gabors, axes.ravel()):\n    ax.imshow(gabor_filter)\nplt.show()\n\n\n\n\nWe can, as well, change the parameters of the Gabor filters independently:\n\nn_gabors = 4\nsigma_i = [0.1, 0.2, 0.3, 0.4]\nsigma_j = [0.1, 0.2, 0.3, 0.4]\nfreq = [10, 20, 30, 40]\ntheta = [0, 45, 90, 135]\nrot_theta = [0, 45, 90, 135]\nsigma_theta = [0, 45, 90, 135]\ngabors = create_multiple_different_rot_gabor_tf(n_gabors=n_gabors, Nrows=20, Ncols=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n                                                theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20)\ngabors.shape\n\n2022-09-14 10:15:19.839502: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at functional_ops.cc:374 : Internal: No function library\n2022-09-14 10:15:19.840573: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at functional_ops.cc:374 : Internal: No function library\n2022-09-14 10:15:19.841554: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at functional_ops.cc:374 : Internal: No function library\n\n\nTensorShape([4, 20, 20])\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\nfor gabor_filter, ax in zip(gabors, axes.ravel()):\n    ax.imshow(gabor_filter)\nplt.show()"
  },
  {
    "objectID": "layers.html#gabor-layer-1",
    "href": "layers.html#gabor-layer-1",
    "title": "Layers",
    "section": "Gabor layer",
    "text": "Gabor layer\n\nGabor layer with pre-defined values.\n\n\nsource\n\nGaborLayer\n\n GaborLayer (*args, **kwargs)\n\nPre-initialized Gabor layer that is trainable through backpropagation.\n\na = GaborLayer(1, 20, 0.5, 0.5, [1], [1], [1], [1], [1], [1], [1])\n\n\nsource\n\n\nGaborLayer.call\n\n GaborLayer.call (inputs)\n\nBuild a set of filters from the stored values and convolve them with the input.\n\n\n\n\nDetails\n\n\n\n\ninputs\nInputs to the layer.\n\n\n\n\nn_gabors = 4\nsigma_i = [0.1, 0.2, 0.3, 0.4]\nsigma_j = [0.1, 0.2, 0.3, 0.4]\nfreq = [10, 20, 30, 40]\ntheta = [0, 45, 90, 135]\nrot_theta = [0, 45, 90, 135]\nsigma_theta = [0, 45, 90, 135]\n\ngaborlayer = GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n                        theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20)\nsample_input = np.random.uniform(0, 1, size=(1, 256, 256, 1))\nsample_output = gaborlayer(sample_input).numpy()\nassert sample_input.shape[1:3] == sample_output.shape[1:3]\nassert sample_output.shape[-1] == 4\nfig, axes = plt.subplots(1,2)\naxes[0].imshow(sample_input.squeeze())\naxes[1].imshow(sample_output.squeeze())\naxes[0].set_title(\"Input\")\naxes[1].set_title(\"Output\")\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\nIt can be helpful to implement a method to show the Gabor filters that are being used by the layer:\n\nsource\n\n\nGaborLayer.show_filters\n\n GaborLayer.show_filters ()\n\nCalculates and plots the filters corresponding to the stored parameters.\n\ngaborlayer = GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n                        theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20)\ngaborlayer.show_filters()\n\n\n\n\nWe can check that the parameters are trainable and thus the gradient is propagated properly:\n\ngaborlayer = GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n                        theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20)\nwith tf.GradientTape() as tape:\n    output = gaborlayer(sample_input)\n    loss = output - output**2\ngradients = tape.gradient(loss, gaborlayer.trainable_variables)\ngradients\n\n[<tf.Tensor: shape=(), dtype=float32, numpy=-44292210.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=12047717.0>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=\n array([-4.6640164e+04, -8.1807392e+07, -1.4079159e+05, -1.2425600e+08],\n       dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=\n array([-4.6981230e+04, -8.1834672e+07, -1.3268044e+05, -1.2433401e+08],\n       dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=\n array([-9.2944937e+00, -2.6185302e+05, -2.2824429e+03, -6.3666250e+06],\n       dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=\n array([ 8.6009467e-01, -9.6875000e+02,  1.1099043e+03, -8.2623000e+05],\n       dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=\n array([-8.5969925e-01,  9.6850000e+02, -1.1098975e+03,  8.2623300e+05],\n       dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.        ,  0.        , -0.00195312,  1.        ], dtype=float32)>]"
  },
  {
    "objectID": "layers.html#random-initialize-a-simple-set-of-gabor-filters",
    "href": "layers.html#random-initialize-a-simple-set-of-gabor-filters",
    "title": "Layers",
    "section": "Random initialize a simple set of Gabor filters",
    "text": "Random initialize a simple set of Gabor filters\n\nInsted of defining ourselves the initial values, we can randomly initialize them. This can speed up our testing.\n\n\nsource\n\ncreate_simple_random_set\n\n create_simple_random_set (n_gabors, size)\n\nCreates a simple set of randomly initialized squared Gabor filters.\n\n\n\n\nDetails\n\n\n\n\nn_gabors\nNumber of Gabor filters we want to create.\n\n\nsize\nSize of the Gabor (they will be square).\n\n\n\n\ngabors = create_simple_random_set(n_gabors=4, size=20)\ngabors.shape\n\nTensorShape([4, 20, 20])\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\nfor gabor_filter, ax in zip(gabors, axes.ravel()):\n    ax.imshow(gabor_filter)\nplt.show()"
  },
  {
    "objectID": "layers.html#randomgabor",
    "href": "layers.html#randomgabor",
    "title": "Layers",
    "section": "RandomGabor",
    "text": "RandomGabor\n\nActually, we can define a different layer that initializes almost all of its parameters randomly by inhereting from GaborLayer.\n\n\nsource\n\nRandomGabor\n\n RandomGabor (*args, **kwargs)\n\nRandomly initialized Gabor layer that is trainable through backpropagation.\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\n\nWe can check that the .call() method from GaborLayer still works:\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\nsample_input = np.random.uniform(0, 1, size=(1, 256, 256, 1))\nsample_output = gaborlayer(sample_input).numpy()\nassert sample_input.shape[1:3] == sample_output.shape[1:3]\nassert sample_output.shape[-1] == 4\nfig, axes = plt.subplots(1,2)\naxes[0].imshow(sample_input.squeeze())\naxes[1].imshow(sample_output.squeeze())\naxes[0].set_title(\"Input\")\naxes[1].set_title(\"Output\")\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\nEnsure that we can still plot its filters:\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\ngaborlayer.show_filters()\n\n\n\n\nWe can check that the parameters are trainable and thus the gradient is propagated properly:\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\nwith tf.GradientTape() as tape:\n    output = gaborlayer(sample_input)\n    loss = output - output**2\ngradients = tape.gradient(loss, gaborlayer.trainable_variables)\ngradients\n\n[<tf.Tensor: shape=(), dtype=float32, numpy=-106343.39>,\n <tf.Tensor: shape=(), dtype=float32, numpy=91521.28>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([-13740.87402344, -44104.23828125, -21713.21679688, -19831.44921875])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([-32583.77929688, -11432.28710938,  -4077.08227539, -32302.77539062])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([  1810.35876465,   6409.01904297, -21779.10742188,  -7655.59228516])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([-90639.3515625 , 135911.        , 149277.640625  ,  28256.85351562])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([  90963.90625 , -136858.09375 , -149574.921875,  -28353.78125 ])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([-324.62695312,  947.14526367,  297.30041504,   96.82617188])>]"
  }
]