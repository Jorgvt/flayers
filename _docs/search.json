[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "flayers",
    "section": "",
    "text": "git clone https://github.com/Jorgvt/flayers.git\ncd flayers\npip install -e .\npip install flayers"
  },
  {
    "objectID": "Experiments/00_randomgabor.html",
    "href": "Experiments/00_randomgabor.html",
    "title": "Random Gabor experiment",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom einops import rearrange, repeat\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import mnist\n\nfrom flayers.layers import RandomGabor"
  },
  {
    "objectID": "Experiments/00_randomgabor.html#data-loading",
    "href": "Experiments/00_randomgabor.html#data-loading",
    "title": "Random Gabor experiment",
    "section": "Data loading",
    "text": "Data loading\n\nWe will be using MNIST for a simple and quick test.\n\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\nX_train = repeat(X_train, \"b h w ->  b h w c\", c=1)/255.0\nX_test = repeat(X_test, \"b h w ->  b h w c\", c=1)/255.0\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\n((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
  },
  {
    "objectID": "Experiments/00_randomgabor.html#definition-of-simple-model",
    "href": "Experiments/00_randomgabor.html#definition-of-simple-model",
    "title": "Random Gabor experiment",
    "section": "Definition of simple model",
    "text": "Definition of simple model\n\nmodel = tf.keras.Sequential([\n    RandomGabor(n_gabors=4, size=20, input_shape=(28,28,1)),\n    # layers.Conv2D(32, 3, input_shape=(28,28,1)),\n    layers.MaxPool2D(2),\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\n2022-09-06 11:22:17.064899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2373 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 780 Ti, pci bus id: 0000:02:00.0, compute capability: 3.5\n2022-09-06 11:22:17.065914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 151 MB memory:  -> device: 1, name: Tesla K40m, pci bus id: 0000:03:00.0, compute capability: 3.5\n2022-09-06 11:22:17.067063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 5435 MB memory:  -> device: 2, name: NVIDIA GeForce GTX TITAN Black, pci bus id: 0000:83:00.0, compute capability: 3.5\n2022-09-06 11:22:17.068686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 5435 MB memory:  -> device: 3, name: NVIDIA GeForce GTX TITAN Black, pci bus id: 0000:84:00.0, compute capability: 3.5\n\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nrandom_gabor (RandomGabor)   (None, 28, 28, 4)         26        \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 14, 14, 4)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 784)               0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                7850      \n=================================================================\nTotal params: 7,876\nTrainable params: 7,876\nNon-trainable params: 0\n_________________________________________________________________\n\n\nWe can show the initial Gabor filters:\n\nmodel.layers[0].show_filters()\n\n2022-09-06 11:22:19.809341: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-09-06 11:22:20.052736: I tensorflow/core/util/cuda_solvers.cc:180] Creating CudaSolver handles for stream 0x563ffaaf38c0\n\n\n\n\n\n\nhistory = model.fit(X_train, Y_train, batch_size=128, epochs=5)\n\nEpoch 1/5\n\n\n2022-09-06 11:22:26.673075: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n2022-09-06 11:22:27.122762: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n\n\n469/469 [==============================] - 18s 28ms/step - loss: 1.2855 - accuracy: 0.7523\nEpoch 2/5\n469/469 [==============================] - 13s 28ms/step - loss: 0.2952 - accuracy: 0.9105\nEpoch 3/5\n469/469 [==============================] - 13s 28ms/step - loss: 0.2307 - accuracy: 0.9303\nEpoch 4/5\n469/469 [==============================] - 13s 28ms/step - loss: 0.1971 - accuracy: 0.9409\nEpoch 5/5\n469/469 [==============================] - 13s 28ms/step - loss: 0.1745 - accuracy: 0.9478\n\n\nWe can visualize the gabor filters after the training process:\n\nmodel.layers[0].show_filters()"
  },
  {
    "objectID": "layers.html",
    "href": "layers.html",
    "title": "Layers",
    "section": "",
    "text": "Convolutional layer that forces a functional Gabor form for its filters. Every parameter of the Gabor can be learnt."
  },
  {
    "objectID": "layers.html#managing-dtype",
    "href": "layers.html#managing-dtype",
    "title": "Layers",
    "section": "Managing dtype",
    "text": "Managing dtype\nTensorflow is a bit picky when it comes to dtype, so it can be useful to define a function that will ensure that every parameter is casted to the same dtype:\n\na, b = tf.convert_to_tensor(1), tf.convert_to_tensor(1.1)\nprint(a.dtype, b.dtype)\n# assert a.dtype != b.dtype\n\n<dtype: 'int32'> <dtype: 'float32'>\n\n\n2022-09-07 12:45:14.923533: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n2022-09-07 12:45:14.923587: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: megatron\n2022-09-07 12:45:14.923602: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: megatron\n2022-09-07 12:45:14.923695: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.57.2\n2022-09-07 12:45:14.923743: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.57.2\n2022-09-07 12:45:14.923757: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.57.2\n\n\n\nc, d = cast_all(a, b)\nprint(c.dtype, d.dtype)\nassert c.dtype == d.dtype\n\n<dtype: 'float32'> <dtype: 'float32'>"
  },
  {
    "objectID": "layers.html#creating-a-gabor-filter-in-tensorflow",
    "href": "layers.html#creating-a-gabor-filter-in-tensorflow",
    "title": "Layers",
    "section": "Creating a Gabor filter in TensorFlow",
    "text": "Creating a Gabor filter in TensorFlow\nFirst of all we need to be able to generate Gabor filters as Tensorflow Tensor:\n\nsource\n\ngabor_2d_tf\n\n gabor_2d_tf (i, j, imean, jmean, sigma_i, sigma_j, freq, theta,\n              sigma_theta)\n\n\n\n\n\nDetails\n\n\n\n\ni\nHorizontal domain\n\n\nj\nVertical domain\n\n\nimean\nHorizontal mean\n\n\njmean\nVertical mean\n\n\nsigma_i\nHorizontal width\n\n\nsigma_j\nVertical width\n\n\nfreq\nFrecuency of the filter\n\n\ntheta\nAngle of the filter\n\n\nsigma_theta\nWidth of the angle?? Rotation of the domain??\n\n\n\n\nsource\n\n\ncreate_gabor_rot_tf\n\n create_gabor_rot_tf (Nrows, Ncols, imean, jmean, sigma_i, sigma_j, freq,\n                      theta, rot_theta, sigma_theta, fs)\n\nCreates a rotated Gabor filter with the input parameters.\n\n\n\n\nDetails\n\n\n\n\nNrows\nNumber of horizontal pixels\n\n\nNcols\nNumber of vertical pixels\n\n\nimean\nHorizontal mean (in degrees)\n\n\njmean\nVertical mean (in degrees)\n\n\nsigma_i\nHorizontal width (in degrees)\n\n\nsigma_j\nVertical width (in degrees)\n\n\nfreq\nFrequency\n\n\ntheta\nAngle\n\n\nrot_theta\nRotation of the domain??\n\n\nsigma_theta\nWidth of the angle?? Rotation of the domain??\n\n\nfs\nSampling frequency\n\n\n\n\ngabor = create_gabor_rot_tf(Nrows=20, Ncols=20, imean=0.5, jmean=0.5, sigma_i=0.1, sigma_j=0.1, freq=10, theta=0, rot_theta=0, sigma_theta=0, fs=20)\nplt.imshow(gabor)\nplt.show()\n\n2022-09-07 12:45:21.765425: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)"
  },
  {
    "objectID": "layers.html#creating-a-set-of-gabor-filters",
    "href": "layers.html#creating-a-set-of-gabor-filters",
    "title": "Layers",
    "section": "Creating a set of Gabor filters",
    "text": "Creating a set of Gabor filters\n\nIt can be a little bit tricky to translate plain Python for loops into tf.function.\n\nIn plain Python, if we wanted to create a set of filters we could initialize an empty array or list and fill it with the different filters generated inside a for loop, but we canâ€™t do that inside a tf.function because Tensorflow tries to build the computational graph and starts to nest graphs inside graphs and the performance is terrible. Luckily for us, they implement a tf.TensorArray that can be used inside a tf.function to this effect.\n\nsource\n\ncreate_multiple_different_rot_gabor_tf\n\n create_multiple_different_rot_gabor_tf (n_gabors, Nrows, Ncols, imean,\n                                         jmean, sigma_i:list,\n                                         sigma_j:list, freq:list,\n                                         theta:list, rot_theta:list,\n                                         sigma_theta:list, fs)\n\nCreates a set of Gabor filters.\n\n\n\n\nType\nDetails\n\n\n\n\nn_gabors\n\nNumber of Gabor filters we want to create.\n\n\nNrows\n\nNumber of horizontal pixels.\n\n\nNcols\n\nNumber of vertical pixels.\n\n\nimean\n\nHorizontal mean (in degrees).\n\n\njmean\n\nVertical mean (in degrees).\n\n\nsigma_i\nlist\nHorizontal width (in degrees).\n\n\nsigma_j\nlist\nVertical width (in degrees).\n\n\nfreq\nlist\nFrequency.\n\n\ntheta\nlist\nAngle.\n\n\nrot_theta\nlist\nRotation of the domain??\n\n\nsigma_theta\nlist\nWidth of the angle?? Rotation of the domain??\n\n\nfs\n\nSampling frequency.\n\n\n\n\nn_gabors = 4\ngabors = create_multiple_different_rot_gabor_tf(n_gabors=n_gabors, Nrows=20, Ncols=20, imean=0.5, jmean=0.5, sigma_i=[0.1]*n_gabors, sigma_j=[0.1]*n_gabors, freq=[10]*n_gabors, \n                                                theta=[0]*n_gabors, rot_theta=[0]*n_gabors, sigma_theta=[0]*n_gabors, fs=20)\ngabors.shape\n\nTensorShape([4, 20, 20])\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\nfor gabor_filter, ax in zip(gabors, axes.ravel()):\n    ax.imshow(gabor_filter)\nplt.show()\n\n\n\n\nWe can, as well, change the parameters of the Gabor filters independently:\n\nn_gabors = 4\nsigma_i = [0.1, 0.2, 0.3, 0.4]\nsigma_j = [0.1, 0.2, 0.3, 0.4]\nfreq = [10, 20, 30, 40]\ntheta = [0, 45, 90, 135]\nrot_theta = [0, 45, 90, 135]\nsigma_theta = [0, 45, 90, 135]\ngabors = create_multiple_different_rot_gabor_tf(n_gabors=n_gabors, Nrows=20, Ncols=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n                                                theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20)\ngabors.shape\n\nTensorShape([4, 20, 20])\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\nfor gabor_filter, ax in zip(gabors, axes.ravel()):\n    ax.imshow(gabor_filter)\nplt.show()"
  },
  {
    "objectID": "layers.html#gabor-layer-1",
    "href": "layers.html#gabor-layer-1",
    "title": "Layers",
    "section": "Gabor layer",
    "text": "Gabor layer\n\nGabor layer with pre-defined values.\n\n\nsource\n\nGaborLayer\n\n GaborLayer (*args, **kwargs)\n\nPre-initialized Gabor layer that is trainable through backpropagation.\n\na = GaborLayer(1, 20, 0.5, 0.5, [1], [1], [1], [1], [1], [1], [1])\n\n\nsource\n\n\nGaborLayer.call\n\n GaborLayer.call (inputs)\n\nBuild a set of filters from the stored values and convolve them with the input.\n\n\n\n\nDetails\n\n\n\n\ninputs\nInputs to the layer.\n\n\n\n\nn_gabors = 4\nsigma_i = [0.1, 0.2, 0.3, 0.4]\nsigma_j = [0.1, 0.2, 0.3, 0.4]\nfreq = [10, 20, 30, 40]\ntheta = [0, 45, 90, 135]\nrot_theta = [0, 45, 90, 135]\nsigma_theta = [0, 45, 90, 135]\n\ngaborlayer = GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n                        theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20)\nsample_input = np.random.uniform(0, 1, size=(1, 256, 256, 1))\nsample_output = gaborlayer(sample_input).numpy()\nassert sample_input.shape[1:3] == sample_output.shape[1:3]\nassert sample_output.shape[-1] == 4\nfig, axes = plt.subplots(1,2)\naxes[0].imshow(sample_input.squeeze())\naxes[1].imshow(sample_output.squeeze())\naxes[0].set_title(\"Input\")\naxes[1].set_title(\"Output\")\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\nIt can be helpful to implement a method to show the Gabor filters that are being used by the layer:\n\nsource\n\n\nGaborLayer.show_filters\n\n GaborLayer.show_filters ()\n\nCalculates and plots the filters corresponding to the stored parameters.\n\ngaborlayer = GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n                        theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20)\ngaborlayer.show_filters()\n\n\n\n\nWe can check that the parameters are trainable and thus the gradient is propagated properly:\n\ngaborlayer = GaborLayer(n_gabors=n_gabors, size=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n                        theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20)\nwith tf.GradientTape() as tape:\n    output = gaborlayer(sample_input)\n    loss = output - output**2\ngradients = tape.gradient(loss, gaborlayer.trainable_variables)\ngradients\n\n[<tf.Tensor: shape=(), dtype=float32, numpy=-518228500.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=464650800.0>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=\n array([1.7069018e+08, 2.3553311e+09, 7.9928645e+06, 2.7543188e+09],\n       dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=\n array([1.7110696e+08, 2.3491379e+09, 7.9020500e+06, 2.7531845e+09],\n       dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=\n array([-3.7713922e+04, -6.8492672e+07, -1.0847496e+05, -1.0130394e+08],\n       dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=\n array([ 2.7155601e+03, -2.8083200e+05,  2.8227250e+04, -1.3219584e+07],\n       dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=\n array([-2.7154375e+03,  2.8070400e+05, -2.8227438e+04,  1.3219496e+07],\n       dtype=float32)>,\n <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.00e+00, 1.28e+02, 6.25e-02, 2.40e+01], dtype=float32)>]"
  },
  {
    "objectID": "layers.html#random-initialize-a-simple-set-of-gabor-filters",
    "href": "layers.html#random-initialize-a-simple-set-of-gabor-filters",
    "title": "Layers",
    "section": "Random initialize a simple set of Gabor filters",
    "text": "Random initialize a simple set of Gabor filters\n\nInsted of defining ourselves the initial values, we can randomly initialize them. This can speed up our testing.\n\n\nsource\n\ncreate_simple_random_set\n\n create_simple_random_set (n_gabors, size)\n\nCreates a simple set of randomly initialized squared Gabor filters.\n\n\n\n\nDetails\n\n\n\n\nn_gabors\nNumber of Gabor filters we want to create.\n\n\nsize\nSize of the Gabor (they will be square).\n\n\n\n\ngabors = create_simple_random_set(n_gabors=4, size=20)\ngabors.shape\n\n2022-09-05 14:41:23.782959: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n\n\nTensorShape([4, 20, 20])\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\nfor gabor_filter, ax in zip(gabors, axes.ravel()):\n    ax.imshow(gabor_filter)\nplt.show()"
  },
  {
    "objectID": "layers.html#randomgabor",
    "href": "layers.html#randomgabor",
    "title": "Layers",
    "section": "RandomGabor",
    "text": "RandomGabor\n\nActually, we can define a different layer that initializes almost all of its parameters randomly by inhereting from GaborLayer.\n\n\nsource\n\nRandomGabor\n\n RandomGabor (*args, **kwargs)\n\nRandomly initialized Gabor layer that is trainable through backpropagation.\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\n\nWe can check that the .call() method from GaborLayer still works:\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\nsample_input = np.random.uniform(0, 1, size=(1, 256, 256, 1))\nsample_output = gaborlayer(sample_input).numpy()\nassert sample_input.shape[1:3] == sample_output.shape[1:3]\nassert sample_output.shape[-1] == 4\nfig, axes = plt.subplots(1,2)\naxes[0].imshow(sample_input.squeeze())\naxes[1].imshow(sample_output.squeeze())\naxes[0].set_title(\"Input\")\naxes[1].set_title(\"Output\")\nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\nEnsure that we can still plot its filters:\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\ngaborlayer.show_filters()\n\n\n\n\nWe can check that the parameters are trainable and thus the gradient is propagated properly:\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\nwith tf.GradientTape() as tape:\n    output = gaborlayer(sample_input)\n    loss = output - output**2\ngradients = tape.gradient(loss, gaborlayer.trainable_variables)\ngradients\n\n[<tf.Tensor: shape=(), dtype=float32, numpy=590727.44>,\n <tf.Tensor: shape=(), dtype=float32, numpy=-1516923.6>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([1157107.625    , 7056590.5      ,  274429.03125  ,  128686.7734375])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([2003976.      , 1678123.25    ,  250463.8125  ,  158964.828125])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([ -9410.62109375, -39677.76953125,  15587.83789062,  21519.70507812])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([-796422.125    , 1111984.625    ,  -71850.453125 , -124460.5546875])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=\n array([  724515.625    , -1006827.4375   ,    73009.8125   ,\n          124548.3828125])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([ 7.19070781e+04, -1.05157609e+05, -1.15905176e+03, -8.77519531e+01])>]"
  }
]