[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "flayers",
    "section": "",
    "text": "git clone https://github.com/Jorgvt/flayers.git\ncd flayers\npip install -e .\npip install flayers"
  },
  {
    "objectID": "Experiments/00_randomgabor.html",
    "href": "Experiments/00_randomgabor.html",
    "title": "Random Gabor experiment",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom einops import rearrange, repeat\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import mnist\n\nfrom flayers.layers import RandomGabor"
  },
  {
    "objectID": "Experiments/00_randomgabor.html#data-loading",
    "href": "Experiments/00_randomgabor.html#data-loading",
    "title": "Random Gabor experiment",
    "section": "Data loading",
    "text": "Data loading\n\nWe will be using MNIST for a simple and quick test.\n\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\nX_train = repeat(X_train, \"b h w ->  b h w c\", c=1)/255.0\nX_test = repeat(X_test, \"b h w ->  b h w c\", c=1)/255.0\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\n((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
  },
  {
    "objectID": "Experiments/00_randomgabor.html#definition-of-simple-model",
    "href": "Experiments/00_randomgabor.html#definition-of-simple-model",
    "title": "Random Gabor experiment",
    "section": "Definition of simple model",
    "text": "Definition of simple model\n\nmodel = tf.keras.Sequential([\n    RandomGabor(n_gabors=4, size=20, input_shape=(28,28,1)),\n    # layers.Conv2D(32, 3, input_shape=(28,28,1)),\n    layers.MaxPool2D(2),\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(10, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.summary()\n\n2022-09-06 11:22:17.064899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2373 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 780 Ti, pci bus id: 0000:02:00.0, compute capability: 3.5\n2022-09-06 11:22:17.065914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 151 MB memory:  -> device: 1, name: Tesla K40m, pci bus id: 0000:03:00.0, compute capability: 3.5\n2022-09-06 11:22:17.067063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 5435 MB memory:  -> device: 2, name: NVIDIA GeForce GTX TITAN Black, pci bus id: 0000:83:00.0, compute capability: 3.5\n2022-09-06 11:22:17.068686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 5435 MB memory:  -> device: 3, name: NVIDIA GeForce GTX TITAN Black, pci bus id: 0000:84:00.0, compute capability: 3.5\n\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nrandom_gabor (RandomGabor)   (None, 28, 28, 4)         26        \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 14, 14, 4)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 784)               0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                7850      \n=================================================================\nTotal params: 7,876\nTrainable params: 7,876\nNon-trainable params: 0\n_________________________________________________________________\n\n\nWe can show the initial Gabor filters:\n\nmodel.layers[0].show_filters()\n\n2022-09-06 11:22:19.809341: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-09-06 11:22:20.052736: I tensorflow/core/util/cuda_solvers.cc:180] Creating CudaSolver handles for stream 0x563ffaaf38c0\n\n\n\n\n\n\nhistory = model.fit(X_train, Y_train, batch_size=128, epochs=5)\n\nEpoch 1/5\n\n\n2022-09-06 11:22:26.673075: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8100\n2022-09-06 11:22:27.122762: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n\n\n469/469 [==============================] - 18s 28ms/step - loss: 1.2855 - accuracy: 0.7523\nEpoch 2/5\n469/469 [==============================] - 13s 28ms/step - loss: 0.2952 - accuracy: 0.9105\nEpoch 3/5\n469/469 [==============================] - 13s 28ms/step - loss: 0.2307 - accuracy: 0.9303\nEpoch 4/5\n469/469 [==============================] - 13s 28ms/step - loss: 0.1971 - accuracy: 0.9409\nEpoch 5/5\n469/469 [==============================] - 13s 28ms/step - loss: 0.1745 - accuracy: 0.9478\n\n\nWe can visualize the gabor filters after the training process:\n\nmodel.layers[0].show_filters()"
  },
  {
    "objectID": "layers.html",
    "href": "layers.html",
    "title": "Layers",
    "section": "",
    "text": "Convolutional layer that forces a functional Gabor form for its filters. Every parameter of the Gabor can be learnt."
  },
  {
    "objectID": "layers.html#managing-dtype",
    "href": "layers.html#managing-dtype",
    "title": "Layers",
    "section": "Managing dtype",
    "text": "Managing dtype\nTensorflow is a bit picky when it comes to dtype, so it can be useful to define a function that will ensure that every parameter is casted to the same dtype:\n\na, b = tf.convert_to_tensor(1), tf.convert_to_tensor(1.1)\nprint(a.dtype, b.dtype)\n# assert a.dtype != b.dtype\n\n<dtype: 'int32'>Metal device set to: Apple M1 Pro\n\nsystemMemory: 16.00 GB\nmaxCacheSize: 5.33 GB\n\n <dtype: 'float32'>\n\n\n2022-09-05 14:41:22.149270: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-09-05 14:41:22.149448: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n\n\n\nc, d = cast_all(a, b)\nprint(c.dtype, d.dtype)\nassert c.dtype == d.dtype\n\n<dtype: 'float32'> <dtype: 'float32'>"
  },
  {
    "objectID": "layers.html#creating-a-gabor-filter-in-tensorflow",
    "href": "layers.html#creating-a-gabor-filter-in-tensorflow",
    "title": "Layers",
    "section": "Creating a Gabor filter in TensorFlow",
    "text": "Creating a Gabor filter in TensorFlow\nFirst of all we need to be able to generate Gabor filters as Tensorflow Tensor:\n\nsource\n\ngabor_2d_tf\n\n gabor_2d_tf (i, j, imean, jmean, sigma_i, sigma_j, freq, theta,\n              sigma_theta)\n\n\n\n\n\nDetails\n\n\n\n\ni\nHorizontal domain\n\n\nj\nVertical domain\n\n\nimean\nHorizontal mean\n\n\njmean\nVertical mean\n\n\nsigma_i\nHorizontal width\n\n\nsigma_j\nVertical width\n\n\nfreq\nFrecuency of the filter\n\n\ntheta\nAngle of the filter\n\n\nsigma_theta\nWidth of the angle?? Rotation of the domain??\n\n\n\n\nsource\n\n\ncreate_gabor_rot_tf\n\n create_gabor_rot_tf (Nrows, Ncols, imean, jmean, sigma_i, sigma_j, freq,\n                      theta, rot_theta, sigma_theta, fs)\n\nCreates a rotated Gabor filter with the input parameters.\n\n\n\n\nDetails\n\n\n\n\nNrows\nNumber of horizontal pixels\n\n\nNcols\nNumber of vertical pixels\n\n\nimean\nHorizontal mean (in degrees)\n\n\njmean\nVertical mean (in degrees)\n\n\nsigma_i\nHorizontal width (in degrees)\n\n\nsigma_j\nVertical width (in degrees)\n\n\nfreq\nFrequency\n\n\ntheta\nAngle\n\n\nrot_theta\nRotation of the domain??\n\n\nsigma_theta\nWidth of the angle?? Rotation of the domain??\n\n\nfs\nSampling frequency\n\n\n\n\ngabor = create_gabor_rot_tf(Nrows=20, Ncols=20, imean=0.5, jmean=0.5, sigma_i=0.1, sigma_j=0.1, freq=10, theta=0, rot_theta=0, sigma_theta=0, fs=20)\nplt.imshow(gabor)\nplt.show()\n\n2022-09-05 14:41:22.617903: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n2022-09-05 14:41:22.618195: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled."
  },
  {
    "objectID": "layers.html#creating-a-set-of-gabor-filters",
    "href": "layers.html#creating-a-set-of-gabor-filters",
    "title": "Layers",
    "section": "Creating a set of Gabor filters",
    "text": "Creating a set of Gabor filters\n\nIt can be a little bit tricky to translate plain Python for loops into tf.function.\n\nIn plain Python, if we wanted to create a set of filters we could initialize an empty array or list and fill it with the different filters generated inside a for loop, but we can’t do that inside a tf.function because Tensorflow tries to build the computational graph and starts to nest graphs inside graphs and the performance is terrible. Luckily for us, they implement a tf.TensorArray that can be used inside a tf.function to this effect.\n\nsource\n\ncreate_multiple_different_rot_gabor_tf\n\n create_multiple_different_rot_gabor_tf (n_gabors, Nrows, Ncols, imean,\n                                         jmean, sigma_i:list,\n                                         sigma_j:list, freq:list,\n                                         theta:list, rot_theta:list,\n                                         sigma_theta:list, fs)\n\nCreates a set of Gabor filters.\n\n\n\n\nType\nDetails\n\n\n\n\nn_gabors\n\nNumber of Gabor filters we want to create.\n\n\nNrows\n\nNumber of horizontal pixels.\n\n\nNcols\n\nNumber of vertical pixels.\n\n\nimean\n\nHorizontal mean (in degrees).\n\n\njmean\n\nVertical mean (in degrees).\n\n\nsigma_i\nlist\nHorizontal width (in degrees).\n\n\nsigma_j\nlist\nVertical width (in degrees).\n\n\nfreq\nlist\nFrequency.\n\n\ntheta\nlist\nAngle.\n\n\nrot_theta\nlist\nRotation of the domain??\n\n\nsigma_theta\nlist\nWidth of the angle?? Rotation of the domain??\n\n\nfs\n\nSampling frequency.\n\n\n\n\nn_gabors = 4\ngabors = create_multiple_different_rot_gabor_tf(n_gabors=n_gabors, Nrows=20, Ncols=20, imean=0.5, jmean=0.5, sigma_i=[0.1]*n_gabors, sigma_j=[0.1]*n_gabors, freq=[10]*n_gabors, \n                                                theta=[0]*n_gabors, rot_theta=[0]*n_gabors, sigma_theta=[0]*n_gabors, fs=20)\ngabors.shape\n\n2022-09-05 14:41:22.929161: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n\n\nTensorShape([4, 20, 20])\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\nfor gabor_filter, ax in zip(gabors, axes.ravel()):\n    ax.imshow(gabor_filter)\nplt.show()\n\n\n\n\nWe can, as well, change the parameters of the Gabor filters independently:\n\nn_gabors = 4\nsigma_i = [0.1, 0.2, 0.3, 0.4]\nsigma_j = [0.1, 0.2, 0.3, 0.4]\nfreq = [10, 20, 30, 40]\ntheta = [0, 45, 90, 135]\nrot_theta = [0, 45, 90, 135]\nsigma_theta = [0, 45, 90, 135]\ngabors = create_multiple_different_rot_gabor_tf(n_gabors=n_gabors, Nrows=20, Ncols=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n                                                theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20)\ngabors.shape\n\n2022-09-05 14:41:23.318301: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n\n\nTensorShape([4, 20, 20])\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\nfor gabor_filter, ax in zip(gabors, axes.ravel()):\n    ax.imshow(gabor_filter)\nplt.show()"
  },
  {
    "objectID": "layers.html#random-initialize-a-simple-set-of-gabor-filters",
    "href": "layers.html#random-initialize-a-simple-set-of-gabor-filters",
    "title": "Layers",
    "section": "Random initialize a simple set of Gabor filters",
    "text": "Random initialize a simple set of Gabor filters\n\nInsted of defining ourselves the initial values, we can randomly initialize them. This can speed up our testing.\n\n\nsource\n\ncreate_simple_random_set\n\n create_simple_random_set (n_gabors, size)\n\nCreates a simple set of randomly initialized squared Gabor filters.\n\n\n\n\nDetails\n\n\n\n\nn_gabors\nNumber of Gabor filters we want to create.\n\n\nsize\nSize of the Gabor (they will be square).\n\n\n\n\ngabors = create_simple_random_set(n_gabors=4, size=20)\ngabors.shape\n\n2022-09-05 14:41:23.782959: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n\n\nTensorShape([4, 20, 20])\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\nfor gabor_filter, ax in zip(gabors, axes.ravel()):\n    ax.imshow(gabor_filter)\nplt.show()"
  },
  {
    "objectID": "layers.html#randomgabor",
    "href": "layers.html#randomgabor",
    "title": "Layers",
    "section": "RandomGabor",
    "text": "RandomGabor\n\nFinally, we can define a new layer based on the Gabor filters we defined.\n\n\nsource\n\nRandomGabor\n\n RandomGabor (*args, **kwargs)\n\nRandomly initialized Gabor layer that is trainable through backpropagation.\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\n\nAs the variables are going to be changing through the training, we need to be re-generating the Gabor filters. The optimum approach would be keeping track of when do the parameters change and re-calculate the filters accordingly, but as a first step, it’s easier to generate the filters each time the layer is called.\n\nThis will be a waste of time in inference, but does the job during the training.\n\n\nsource\n\n\nRandomGabor.call\n\n RandomGabor.call (inputs)\n\nBuild a set of filters from the stored values and convolve them with the input.\n\n\n\n\nDetails\n\n\n\n\ninputs\nInputs to the layer.\n\n\n\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\nsample_input = np.random.uniform(0, 1, size=(1, 256, 256, 1))\nsample_output = gaborlayer(sample_input).numpy()\nassert sample_input.shape[1:3] == sample_output.shape[1:3]\nassert sample_output.shape[-1] == 4\nfig, axes = plt.subplots(1,2)\naxes[0].imshow(sample_input.squeeze())\naxes[1].imshow(sample_output.squeeze())\naxes[0].set_title(\"Input\")\naxes[1].set_title(\"Output\")\nplt.show()\n\n2022-09-05 14:41:24.475979: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\nIt can be helpful to implement a method to show the Gabor filters that are being used by the layer:\n\nsource\n\n\nRandomGabor.show_filters\n\n RandomGabor.show_filters ()\n\nCalculates and plots the filters corresponding to the stored parameters.\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\ngaborlayer.show_filters()\n\n\n\n\nWe can check that the parameters are trainable and thus the gradient is propagated properly:\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\nwith tf.GradientTape() as tape:\n    output = gaborlayer(sample_input)\n    loss = output - output**2\ngradients = tape.gradient(loss, gaborlayer.trainable_variables)\ngradients\n\n[<tf.Tensor: shape=(), dtype=float32, numpy=-95771330.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=-256691950.0>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([1.12147510e+07, 2.08058700e+06, 1.88292202e+11, 1.19323325e+06])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([4.04293825e+06, 1.21869860e+07, 1.56390707e+09, 7.58140125e+05])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([-6.51747734e+04, -7.98653369e+03, -1.17340864e+09,  3.53471523e+04])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([-4.07312281e+05, -1.30986512e+06, -2.92020388e+10, -2.28357281e+05])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([ 5.70060875e+05,  1.05778288e+06, -2.78123059e+09,  2.44379922e+05])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([-1.62746047e+05,  2.52082250e+05,  3.19831286e+10, -1.60228037e+04])>]"
  }
]