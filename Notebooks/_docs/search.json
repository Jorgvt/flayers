[
  {
    "objectID": "layers.html",
    "href": "layers.html",
    "title": "Layers",
    "section": "",
    "text": "Convolutional layer that forces a functional Gabor form for its filters. Every parameter of the Gabor can be learnt."
  },
  {
    "objectID": "layers.html#managing-dtype",
    "href": "layers.html#managing-dtype",
    "title": "Layers",
    "section": "Managing dtype",
    "text": "Managing dtype\nTensorflow is a bit picky when it comes to dtype, so it can be useful to define a function that will ensure that every parameter is casted to the same dtype:\n\na, b = tf.convert_to_tensor(1), tf.convert_to_tensor(1.1)\nprint(a.dtype, b.dtype)\n# assert a.dtype != b.dtype\n\n<dtype: 'int32'>Metal device set to: Apple M1 Pro\n\nsystemMemory: 16.00 GB\nmaxCacheSize: 5.33 GB\n\n <dtype: 'float32'>\n\n\n2022-09-05 14:41:22.149270: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-09-05 14:41:22.149448: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n\n\n\nc, d = cast_all(a, b)\nprint(c.dtype, d.dtype)\nassert c.dtype == d.dtype\n\n<dtype: 'float32'> <dtype: 'float32'>"
  },
  {
    "objectID": "layers.html#creating-a-gabor-filter-in-tensorflow",
    "href": "layers.html#creating-a-gabor-filter-in-tensorflow",
    "title": "Layers",
    "section": "Creating a Gabor filter in TensorFlow",
    "text": "Creating a Gabor filter in TensorFlow\nFirst of all we need to be able to generate Gabor filters as Tensorflow Tensor:\n\nsource\n\ngabor_2d_tf\n\n gabor_2d_tf (i, j, imean, jmean, sigma_i, sigma_j, freq, theta,\n              sigma_theta)\n\n\n\n\n\nDetails\n\n\n\n\ni\nHorizontal domain\n\n\nj\nVertical domain\n\n\nimean\nHorizontal mean\n\n\njmean\nVertical mean\n\n\nsigma_i\nHorizontal width\n\n\nsigma_j\nVertical width\n\n\nfreq\nFrecuency of the filter\n\n\ntheta\nAngle of the filter\n\n\nsigma_theta\nWidth of the angle?? Rotation of the domain??\n\n\n\n\nsource\n\n\ncreate_gabor_rot_tf\n\n create_gabor_rot_tf (Nrows, Ncols, imean, jmean, sigma_i, sigma_j, freq,\n                      theta, rot_theta, sigma_theta, fs)\n\nCreates a rotated Gabor filter with the input parameters.\n\n\n\n\nDetails\n\n\n\n\nNrows\nNumber of horizontal pixels\n\n\nNcols\nNumber of vertical pixels\n\n\nimean\nHorizontal mean (in degrees)\n\n\njmean\nVertical mean (in degrees)\n\n\nsigma_i\nHorizontal width (in degrees)\n\n\nsigma_j\nVertical width (in degrees)\n\n\nfreq\nFrequency\n\n\ntheta\nAngle\n\n\nrot_theta\nRotation of the domain??\n\n\nsigma_theta\nWidth of the angle?? Rotation of the domain??\n\n\nfs\nSampling frequency\n\n\n\n\ngabor = create_gabor_rot_tf(Nrows=20, Ncols=20, imean=0.5, jmean=0.5, sigma_i=0.1, sigma_j=0.1, freq=10, theta=0, rot_theta=0, sigma_theta=0, fs=20)\nplt.imshow(gabor)\nplt.show()\n\n2022-09-05 14:41:22.617903: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n2022-09-05 14:41:22.618195: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled."
  },
  {
    "objectID": "layers.html#creating-a-set-of-gabor-filters",
    "href": "layers.html#creating-a-set-of-gabor-filters",
    "title": "Layers",
    "section": "Creating a set of Gabor filters",
    "text": "Creating a set of Gabor filters\n\nIt can be a little bit tricky to translate plain Python for loops into tf.function.\n\nIn plain Python, if we wanted to create a set of filters we could initialize an empty array or list and fill it with the different filters generated inside a for loop, but we can’t do that inside a tf.function because Tensorflow tries to build the computational graph and starts to nest graphs inside graphs and the performance is terrible. Luckily for us, they implement a tf.TensorArray that can be used inside a tf.function to this effect.\n\nsource\n\ncreate_multiple_different_rot_gabor_tf\n\n create_multiple_different_rot_gabor_tf (n_gabors, Nrows, Ncols, imean,\n                                         jmean, sigma_i:list,\n                                         sigma_j:list, freq:list,\n                                         theta:list, rot_theta:list,\n                                         sigma_theta:list, fs)\n\nCreates a set of Gabor filters.\n\n\n\n\nType\nDetails\n\n\n\n\nn_gabors\n\nNumber of Gabor filters we want to create.\n\n\nNrows\n\nNumber of horizontal pixels.\n\n\nNcols\n\nNumber of vertical pixels.\n\n\nimean\n\nHorizontal mean (in degrees).\n\n\njmean\n\nVertical mean (in degrees).\n\n\nsigma_i\nlist\nHorizontal width (in degrees).\n\n\nsigma_j\nlist\nVertical width (in degrees).\n\n\nfreq\nlist\nFrequency.\n\n\ntheta\nlist\nAngle.\n\n\nrot_theta\nlist\nRotation of the domain??\n\n\nsigma_theta\nlist\nWidth of the angle?? Rotation of the domain??\n\n\nfs\n\nSampling frequency.\n\n\n\n\nn_gabors = 4\ngabors = create_multiple_different_rot_gabor_tf(n_gabors=n_gabors, Nrows=20, Ncols=20, imean=0.5, jmean=0.5, sigma_i=[0.1]*n_gabors, sigma_j=[0.1]*n_gabors, freq=[10]*n_gabors, \n                                                theta=[0]*n_gabors, rot_theta=[0]*n_gabors, sigma_theta=[0]*n_gabors, fs=20)\ngabors.shape\n\n2022-09-05 14:41:22.929161: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n\n\nTensorShape([4, 20, 20])\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\nfor gabor_filter, ax in zip(gabors, axes.ravel()):\n    ax.imshow(gabor_filter)\nplt.show()\n\n\n\n\nWe can, as well, change the parameters of the Gabor filters independently:\n\nn_gabors = 4\nsigma_i = [0.1, 0.2, 0.3, 0.4]\nsigma_j = [0.1, 0.2, 0.3, 0.4]\nfreq = [10, 20, 30, 40]\ntheta = [0, 45, 90, 135]\nrot_theta = [0, 45, 90, 135]\nsigma_theta = [0, 45, 90, 135]\ngabors = create_multiple_different_rot_gabor_tf(n_gabors=n_gabors, Nrows=20, Ncols=20, imean=0.5, jmean=0.5, sigma_i=sigma_i, sigma_j=sigma_j, freq=freq, \n                                                theta=theta, rot_theta=rot_theta, sigma_theta=sigma_theta, fs=20)\ngabors.shape\n\n2022-09-05 14:41:23.318301: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n\n\nTensorShape([4, 20, 20])\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\nfor gabor_filter, ax in zip(gabors, axes.ravel()):\n    ax.imshow(gabor_filter)\nplt.show()"
  },
  {
    "objectID": "layers.html#random-initialize-a-set-of-gabor-filters",
    "href": "layers.html#random-initialize-a-set-of-gabor-filters",
    "title": "Layers",
    "section": "Random initialize a set of Gabor filters",
    "text": "Random initialize a set of Gabor filters\n\nInsted of defining ourselves the initial values, we can randomly initialize them. This can speed up our testing.\n\n\n\ncreate_multiple_random_rot_gabor_tf\n\n create_multiple_random_rot_gabor_tf (n_gabors, Nrows, Ncols, imean,\n                                      jmean, sigma_i:list, sigma_j:list,\n                                      freq:list, theta:list,\n                                      rot_theta:list, sigma_theta:list,\n                                      fs)\n\nCreates a set of Gabor filters.\n\n\n\n\nType\nDetails\n\n\n\n\nn_gabors\n\nNumber of Gabor filters we want to create.\n\n\nNrows\n\nNumber of horizontal pixels.\n\n\nNcols\n\nNumber of vertical pixels.\n\n\nimean\n\nHorizontal mean (in degrees).\n\n\njmean\n\nVertical mean (in degrees).\n\n\nsigma_i\nlist\nHorizontal width (in degrees).\n\n\nsigma_j\nlist\nVertical width (in degrees).\n\n\nfreq\nlist\nFrequency.\n\n\ntheta\nlist\nAngle.\n\n\nrot_theta\nlist\nRotation of the domain??\n\n\nsigma_theta\nlist\nWidth of the angle?? Rotation of the domain??\n\n\nfs\n\nSampling frequency."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "flayers",
    "section": "",
    "text": "git clone https://github.com/Jorgvt/flayers.git\ncd flayers\npip install -e .\npip install flayers"
  },
  {
    "objectID": "layers.html#random-initialize-a-simple-set-of-gabor-filters",
    "href": "layers.html#random-initialize-a-simple-set-of-gabor-filters",
    "title": "Layers",
    "section": "Random initialize a simple set of Gabor filters",
    "text": "Random initialize a simple set of Gabor filters\n\nInsted of defining ourselves the initial values, we can randomly initialize them. This can speed up our testing.\n\n\nsource\n\ncreate_simple_random_set\n\n create_simple_random_set (n_gabors, size)\n\nCreates a simple set of randomly initialized squared Gabor filters.\n\n\n\n\nDetails\n\n\n\n\nn_gabors\nNumber of Gabor filters we want to create.\n\n\nsize\nSize of the Gabor (they will be square).\n\n\n\n\ngabors = create_simple_random_set(n_gabors=4, size=20)\ngabors.shape\n\n2022-09-05 14:41:23.782959: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n\n\nTensorShape([4, 20, 20])\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\nfor gabor_filter, ax in zip(gabors, axes.ravel()):\n    ax.imshow(gabor_filter)\nplt.show()"
  },
  {
    "objectID": "layers.html#randomgabor",
    "href": "layers.html#randomgabor",
    "title": "Layers",
    "section": "RandomGabor",
    "text": "RandomGabor\n\nFinally, we can define a new layer based on the Gabor filters we defined.\n\n\nsource\n\nRandomGabor\n\n RandomGabor (*args, **kwargs)\n\nRandomly initialized Gabor layer that is trainable through backpropagation.\n\n\n\n\nDetails\n\n\n\n\nkwargs\n\n\n\n\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\n\nAs the variables are going to be changing through the training, we need to be re-generating the Gabor filters. The optimum approach would be keeping track of when do the parameters change and re-calculate the filters accordingly, but as a first step, it’s easier to generate the filters each time the layer is called.\n\nThis will be a waste of time in inference, but does the job during the training.\n\n\nsource\n\n\nRandomGabor.call\n\n RandomGabor.call (inputs)\n\nBuild a set of filters from the stored values and convolve them with the input.\n\n\n\n\nDetails\n\n\n\n\ninputs\nInputs to the layer.\n\n\n\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\nsample_input = np.random.uniform(0, 1, size=(1, 256, 256, 1))\nsample_output = gaborlayer(sample_input).numpy()\nassert sample_input.shape[1:3] == sample_output.shape[1:3]\nassert sample_output.shape[-1] == 4\nfig, axes = plt.subplots(1,2)\naxes[0].imshow(sample_input.squeeze())\naxes[1].imshow(sample_output.squeeze())\naxes[0].set_title(\"Input\")\naxes[1].set_title(\"Output\")\nplt.show()\n\n2022-09-05 14:41:24.475979: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\nIt can be helpful to implement a method to show the Gabor filters that are being used by the layer:\n\nsource\n\n\nRandomGabor.show_filters\n\n RandomGabor.show_filters ()\n\nCalculates and plots the filters corresponding to the stored parameters.\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\ngaborlayer.show_filters()\n\n\n\n\nWe can check that the parameters are trainable and thus the gradient is propagated properly:\n\ngaborlayer = RandomGabor(n_gabors=4, size=20)\nwith tf.GradientTape() as tape:\n    output = gaborlayer(sample_input)\n    loss = output - output**2\ngradients = tape.gradient(loss, gaborlayer.trainable_variables)\ngradients\n\n[<tf.Tensor: shape=(), dtype=float32, numpy=-95771330.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=-256691950.0>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([1.12147510e+07, 2.08058700e+06, 1.88292202e+11, 1.19323325e+06])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([4.04293825e+06, 1.21869860e+07, 1.56390707e+09, 7.58140125e+05])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([-6.51747734e+04, -7.98653369e+03, -1.17340864e+09,  3.53471523e+04])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([-4.07312281e+05, -1.30986512e+06, -2.92020388e+10, -2.28357281e+05])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([ 5.70060875e+05,  1.05778288e+06, -2.78123059e+09,  2.44379922e+05])>,\n <tf.Tensor: shape=(4,), dtype=float64, numpy=array([-1.62746047e+05,  2.52082250e+05,  3.19831286e+10, -1.60228037e+04])>]"
  }
]