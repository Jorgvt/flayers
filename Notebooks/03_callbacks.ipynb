{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "\n",
    "> Useful callbacks to use with the functional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import wandb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from flayers.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import wandb\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from einops import repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gabor parameter logging\n",
    "\n",
    "> Logging Gabor parameters into *wandb*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that sometimes, during training, an error would rise regarding the inverse of the matrix during the calculation of the Gabor filters. Our first thought was that the covariance matrix (calculated with the parameters `sigma_i` and `sigma_j`) wasn't invertible, meaning that `sigma_i` and `sigma_j` were non-positive, but a constraint on the variables did not fix the problem. To inspect it in more detail, we are going to log all the layer's weights during training to *wandb* to try and find the root of the problem.\n",
    "\n",
    "> To avoid introducing dependencies that won't be used by many people, we can put the `import wandb` in the instantiation of the callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class GaborLayerLogger(Callback):\n",
    "    import wandb\n",
    "\n",
    "    \"\"\"Logs the gabor parameters into wandb during training.\"\"\"\n",
    "    def on_batch_end(self, \n",
    "                     batch, # Batch number.\n",
    "                     logs=None, # Dictionary containing metrics and information of the training.\n",
    "                     ):\n",
    "        \"\"\"Logs the gabor parameters after each batch (after each parameter update).\"\"\"\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, GaborLayer):\n",
    "                for weight in layer.weights:\n",
    "                    wandb.log({f'{layer.name}.{weight.name}': wandb.Histogram(weight)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class GaborLayerSeqLogger(Callback):\n",
    "    import wandb\n",
    "\n",
    "    \"\"\"Logs the gabor parameters into wandb during training.\"\"\"\n",
    "    def on_batch_end(self, \n",
    "                     batch, # Batch number.\n",
    "                     logs=None, # Dictionary containing metrics and information of the training.\n",
    "                     ):\n",
    "        \"\"\"Logs the gabor parameters after each batch (after each parameter update).\"\"\"\n",
    "        for layer in self.model.feature_extractor.layers:\n",
    "            if isinstance(layer, GaborLayer):\n",
    "                for weight in layer.weights:\n",
    "                    wandb.log({f'{layer.name}.{weight.name}': wandb.Histogram(weight)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if it logs the parameters appropriately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (60000,), (10000, 28, 28, 1), (10000,))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "X_train = repeat(X_train, \"b h w ->  b h w c\", c=1)/255.0\n",
    "X_test = repeat(X_test, \"b h w ->  b h w c\", c=1)/255.0\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "random_gabor_3 (RandomGabor) (None, 28, 28, 4)         1626      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 4)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                50        \n",
      "=================================================================\n",
      "Total params: 1,676\n",
      "Trainable params: 76\n",
      "Non-trainable params: 1,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    RandomGabor(n_gabors=4, size=20, input_shape=(28,28,1)),\n",
    "    layers.MaxPool2D(2),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"epochs\":5,\n",
    "    \"batch_size\":64,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjorgvt\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/disk/users-muten/vitojor/flayers/Notebooks/wandb/run-20220921_125140-lk0tpm9i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jorgvt/Testing/runs/lk0tpm9i\" target=\"_blank\">smooth-donkey-1</a></strong> to <a href=\"https://wandb.ai/jorgvt/Testing\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"Testing\",\n",
    "           config=config)\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, epochs=config.epochs, batch_size=config.batch_size, callbacks=[GaborLayerLogger()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tf26')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
